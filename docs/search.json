[
  {
    "objectID": "Research Projects/Long-term Short-term Time-series Modeling/covid_analysis.html",
    "href": "Research Projects/Long-term Short-term Time-series Modeling/covid_analysis.html",
    "title": "Predicting covid cases with LSTM Machine Learning Model",
    "section": "",
    "text": "# Import various libraries and routines needed for computation\nimport math \nimport pandas as pd\nimport numpy as np\nimport tensorflow as tf\n%matplotlib inline\nimport matplotlib.pyplot as plt\nfrom math import sqrt\nfrom numpy import concatenate\nfrom matplotlib import pyplot\nfrom pandas import read_csv\nfrom pandas import DataFrame\nfrom sklearn.preprocessing import MinMaxScaler, LabelEncoder\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nimport keras.backend as K\nfrom keras.layers import LSTM\nfrom keras.callbacks import EarlyStopping\nfrom datetime import date, timedelta, datetime \n\n\nplt.rcParams.update({\n    \"lines.color\": \"white\",\n    \"patch.edgecolor\": \"white\",\n    \"text.color\": \"black\",\n    \"axes.facecolor\": \"white\",\n    \"axes.edgecolor\": \"lightgray\",\n    \"axes.labelcolor\": \"white\",\n    \"xtick.color\": \"white\",\n    \"ytick.color\": \"white\",\n    \"grid.color\": \"lightgray\",\n    \"figure.facecolor\": \"black\",\n    \"figure.edgecolor\": \"black\",\n    \"savefig.facecolor\": \"black\",\n    \"savefig.edgecolor\": \"black\"})\nplt.rcParams['figure.figsize'] = [10, 7]\n\n\ndf = pd.read_csv('covid_final.csv')  \ndataset = df.set_index(['date'])\ndataset.drop(dataset.tail(10).index,\n        inplace = True)\nvalues = dataset.values\n\n\ndate_index = dataset.index\n\n\ndata_clean = dataset.copy()\ndata_clean_ext = dataset.copy()\ndata_clean_ext['new_cases_predictions'] = data_clean_ext['new_cases_smoothed']\ndata_clean.tail()\n\n\n\n\n\n\n\n\nnew_cases_smoothed\nreproduction_rate\nnew_tests_smoothed_per_thousand\nnew_vaccinations_smoothed_per_million\npeople_fully_vaccinated_per_hundred\ntotal_boosters_per_hundred\nstringency_index\n\n\ndate\n\n\n\n\n\n\n\n\n\n\n\n2022-03-08\n38934.286\n0.65\n2.748\n621\n65.24\n28.89\n53.24\n\n\n2022-03-09\n36641.429\n0.66\n2.699\n601\n65.25\n28.91\n53.24\n\n\n2022-03-10\n36330.429\n0.69\n2.613\n583\n65.27\n28.94\n53.24\n\n\n2022-03-11\n36104.714\n0.71\n2.580\n557\n65.29\n28.97\n53.24\n\n\n2022-03-12\n35464.143\n0.71\n2.561\n540\n65.30\n28.99\n53.24\n\n\n\n\n\n\n\n\n# number of rows in the data\nnrows = data_clean.shape[0]\n\n\n# Convert the data to numpy values\nnp_data_unscaled = np.array(data_clean)\nnp_data = np.reshape(np_data_unscaled, (nrows, -1))\n\n\n# ensure all data is float\nvalues = values.astype('float64')\n\n\n# Transform the data by scaling each feature to a range between 0 and 1\nscaler = MinMaxScaler()\nnp_data_scaled = scaler.fit_transform(np_data_unscaled)\n\n\n# Creating a separate scaler that works on a single column for scaling predictions\nscaler_pred = MinMaxScaler()\ndf_cases = pd.DataFrame(data_clean_ext['new_cases_smoothed'])\nnp_cases_scaled = scaler_pred.fit_transform(df_cases)\n\n\n# Set the sequence length - this is the timeframe used to make a single prediction\nsequence_length = 31\n\n# Prediction Index\nindex_cases = dataset.columns.get_loc(\"new_cases_smoothed\")\n\n# Split the training data into train and train data sets\n# As a first step, we get the number of rows to train the model on 80% of the data \ntrain_data_len = math.ceil(np_data_scaled.shape[0] * 0.8)\n\n# Create the training and test data\ntrain_data = np_data_scaled[0:train_data_len, :]\ntest_data = np_data_scaled[train_data_len - sequence_length:, :]\n\n# The RNN needs data with the format of [samples, time steps, features]\n# Here, we create N samples, sequence_length time steps per sample, and 6 features\ndef partition_dataset(sequence_length, data):\n    x, y = [], []\n    data_len = data.shape[0]\n    for i in range(sequence_length, data_len):\n        x.append(data[i-sequence_length:i,:]) #contains sequence_length values 0-sequence_length * columsn\n        y.append(data[i, index_cases]) #contains the prediction values for validation,  for single-step prediction\n    \n    # Convert the x and y to numpy arrays\n    x = np.array(x)\n    y = np.array(y)\n    return x, y\n\n# Generate training data and test data\nx_train, y_train = partition_dataset(sequence_length, train_data)\nx_test, y_test = partition_dataset(sequence_length, test_data)\n\n\n\n# Configure the neural network model\nmodel = Sequential()\n# Model with n_neurons = inputshape Timestamps, each with x_train.shape[2] variables\nn_neurons = x_train.shape[1] * x_train.shape[2]\nmodel.add(LSTM(n_neurons, return_sequences=False, input_shape=(x_train.shape[1], x_train.shape[2])))\nmodel.add(Dense(1))\n\n\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n# Compiling the LSTM\nmodel.compile(optimizer = 'adam', loss = 'mean_squared_error')\n\n\ncheckpoint_path = 'my_best_model.hdf5'\ncheckpoint = ModelCheckpoint(filepath=checkpoint_path, \n                             monitor='val_loss',\n                             verbose=1, \n                             save_best_only=True,\n                             mode='min')\n\nearlystopping = EarlyStopping(monitor='val_loss', patience=50, restore_best_weights=True, verbose =0)\ncallbacks = [checkpoint, earlystopping]\n\n\n# Training the model\nepochs = 300\nbatch_size = 20\nhistory = model.fit(x_train, y_train,\n                     batch_size=batch_size, \n                     epochs=epochs,\n                     validation_data=(x_test, y_test),\n                     callbacks = callbacks,\n                     verbose = 0)\n\n\nEpoch 00001: val_loss improved from inf to 0.04791, saving model to my_best_model.hdf5\n\nEpoch 00002: val_loss did not improve from 0.04791\n\nEpoch 00003: val_loss did not improve from 0.04791\n\nEpoch 00004: val_loss did not improve from 0.04791\n\nEpoch 00005: val_loss did not improve from 0.04791\n\nEpoch 00006: val_loss did not improve from 0.04791\n\nEpoch 00007: val_loss did not improve from 0.04791\n\nEpoch 00008: val_loss did not improve from 0.04791\n\nEpoch 00009: val_loss did not improve from 0.04791\n\nEpoch 00010: val_loss did not improve from 0.04791\n\nEpoch 00011: val_loss did not improve from 0.04791\n\nEpoch 00012: val_loss did not improve from 0.04791\n\nEpoch 00013: val_loss did not improve from 0.04791\n\nEpoch 00014: val_loss did not improve from 0.04791\n\nEpoch 00015: val_loss did not improve from 0.04791\n\nEpoch 00016: val_loss did not improve from 0.04791\n\nEpoch 00017: val_loss did not improve from 0.04791\n\nEpoch 00018: val_loss did not improve from 0.04791\n\nEpoch 00019: val_loss improved from 0.04791 to 0.04770, saving model to my_best_model.hdf5\n\nEpoch 00020: val_loss improved from 0.04770 to 0.03909, saving model to my_best_model.hdf5\n\nEpoch 00021: val_loss did not improve from 0.03909\n\nEpoch 00022: val_loss did not improve from 0.03909\n\nEpoch 00023: val_loss improved from 0.03909 to 0.03220, saving model to my_best_model.hdf5\n\nEpoch 00024: val_loss did not improve from 0.03220\n\nEpoch 00025: val_loss improved from 0.03220 to 0.02719, saving model to my_best_model.hdf5\n\nEpoch 00026: val_loss did not improve from 0.02719\n\nEpoch 00027: val_loss improved from 0.02719 to 0.02522, saving model to my_best_model.hdf5\n\nEpoch 00028: val_loss improved from 0.02522 to 0.02033, saving model to my_best_model.hdf5\n\nEpoch 00029: val_loss did not improve from 0.02033\n\nEpoch 00030: val_loss improved from 0.02033 to 0.01771, saving model to my_best_model.hdf5\n\nEpoch 00031: val_loss improved from 0.01771 to 0.01746, saving model to my_best_model.hdf5\n\nEpoch 00032: val_loss did not improve from 0.01746\n\nEpoch 00033: val_loss improved from 0.01746 to 0.01453, saving model to my_best_model.hdf5\n\nEpoch 00034: val_loss improved from 0.01453 to 0.01420, saving model to my_best_model.hdf5\n\nEpoch 00035: val_loss improved from 0.01420 to 0.01369, saving model to my_best_model.hdf5\n\nEpoch 00036: val_loss did not improve from 0.01369\n\nEpoch 00037: val_loss improved from 0.01369 to 0.01296, saving model to my_best_model.hdf5\n\nEpoch 00038: val_loss improved from 0.01296 to 0.01224, saving model to my_best_model.hdf5\n\nEpoch 00039: val_loss did not improve from 0.01224\n\nEpoch 00040: val_loss did not improve from 0.01224\n\nEpoch 00041: val_loss did not improve from 0.01224\n\nEpoch 00042: val_loss improved from 0.01224 to 0.01090, saving model to my_best_model.hdf5\n\nEpoch 00043: val_loss did not improve from 0.01090\n\nEpoch 00044: val_loss improved from 0.01090 to 0.00946, saving model to my_best_model.hdf5\n\nEpoch 00045: val_loss did not improve from 0.00946\n\nEpoch 00046: val_loss improved from 0.00946 to 0.00801, saving model to my_best_model.hdf5\n\nEpoch 00047: val_loss did not improve from 0.00801\n\nEpoch 00048: val_loss did not improve from 0.00801\n\nEpoch 00049: val_loss improved from 0.00801 to 0.00685, saving model to my_best_model.hdf5\n\nEpoch 00050: val_loss improved from 0.00685 to 0.00602, saving model to my_best_model.hdf5\n\nEpoch 00051: val_loss did not improve from 0.00602\n\nEpoch 00052: val_loss did not improve from 0.00602\n\nEpoch 00053: val_loss did not improve from 0.00602\n\nEpoch 00054: val_loss did not improve from 0.00602\n\nEpoch 00055: val_loss did not improve from 0.00602\n\nEpoch 00056: val_loss did not improve from 0.00602\n\nEpoch 00057: val_loss did not improve from 0.00602\n\nEpoch 00058: val_loss did not improve from 0.00602\n\nEpoch 00059: val_loss did not improve from 0.00602\n\nEpoch 00060: val_loss did not improve from 0.00602\n\nEpoch 00061: val_loss did not improve from 0.00602\n\nEpoch 00062: val_loss did not improve from 0.00602\n\nEpoch 00063: val_loss did not improve from 0.00602\n\nEpoch 00064: val_loss improved from 0.00602 to 0.00601, saving model to my_best_model.hdf5\n\nEpoch 00065: val_loss improved from 0.00601 to 0.00540, saving model to my_best_model.hdf5\n\nEpoch 00066: val_loss improved from 0.00540 to 0.00501, saving model to my_best_model.hdf5\n\nEpoch 00067: val_loss improved from 0.00501 to 0.00456, saving model to my_best_model.hdf5\n\nEpoch 00068: val_loss did not improve from 0.00456\n\nEpoch 00069: val_loss improved from 0.00456 to 0.00452, saving model to my_best_model.hdf5\n\nEpoch 00070: val_loss did not improve from 0.00452\n\nEpoch 00071: val_loss did not improve from 0.00452\n\nEpoch 00072: val_loss did not improve from 0.00452\n\nEpoch 00073: val_loss did not improve from 0.00452\n\nEpoch 00074: val_loss did not improve from 0.00452\n\nEpoch 00075: val_loss did not improve from 0.00452\n\nEpoch 00076: val_loss did not improve from 0.00452\n\nEpoch 00077: val_loss did not improve from 0.00452\n\nEpoch 00078: val_loss improved from 0.00452 to 0.00433, saving model to my_best_model.hdf5\n\nEpoch 00079: val_loss did not improve from 0.00433\n\nEpoch 00080: val_loss did not improve from 0.00433\n\nEpoch 00081: val_loss improved from 0.00433 to 0.00428, saving model to my_best_model.hdf5\n\nEpoch 00082: val_loss improved from 0.00428 to 0.00406, saving model to my_best_model.hdf5\n\nEpoch 00083: val_loss did not improve from 0.00406\n\nEpoch 00084: val_loss did not improve from 0.00406\n\nEpoch 00085: val_loss did not improve from 0.00406\n\nEpoch 00086: val_loss did not improve from 0.00406\n\nEpoch 00087: val_loss improved from 0.00406 to 0.00371, saving model to my_best_model.hdf5\n\nEpoch 00088: val_loss did not improve from 0.00371\n\nEpoch 00089: val_loss did not improve from 0.00371\n\nEpoch 00090: val_loss did not improve from 0.00371\n\nEpoch 00091: val_loss improved from 0.00371 to 0.00341, saving model to my_best_model.hdf5\n\nEpoch 00092: val_loss did not improve from 0.00341\n\nEpoch 00093: val_loss did not improve from 0.00341\n\nEpoch 00094: val_loss did not improve from 0.00341\n\nEpoch 00095: val_loss improved from 0.00341 to 0.00317, saving model to my_best_model.hdf5\n\nEpoch 00096: val_loss improved from 0.00317 to 0.00281, saving model to my_best_model.hdf5\n\nEpoch 00097: val_loss did not improve from 0.00281\n\nEpoch 00098: val_loss did not improve from 0.00281\n\nEpoch 00099: val_loss improved from 0.00281 to 0.00233, saving model to my_best_model.hdf5\n\nEpoch 00100: val_loss did not improve from 0.00233\n\nEpoch 00101: val_loss did not improve from 0.00233\n\nEpoch 00102: val_loss did not improve from 0.00233\n\nEpoch 00103: val_loss improved from 0.00233 to 0.00224, saving model to my_best_model.hdf5\n\nEpoch 00104: val_loss did not improve from 0.00224\n\nEpoch 00105: val_loss did not improve from 0.00224\n\nEpoch 00106: val_loss did not improve from 0.00224\n\nEpoch 00107: val_loss did not improve from 0.00224\n\nEpoch 00108: val_loss did not improve from 0.00224\n\nEpoch 00109: val_loss did not improve from 0.00224\n\nEpoch 00110: val_loss did not improve from 0.00224\n\nEpoch 00111: val_loss did not improve from 0.00224\n\nEpoch 00112: val_loss did not improve from 0.00224\n\nEpoch 00113: val_loss did not improve from 0.00224\n\nEpoch 00114: val_loss did not improve from 0.00224\n\nEpoch 00115: val_loss improved from 0.00224 to 0.00220, saving model to my_best_model.hdf5\n\nEpoch 00116: val_loss did not improve from 0.00220\n\nEpoch 00117: val_loss did not improve from 0.00220\n\nEpoch 00118: val_loss did not improve from 0.00220\n\nEpoch 00119: val_loss improved from 0.00220 to 0.00216, saving model to my_best_model.hdf5\n\nEpoch 00120: val_loss did not improve from 0.00216\n\nEpoch 00121: val_loss did not improve from 0.00216\n\nEpoch 00122: val_loss did not improve from 0.00216\n\nEpoch 00123: val_loss did not improve from 0.00216\n\nEpoch 00124: val_loss improved from 0.00216 to 0.00171, saving model to my_best_model.hdf5\n\nEpoch 00125: val_loss did not improve from 0.00171\n\nEpoch 00126: val_loss did not improve from 0.00171\n\nEpoch 00127: val_loss did not improve from 0.00171\n\nEpoch 00128: val_loss did not improve from 0.00171\n\nEpoch 00129: val_loss did not improve from 0.00171\n\nEpoch 00130: val_loss did not improve from 0.00171\n\nEpoch 00131: val_loss did not improve from 0.00171\n\nEpoch 00132: val_loss did not improve from 0.00171\n\nEpoch 00133: val_loss did not improve from 0.00171\n\nEpoch 00134: val_loss improved from 0.00171 to 0.00161, saving model to my_best_model.hdf5\n\nEpoch 00135: val_loss did not improve from 0.00161\n\nEpoch 00136: val_loss did not improve from 0.00161\n\nEpoch 00137: val_loss did not improve from 0.00161\n\nEpoch 00138: val_loss did not improve from 0.00161\n\nEpoch 00139: val_loss did not improve from 0.00161\n\nEpoch 00140: val_loss did not improve from 0.00161\n\nEpoch 00141: val_loss did not improve from 0.00161\n\nEpoch 00142: val_loss did not improve from 0.00161\n\nEpoch 00143: val_loss did not improve from 0.00161\n\nEpoch 00144: val_loss did not improve from 0.00161\n\nEpoch 00145: val_loss did not improve from 0.00161\n\nEpoch 00146: val_loss did not improve from 0.00161\n\nEpoch 00147: val_loss did not improve from 0.00161\n\nEpoch 00148: val_loss did not improve from 0.00161\n\nEpoch 00149: val_loss did not improve from 0.00161\n\nEpoch 00150: val_loss did not improve from 0.00161\n\nEpoch 00151: val_loss improved from 0.00161 to 0.00149, saving model to my_best_model.hdf5\n\nEpoch 00152: val_loss did not improve from 0.00149\n\nEpoch 00153: val_loss did not improve from 0.00149\n\nEpoch 00154: val_loss did not improve from 0.00149\n\nEpoch 00155: val_loss did not improve from 0.00149\n\nEpoch 00156: val_loss did not improve from 0.00149\n\nEpoch 00157: val_loss did not improve from 0.00149\n\nEpoch 00158: val_loss did not improve from 0.00149\n\nEpoch 00159: val_loss did not improve from 0.00149\n\nEpoch 00160: val_loss did not improve from 0.00149\n\nEpoch 00161: val_loss did not improve from 0.00149\n\nEpoch 00162: val_loss improved from 0.00149 to 0.00127, saving model to my_best_model.hdf5\n\nEpoch 00163: val_loss did not improve from 0.00127\n\nEpoch 00164: val_loss did not improve from 0.00127\n\nEpoch 00165: val_loss did not improve from 0.00127\n\nEpoch 00166: val_loss did not improve from 0.00127\n\nEpoch 00167: val_loss did not improve from 0.00127\n\nEpoch 00168: val_loss did not improve from 0.00127\n\nEpoch 00169: val_loss did not improve from 0.00127\n\nEpoch 00170: val_loss did not improve from 0.00127\n\nEpoch 00171: val_loss did not improve from 0.00127\n\nEpoch 00172: val_loss did not improve from 0.00127\n\nEpoch 00173: val_loss did not improve from 0.00127\n\nEpoch 00174: val_loss did not improve from 0.00127\n\nEpoch 00175: val_loss did not improve from 0.00127\n\nEpoch 00176: val_loss did not improve from 0.00127\n\nEpoch 00177: val_loss did not improve from 0.00127\n\nEpoch 00178: val_loss did not improve from 0.00127\n\nEpoch 00179: val_loss did not improve from 0.00127\n\nEpoch 00180: val_loss did not improve from 0.00127\n\nEpoch 00181: val_loss improved from 0.00127 to 0.00125, saving model to my_best_model.hdf5\n\nEpoch 00182: val_loss did not improve from 0.00125\n\nEpoch 00183: val_loss did not improve from 0.00125\n\nEpoch 00184: val_loss did not improve from 0.00125\n\nEpoch 00185: val_loss did not improve from 0.00125\n\nEpoch 00186: val_loss improved from 0.00125 to 0.00123, saving model to my_best_model.hdf5\n\nEpoch 00187: val_loss did not improve from 0.00123\n\nEpoch 00188: val_loss did not improve from 0.00123\n\nEpoch 00189: val_loss did not improve from 0.00123\n\nEpoch 00190: val_loss did not improve from 0.00123\n\nEpoch 00191: val_loss improved from 0.00123 to 0.00122, saving model to my_best_model.hdf5\n\nEpoch 00192: val_loss did not improve from 0.00122\n\nEpoch 00193: val_loss did not improve from 0.00122\n\nEpoch 00194: val_loss did not improve from 0.00122\n\nEpoch 00195: val_loss did not improve from 0.00122\n\nEpoch 00196: val_loss did not improve from 0.00122\n\nEpoch 00197: val_loss improved from 0.00122 to 0.00105, saving model to my_best_model.hdf5\n\nEpoch 00198: val_loss did not improve from 0.00105\n\nEpoch 00199: val_loss did not improve from 0.00105\n\nEpoch 00200: val_loss did not improve from 0.00105\n\nEpoch 00201: val_loss did not improve from 0.00105\n\nEpoch 00202: val_loss did not improve from 0.00105\n\nEpoch 00203: val_loss did not improve from 0.00105\n\nEpoch 00204: val_loss did not improve from 0.00105\n\nEpoch 00205: val_loss did not improve from 0.00105\n\nEpoch 00206: val_loss did not improve from 0.00105\n\nEpoch 00207: val_loss did not improve from 0.00105\n\nEpoch 00208: val_loss did not improve from 0.00105\n\nEpoch 00209: val_loss did not improve from 0.00105\n\nEpoch 00210: val_loss did not improve from 0.00105\n\nEpoch 00211: val_loss did not improve from 0.00105\n\nEpoch 00212: val_loss did not improve from 0.00105\n\nEpoch 00213: val_loss did not improve from 0.00105\n\nEpoch 00214: val_loss did not improve from 0.00105\n\nEpoch 00215: val_loss did not improve from 0.00105\n\nEpoch 00216: val_loss did not improve from 0.00105\n\nEpoch 00217: val_loss did not improve from 0.00105\n\nEpoch 00218: val_loss did not improve from 0.00105\n\nEpoch 00219: val_loss did not improve from 0.00105\n\nEpoch 00220: val_loss did not improve from 0.00105\n\nEpoch 00221: val_loss did not improve from 0.00105\n\nEpoch 00222: val_loss did not improve from 0.00105\n\nEpoch 00223: val_loss did not improve from 0.00105\n\nEpoch 00224: val_loss did not improve from 0.00105\n\nEpoch 00225: val_loss did not improve from 0.00105\n\nEpoch 00226: val_loss did not improve from 0.00105\n\nEpoch 00227: val_loss did not improve from 0.00105\n\nEpoch 00228: val_loss did not improve from 0.00105\n\nEpoch 00229: val_loss did not improve from 0.00105\n\nEpoch 00230: val_loss did not improve from 0.00105\n\nEpoch 00231: val_loss improved from 0.00105 to 0.00105, saving model to my_best_model.hdf5\n\nEpoch 00232: val_loss did not improve from 0.00105\n\nEpoch 00233: val_loss did not improve from 0.00105\n\nEpoch 00234: val_loss did not improve from 0.00105\n\nEpoch 00235: val_loss did not improve from 0.00105\n\nEpoch 00236: val_loss improved from 0.00105 to 0.00105, saving model to my_best_model.hdf5\n\nEpoch 00237: val_loss did not improve from 0.00105\n\nEpoch 00238: val_loss did not improve from 0.00105\n\nEpoch 00239: val_loss did not improve from 0.00105\n\nEpoch 00240: val_loss did not improve from 0.00105\n\nEpoch 00241: val_loss did not improve from 0.00105\n\nEpoch 00242: val_loss did not improve from 0.00105\n\nEpoch 00243: val_loss did not improve from 0.00105\n\nEpoch 00244: val_loss improved from 0.00105 to 0.00097, saving model to my_best_model.hdf5\n\nEpoch 00245: val_loss did not improve from 0.00097\n\nEpoch 00246: val_loss did not improve from 0.00097\n\nEpoch 00247: val_loss did not improve from 0.00097\n\nEpoch 00248: val_loss did not improve from 0.00097\n\nEpoch 00249: val_loss did not improve from 0.00097\n\nEpoch 00250: val_loss did not improve from 0.00097\n\nEpoch 00251: val_loss did not improve from 0.00097\n\nEpoch 00252: val_loss did not improve from 0.00097\n\nEpoch 00253: val_loss did not improve from 0.00097\n\nEpoch 00254: val_loss did not improve from 0.00097\n\nEpoch 00255: val_loss did not improve from 0.00097\n\nEpoch 00256: val_loss improved from 0.00097 to 0.00092, saving model to my_best_model.hdf5\n\nEpoch 00257: val_loss did not improve from 0.00092\n\nEpoch 00258: val_loss did not improve from 0.00092\n\nEpoch 00259: val_loss did not improve from 0.00092\n\nEpoch 00260: val_loss did not improve from 0.00092\n\nEpoch 00261: val_loss did not improve from 0.00092\n\nEpoch 00262: val_loss did not improve from 0.00092\n\nEpoch 00263: val_loss did not improve from 0.00092\n\nEpoch 00264: val_loss improved from 0.00092 to 0.00090, saving model to my_best_model.hdf5\n\nEpoch 00265: val_loss did not improve from 0.00090\n\nEpoch 00266: val_loss did not improve from 0.00090\n\nEpoch 00267: val_loss did not improve from 0.00090\n\nEpoch 00268: val_loss did not improve from 0.00090\n\nEpoch 00269: val_loss did not improve from 0.00090\n\nEpoch 00270: val_loss did not improve from 0.00090\n\nEpoch 00271: val_loss did not improve from 0.00090\n\nEpoch 00272: val_loss did not improve from 0.00090\n\nEpoch 00273: val_loss did not improve from 0.00090\n\nEpoch 00274: val_loss did not improve from 0.00090\n\nEpoch 00275: val_loss did not improve from 0.00090\n\nEpoch 00276: val_loss did not improve from 0.00090\n\nEpoch 00277: val_loss did not improve from 0.00090\n\nEpoch 00278: val_loss did not improve from 0.00090\n\nEpoch 00279: val_loss did not improve from 0.00090\n\nEpoch 00280: val_loss did not improve from 0.00090\n\nEpoch 00281: val_loss did not improve from 0.00090\n\nEpoch 00282: val_loss did not improve from 0.00090\n\nEpoch 00283: val_loss did not improve from 0.00090\n\nEpoch 00284: val_loss did not improve from 0.00090\n\nEpoch 00285: val_loss did not improve from 0.00090\n\nEpoch 00286: val_loss did not improve from 0.00090\n\nEpoch 00287: val_loss did not improve from 0.00090\n\nEpoch 00288: val_loss did not improve from 0.00090\n\nEpoch 00289: val_loss did not improve from 0.00090\n\nEpoch 00290: val_loss did not improve from 0.00090\n\nEpoch 00291: val_loss did not improve from 0.00090\n\nEpoch 00292: val_loss did not improve from 0.00090\n\nEpoch 00293: val_loss did not improve from 0.00090\n\nEpoch 00294: val_loss did not improve from 0.00090\n\nEpoch 00295: val_loss did not improve from 0.00090\n\nEpoch 00296: val_loss did not improve from 0.00090\n\nEpoch 00297: val_loss did not improve from 0.00090\n\nEpoch 00298: val_loss did not improve from 0.00090\n\nEpoch 00299: val_loss did not improve from 0.00090\n\nEpoch 00300: val_loss did not improve from 0.00090\n\n\n\nfrom tensorflow.keras.models import load_model\nmodel_from_saved_checkpoint = load_model(checkpoint_path)\n\n\n# Plot training & validation loss values\nplt.figure(figsize=(16,7))\nplt.plot(history.history['loss'], label='train')\nplt.plot(history.history['val_loss'], label='test')\nplt.legend()\nplt.show()\n\n\n\n\n\n# Get the predicted values\ny_pred_scaled = model_from_saved_checkpoint.predict(x_test)\n\n\n# Unscale the predicted values\ny_pred = scaler_pred.inverse_transform(y_pred_scaled)\n\n\ny_test_unscaled = scaler_pred.inverse_transform(y_test.reshape(-1, 1))\n\n\n# Mean Absolute Error (MAE)\nMAE = mean_absolute_error(y_test_unscaled, y_pred)\nprint(f'Median Absolute Error (MAE): {np.round(MAE, 2)}')\n\n# Mean Absolute Percentage Error (MAPE)\nMAPE = np.mean((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100\nprint(f'Mean Absolute Percentage Error (MAPE): {np.round(MAPE, 2)} %')\n\n# Median Absolute Percentage Error (MDAPE)\nMDAPE = np.median((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled)) ) * 100\nprint(f'Median Absolute Percentage Error (MDAPE): {np.round(MDAPE, 2)} %')\n\nMedian Absolute Error (MAE): 16315.09\nMean Absolute Percentage Error (MAPE): 9.79 %\nMedian Absolute Percentage Error (MDAPE): 5.88 %\n\n\n\nplt.plot(y_test_unscaled, label='True')\nplt.plot(y_pred, label='LSTM')\nplt.title(\"LSTM's_Prediction\")\nplt.xlabel('Observation')\nplt.ylabel('Cases Prediction')\nplt.legend()\nplt.show()\n\n\n\n\n\nnew_df = data_clean[-sequence_length:]\nN = sequence_length\n\n\n# Get the last N day closing price values and scale the data to be values between 0 and 1\nlast_N_days = new_df[-sequence_length:].values\nlast_N_days_scaled = scaler.transform(last_N_days)\n\n\n# Create an empty list and Append past N days\nX_test_new = []\nX_test_new.append(last_N_days_scaled)\n\n# Convert the X_test data set to a numpy array and reshape the data\npred_cases_scaled = model_from_saved_checkpoint.predict(np.array(X_test_new))\npred_cases_unscaled = scaler_pred.inverse_transform(pred_cases_scaled.reshape(-1, 1))\n\n\n# Print last price and predicted price for the next day\ncases_today = np.round(new_df['new_cases_smoothed'][-1])\npredicted_cases = np.round(pred_cases_unscaled.ravel()[0])\nchange_percent = np.round(100 - (cases_today * 100)/predicted_cases)\n\n\nplus = '+'; minus = ''\nprint(f'The close covid cases count today is  {cases_today}')\nprint(f'The predicted case count for the next day is {predicted_cases} ({plus if change_percent &gt; 0 else minus}{change_percent}%)')\n\nThe close covid cases count today is  35464.0\nThe predicted case count for the next day is 50879.0 (+30.0%)\n\n\n\n!jupyter nbconvert covid_analysis.ipynb --to markdown --NbConvertApp.output_files_dir=.\n#!cat covid_analysis.md | tee -a index.md\n!rm covid_analysis.md\n\n[NbConvertApp] Converting notebook covid_analysis.ipynb to markdown"
  },
  {
    "objectID": "Research Projects/Long-term Short-term Time-series Modeling/index.html",
    "href": "Research Projects/Long-term Short-term Time-series Modeling/index.html",
    "title": "Long-term Short-term Time-series Modeling",
    "section": "",
    "text": "It would be nice to predict the number of positive covid cases depending on past cases evolution. Regression models based on recurrent neural networks (RNNs) are proven to identify patterns in time series data and this allows us to make accurate short-term predictions.\nThe model used in the following example is based on long-term short-term memory (LSTM) model that uses more than one features to make informed predictions. LSTMs are recurrent neural networks that avoid the vanishing gradient problem prevalent in feed-forward type of algorithms by imposing filtering mechanisms in the gates using a technique known as back-propagation.\nThe following set of codes loads all the required Python libraries, packages, and subroutines required for LSTM modeling. This blog post is just intended to give a high level summary of how to realize a covid case count prediction in the United States using some convenient features readily available.\n# Import various libraries and routines needed for computation\nimport math \nimport pandas as pd\nimport numpy as np\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport keras.backend as K\nfrom math import sqrt\nfrom numpy import concatenate\nfrom matplotlib import pyplot\nfrom pandas import read_csv, DataFrame\nfrom sklearn.preprocessing import MinMaxScaler, LabelEncoder\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, LSTM\nfrom keras.callbacks import EarlyStopping\nfrom datetime import date, timedelta, datetime \n# Read in the data file that has relevant features \ndf = pd.read_csv('covid_final.csv')  \ndataset = df.set_index(['date'])\n# Drop the last 10 row as they are incomplete\ndataset.drop(dataset.tail(10).index,\n        inplace = True)\nvalues = dataset.values\n# Store the indexes (i.e., dates)\ndate_index = dataset.index\n# Clean up the dataset more for predictions and inverse transformations (Re-scaling)\ndata_clean = dataset.copy()\ndata_clean_ext = dataset.copy()\ndata_clean_ext['new_cases_predictions'] = data_clean_ext['new_cases_smoothed']\ndata_clean.tail()\n\n\n\n\n\n\n\n\n\nnew_cases_smoothed\n\n\nreproduction_rate\n\n\nnew_tests_smoothed_per_thousand\n\n\nnew_vaccinations_smoothed_per_million\n\n\npeople_fully_vaccinated_per_hundred\n\n\ntotal_boosters_per_hundred\n\n\nstringency_index\n\n\n\n\ndate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2022-03-08\n\n\n38934.286\n\n\n0.65\n\n\n2.748\n\n\n621\n\n\n65.24\n\n\n28.89\n\n\n53.24\n\n\n\n\n2022-03-09\n\n\n36641.429\n\n\n0.66\n\n\n2.699\n\n\n601\n\n\n65.25\n\n\n28.91\n\n\n53.24\n\n\n\n\n2022-03-10\n\n\n36330.429\n\n\n0.69\n\n\n2.613\n\n\n583\n\n\n65.27\n\n\n28.94\n\n\n53.24\n\n\n\n\n2022-03-11\n\n\n36104.714\n\n\n0.71\n\n\n2.580\n\n\n557\n\n\n65.29\n\n\n28.97\n\n\n53.24\n\n\n\n\n2022-03-12\n\n\n35464.143\n\n\n0.71\n\n\n2.561\n\n\n540\n\n\n65.30\n\n\n28.99\n\n\n53.24\n\n\n\n\n\n\n# number of rows in the data\nnrows = data_clean.shape[0]\nThe day-to-day case counts can be regarded as a time series and the data needs to be prepared before training a supervised learning model. For LSTM, the data is composed of inputs and outputs, and the inputs can be seen as a moving window blocks consisting of the feature values to predict the outcome. The size of the window is a free parameter that the user must optimize.\n# Convert the data to numpy values\nnp_data_unscaled = np.array(data_clean)\nnp_data = np.reshape(np_data_unscaled, (nrows, -1))\n# ensure all data is float\nvalues = values.astype('float64')\n# Transform the data by scaling each feature to a range between 0 and 1\nscaler = MinMaxScaler()\nnp_data_scaled = scaler.fit_transform(np_data_unscaled)\n# Creating a separate scaler that works on a single column for scaling predictions\nscaler_pred = MinMaxScaler()\ndf_cases = pd.DataFrame(data_clean_ext['new_cases_smoothed'])\nnp_cases_scaled = scaler_pred.fit_transform(df_cases)\nIn LSTM methodology, it is required to reshape the input to be a 3D tensor of samples, time steps, and features. This is more important when we are fitting the model later.\n# Set the sequence length - this is the timeframe used to make a single prediction\nsequence_length = 31  # rolling window size\n\n# Prediction Index\nindex_cases = dataset.columns.get_loc(\"new_cases_smoothed\")\n\n# Split the training data into train and train data sets\n# As a first step, we get the number of rows to train the model on 80% of the data \ntrain_data_len = math.ceil(np_data_scaled.shape[0] * 0.8)\n\n# Create the training and test data\ntrain_data = np_data_scaled[0:train_data_len, :]\ntest_data = np_data_scaled[train_data_len - sequence_length:, :]\n\n# The RNN needs data with the format of [samples, time steps, features]\n# Here, we create N samples, sequence_length time steps per sample, and 6 features\ndef partition_dataset(sequence_length, data):\n    x, y = [], []\n    data_len = data.shape[0]\n    for i in range(sequence_length, data_len):\n        x.append(data[i-sequence_length:i,:]) #contains sequence_length values 0-sequence_length * columsn\n        y.append(data[i, index_cases]) #contains the prediction values for validation,  for single-step prediction\n    \n    # Convert the x and y to numpy arrays\n    x = np.array(x)\n    y = np.array(y)\n    return x, y\n\n# Generate training data and test data\nx_train, y_train = partition_dataset(sequence_length, train_data)\nx_test, y_test = partition_dataset(sequence_length, test_data)\n# Configure the neural network model\nmodel = Sequential()\n# Model with n_neurons = inputshape Timestamps, each with x_train.shape[2] variables\nn_neurons = x_train.shape[1] * x_train.shape[2]\nmodel.add(LSTM(n_neurons, return_sequences=False, input_shape=(x_train.shape[1], x_train.shape[2])))\nmodel.add(Dense(1))\n# Check-points and early stopping parameters make our modeling easier\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n# Compiling the LSTM\nmodel.compile(optimizer = 'adam', loss = 'mean_squared_error')\n# Specfy the file and file path for the best model\ncheckpoint_path = 'my_best_model.hdf5'\ncheckpoint = ModelCheckpoint(filepath=checkpoint_path, \n                             monitor='val_loss',\n                             verbose=1, \n                             save_best_only=True,\n                             mode='min')\n\nearlystopping = EarlyStopping(monitor='val_loss', patience=50, restore_best_weights=True, verbose =0)\ncallbacks = [checkpoint, earlystopping]\n# Training the model\nepochs = 300\nbatch_size = 20\nhistory = model.fit(x_train, y_train,\n                     batch_size=batch_size, \n                     epochs=epochs,\n                     validation_data=(x_test, y_test),\n                     callbacks = callbacks,\n                     verbose = 0)\n# Load the best model\nfrom tensorflow.keras.models import load_model\nmodel_from_saved_checkpoint = load_model(checkpoint_path)\n# Plot training & validation loss values\nplt.figure(figsize=(16,7))\nplt.plot(history.history['loss'], label='train')\nplt.plot(history.history['val_loss'], label='test')\nplt.legend()\nplt.show()\n\n\n\npng\n\n\n# Get the predicted values\ny_pred_scaled = model_from_saved_checkpoint.predict(x_test)\n# Unscale the predicted values\ny_pred = scaler_pred.inverse_transform(y_pred_scaled)\n# reshape \ny_test_unscaled = scaler_pred.inverse_transform(y_test.reshape(-1, 1))\n# Mean Absolute Error (MAE)\nMAE = mean_absolute_error(y_test_unscaled, y_pred)\nprint(f'Median Absolute Error (MAE): {np.round(MAE, 2)}')\n\n# Mean Absolute Percentage Error (MAPE)\nMAPE = np.mean((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100\nprint(f'Mean Absolute Percentage Error (MAPE): {np.round(MAPE, 2)} %')\n\n# Median Absolute Percentage Error (MDAPE)\nMDAPE = np.median((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled)) ) * 100\nprint(f'Median Absolute Percentage Error (MDAPE): {np.round(MDAPE, 2)} %')\n# Plot of the true and predicted case counts\nplt.plot(y_test_unscaled, label='True')\nplt.plot(y_pred, label='LSTM')\nplt.title(\"LSTM's_Prediction\")\nplt.xlabel('Time steps')\nplt.ylabel('Cases')\nplt.legend()\nplt.show()\n\n\n\npng\n\n\n# New data frame for predicting the next day count\nnew_df = data_clean[-sequence_length:] # gets the last N days\nN = sequence_length\n# Get the values of the last N day cases counts \n# scale the data to be values between 0 and 1\nlast_N_days = new_df[-sequence_length:].values\nlast_N_days_scaled = scaler.transform(last_N_days)\n# Create an empty list and Append past N days\nX_test_new = []\nX_test_new.append(last_N_days_scaled)\n\n# Convert the X_test data set to a numpy array and reshape the data\npred_cases_scaled = model_from_saved_checkpoint.predict(np.array(X_test_new))\npred_cases_unscaled = scaler_pred.inverse_transform(pred_cases_scaled.reshape(-1, 1))\n# Print last count, predicted counts, and change percent for the next day\ncases_today = np.round(new_df['new_cases_smoothed'][-1])\npredicted_cases = np.round(pred_cases_unscaled.ravel()[0])\nchange_percent = np.round(100 - (cases_today * 100)/predicted_cases)"
  },
  {
    "objectID": "Research Projects/Informer Model/Multivariate_multistep_Informer.html",
    "href": "Research Projects/Informer Model/Multivariate_multistep_Informer.html",
    "title": "Informer",
    "section": "",
    "text": "!pip install yfinance\n\nRequirement already satisfied: yfinance in /home/deepak/anaconda3/lib/python3.7/site-packages (0.1.81)\nRequirement already satisfied: pandas&gt;=0.24.0 in /home/deepak/anaconda3/lib/python3.7/site-packages (from yfinance) (1.3.5)\nRequirement already satisfied: numpy&gt;=1.15 in /home/deepak/anaconda3/lib/python3.7/site-packages (from yfinance) (1.21.5)\nRequirement already satisfied: requests&gt;=2.26 in /home/deepak/anaconda3/lib/python3.7/site-packages (from yfinance) (2.31.0)\nRequirement already satisfied: multitasking&gt;=0.0.7 in /home/deepak/anaconda3/lib/python3.7/site-packages (from yfinance) (0.0.11)\nRequirement already satisfied: lxml&gt;=4.5.1 in /home/deepak/anaconda3/lib/python3.7/site-packages (from yfinance) (4.9.1)\nRequirement already satisfied: appdirs&gt;=1.4.4 in /home/deepak/anaconda3/lib/python3.7/site-packages (from yfinance) (1.4.4)\nRequirement already satisfied: python-dateutil&gt;=2.7.3 in /home/deepak/anaconda3/lib/python3.7/site-packages (from pandas&gt;=0.24.0-&gt;yfinance) (2.8.2)\nRequirement already satisfied: pytz&gt;=2017.3 in /home/deepak/anaconda3/lib/python3.7/site-packages (from pandas&gt;=0.24.0-&gt;yfinance) (2023.3)\nRequirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /home/deepak/anaconda3/lib/python3.7/site-packages (from requests&gt;=2.26-&gt;yfinance) (2.1.1)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in /home/deepak/anaconda3/lib/python3.7/site-packages (from requests&gt;=2.26-&gt;yfinance) (3.4)\nRequirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /home/deepak/anaconda3/lib/python3.7/site-packages (from requests&gt;=2.26-&gt;yfinance) (2.0.4)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /home/deepak/anaconda3/lib/python3.7/site-packages (from requests&gt;=2.26-&gt;yfinance) (2023.7.22)\nRequirement already satisfied: six&gt;=1.5 in /home/deepak/anaconda3/lib/python3.7/site-packages (from python-dateutil&gt;=2.7.3-&gt;pandas&gt;=0.24.0-&gt;yfinance) (1.16.0)\n\n\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\nimport tensorflow as tf\nimport yfinance as yf\n\n\n# Download historical data as dataframe\ndata = yf.download(\"AAPL\", start=\"2018-01-01\", end=\"2023-09-01\")\n\n[*********************100%***********************]  1 of 1 completed\n\n1 Failed download:\n- AAPL: No data found for this date range, symbol may be delisted\n\n\nThe Informer model variant is designed for multivariate prediction. Let’s consider ‘Open’, ‘High’, ‘Low’, and ‘Close’ prices for simplicity. The provided code is designed to fetch and preprocess historical stock prices for Apple Inc. for the purpose of multivariate time series forecasting using an LSTM model. Initially, the code downloads Apple’s stock data, specifically capturing four significant features: Open, High, Low, and Close prices. To make the data suitable for deep learning models, it is normalized to fit within a range of 0 to 1. The sequential data is then transformed into a format suitable for supervised learning, where the data from the past look_back days is used to predict the next day’s features. Finally, the data is partitioned into training (67%) and test sets, ensuring separate datasets for model training and evaluation.\n\n# Using multiple columns for multivariate prediction\ndf = data[[\"Open\", \"High\", \"Low\", \"Close\"]]\n\n# Normalize the data\nscaler = MinMaxScaler(feature_range=(0, 1))\ndf_scaled = scaler.fit_transform(df)\n\n# Prepare data for LSTM\nlook_back = 10  # Number of previous time steps to use as input variables\nn_features = df.shape[1]  # number of features\n\n# Convert to supervised learning problem\nX, y = [], []\nfor i in range(len(df_scaled) - look_back):\n    X.append(df_scaled[i:i+look_back, :])\n    y.append(df_scaled[i + look_back, :])\nX, y = np.array(X), np.array(y)\n\n# Reshape input to be [samples, time steps, features]\nX = np.reshape(X, (X.shape[0], X.shape[1], n_features))\n\n# Train-test split\ntrain_size = int(len(X) * 0.67)\ntest_size = len(X) - train_size\nX_train, X_test = X[0:train_size], X[train_size:]\ny_train, y_test = y[0:train_size], y[train_size:]\n\nprint(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n\nNameError: name 'data' is not defined\n\n\n\nProbSparse Self-Attention:\nSelf-attention involves computing a weighted sum of all values in the sequence, based on the dot product between the query and key. In ProbSparse, we don’t compute this for all query-key pairs, but rather select dominant ones, thus making the computation more efficient. The given code defines a custom Keras layer, ProbSparseSelfAttention, which implements the multi-head self-attention mechanism, a critical component of Transformer models. This layer initializes three dense networks for the Query, Key, and Value matrices, and splits the input data into multiple heads to enable parallel processing. During the forward pass (call method), the Query, Key, and Value matrices are calculated, scaled, and then used to compute attention scores. These scores indicate the importance of each element in the sequence when predicting another element. The output is a weighted sum of the input values, which is then passed through another dense layer to produce the final result.\n\nclass ProbSparseSelfAttention(tf.keras.layers.Layer):\n    def __init__(self, d_model, num_heads, **kwargs):\n        super(ProbSparseSelfAttention, self).__init__(**kwargs)\n        self.num_heads = num_heads\n        self.d_model = d_model\n\n        # Assert that d_model is divisible by num_heads\n        assert self.d_model % self.num_heads == 0, f\"d_model ({d_model}) must be divisible by num_heads ({num_heads})\"\n\n        self.depth = d_model // self.num_heads\n\n        # Defining the dense layers for Query, Key and Value\n        self.wq = tf.keras.layers.Dense(d_model)\n        self.wk = tf.keras.layers.Dense(d_model)\n        self.wv = tf.keras.layers.Dense(d_model)\n\n        self.dense = tf.keras.layers.Dense(d_model)\n\n    def split_heads(self, x, batch_size):\n        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n        return tf.transpose(x, perm=[0, 2, 1, 3])\n\n    def call(self, v, k, q):\n        batch_size = tf.shape(q)[0]\n\n        q = self.wq(q)\n        k = self.wk(k)\n        v = self.wv(v)\n\n        q = self.split_heads(q, batch_size)\n        k = self.split_heads(k, batch_size)\n        v = self.split_heads(v, batch_size)\n\n        # Fixing matrix multiplication\n        matmul_qk = tf.matmul(q, k, transpose_b=True)\n\n        d_k = tf.cast(self.depth, tf.float32)\n        scaled_attention_logits = matmul_qk / tf.math.sqrt(d_k)\n\n        attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n        output = tf.matmul(attention_weights, v)\n\n        output = tf.transpose(output, perm=[0, 2, 1, 3])\n        concat_attention = tf.reshape(output, (batch_size, -1, self.d_model))\n\n        return self.dense(concat_attention)\n\n\n\nInformer Encoder:\nThe InformerEncoder is a custom Keras layer designed to process sequential data using a combination of attention and convolutional mechanisms. Within the encoder, the input data undergoes multi-head self-attention, utilizing the ProbSparseSelfAttention mechanism, to capture relationships in the data regardless of their distance. Post attention, the data is transformed and normalized, then further processed using two 1D convolutional layers, emphasizing local features in the data. After another normalization step, the processed data is pooled to a lower dimensionality, ensuring the model captures global context, and then passed through a dense layer to produce the final output.\n\nclass InformerEncoder(tf.keras.layers.Layer):\n    def __init__(self, d_model, num_heads, conv_filters, **kwargs):\n        super(InformerEncoder, self).__init__(**kwargs)\n        self.d_model = d_model\n        self.num_heads = num_heads\n\n        # Assert that d_model is divisible by num_heads\n        assert self.d_model % self.num_heads == 0, f\"d_model ({d_model}) must be divisible by num_heads ({num_heads})\"\n\n        self.self_attention = ProbSparseSelfAttention(d_model=d_model, num_heads=num_heads)\n\n        self.norm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n\n        # This dense layer will transform the input 'x' to have the dimensionality 'd_model'\n        self.dense_transform = tf.keras.layers.Dense(d_model)\n\n        self.conv1 = tf.keras.layers.Conv1D(conv_filters, 3, padding='same')\n        self.conv2 = tf.keras.layers.Conv1D(d_model, 3, padding='same')\n        self.norm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n\n        self.global_avg_pooling = tf.keras.layers.GlobalAveragePooling1D()\n        self.dense = tf.keras.layers.Dense(d_model)\n\n    def call(self, x):\n        attn_output = self.self_attention(x, x, x)\n\n        # Transform 'x' to have the desired dimensionality\n        x_transformed = self.dense_transform(x)\n        attn_output = self.norm1(attn_output + x_transformed)\n\n        conv_output = self.conv1(attn_output)\n        conv_output = tf.nn.relu(conv_output)\n        conv_output = self.conv2(conv_output)\n\n        encoded_output = self.norm2(conv_output + attn_output)\n\n        pooled_output = self.global_avg_pooling(encoded_output)\n        return self.dense(pooled_output)[:, -4:]\n\n\n\n\ninput_layer = tf.keras.layers.Input(shape=(look_back, n_features))\n\n# Encoder\nencoder_output = InformerEncoder(d_model=360, num_heads=8, conv_filters=64)(input_layer)\n\n# Decoder (with attention)\ndecoder_lstm = tf.keras.layers\n\nThe InformerModel function is designed to create a deep learning architecture tailored for sequential data prediction. It takes an input sequence and processes it using the InformerEncoder, a custom encoder layer, which captures both local and global patterns in the data. Following the encoding step, a decoder structure unravels the encoded data by first repeating the encoder’s output, then passing it through an LSTM layer to retain sequential dependencies, and finally making predictions using a dense layer. The resulting architecture is then compiled with the Adam optimizer and Mean Squared Error loss, ready for training on time series data.\n\nfrom tensorflow.keras.layers import RepeatVector\nfrom tensorflow.keras.layers import Input\nfrom tensorflow.keras.layers import LSTM\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\n\n\ndef InformerModel(input_shape, d_model=64, num_heads=2, conv_filters=256, learning_rate= 1e-3):\n    # Input\n    input_layer = Input(shape=input_shape)\n\n    # Encoder\n    encoder_output = InformerEncoder(d_model=d_model, num_heads=num_heads, conv_filters=conv_filters)(input_layer)\n\n    # Decoder\n    repeated_output = RepeatVector(4)(encoder_output)  # Repeating encoder's output\n    decoder_lstm = LSTM(312, return_sequences=True)(repeated_output)\n    decoder_output = Dense(4)(decoder_lstm[:, -1, :])  # Use the last sequence output to predict the next value\n\n    # Model\n    model = Model(inputs=input_layer, outputs=decoder_output)\n    # Compile the model with the specified learning rate\n    optimizer = Adam(learning_rate=learning_rate)\n    model.compile(optimizer=optimizer, loss='mean_squared_error')\n\n\n\n    return model\n\nmodel = InformerModel(input_shape=(look_back, n_features))\nmodel.compile(optimizer='adam', loss='mean_squared_error')\nmodel.summary()\n\n\n\nModel: \"model_10\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_12 (InputLayer)       [(None, 10, 4)]           0         \n                                                                 \n informer_encoder_11 (Infor  (None, 4)                 108480    \n merEncoder)                                                     \n                                                                 \n repeat_vector_10 (RepeatVe  (None, 4, 4)              0         \n ctor)                                                           \n                                                                 \n lstm_10 (LSTM)              (None, 4, 312)            395616    \n                                                                 \n tf.__operators__.getitem_1  (None, 312)               0         \n 0 (SlicingOpLambda)                                             \n                                                                 \n dense_82 (Dense)            (None, 4)                 1252      \n                                                                 \n=================================================================\nTotal params: 505348 (1.93 MB)\nTrainable params: 505348 (1.93 MB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n\n\nThe model.fit method trains the neural network using the provided training data (X_train and y_train) for a total of 50 epochs with mini-batches of 32 samples. During training, 20% of the training data is set aside for validation to monitor and prevent overfitting.\n\nhistory = model.fit(\n    X_train, y_train,\n    epochs=50,\n    batch_size=64,\n    validation_split=0.3,\n    shuffle=True\n)\n\nEpoch 1/50\n11/11 [==============================] - 7s 85ms/step - loss: 0.0148 - val_loss: 0.0845\nEpoch 2/50\n11/11 [==============================] - 0s 13ms/step - loss: 0.0050 - val_loss: 0.0753\nEpoch 3/50\n11/11 [==============================] - 0s 13ms/step - loss: 0.0020 - val_loss: 0.0373\nEpoch 4/50\n11/11 [==============================] - 0s 13ms/step - loss: 8.5089e-04 - val_loss: 0.0167\nEpoch 5/50\n11/11 [==============================] - 0s 13ms/step - loss: 3.6341e-04 - val_loss: 0.0052\nEpoch 6/50\n11/11 [==============================] - 0s 13ms/step - loss: 3.2985e-04 - val_loss: 0.0072\nEpoch 7/50\n11/11 [==============================] - 0s 13ms/step - loss: 3.6485e-04 - val_loss: 0.0032\nEpoch 8/50\n11/11 [==============================] - 0s 14ms/step - loss: 2.5527e-04 - val_loss: 0.0014\nEpoch 9/50\n11/11 [==============================] - 0s 19ms/step - loss: 2.0932e-04 - val_loss: 0.0027\nEpoch 10/50\n11/11 [==============================] - 0s 15ms/step - loss: 2.2858e-04 - val_loss: 0.0012\nEpoch 11/50\n11/11 [==============================] - 0s 12ms/step - loss: 2.3448e-04 - val_loss: 0.0026\nEpoch 12/50\n11/11 [==============================] - 0s 12ms/step - loss: 2.2099e-04 - val_loss: 0.0015\nEpoch 13/50\n11/11 [==============================] - 0s 13ms/step - loss: 1.9906e-04 - val_loss: 0.0023\nEpoch 14/50\n11/11 [==============================] - 0s 18ms/step - loss: 1.9555e-04 - val_loss: 0.0010\nEpoch 15/50\n11/11 [==============================] - 0s 16ms/step - loss: 2.4684e-04 - val_loss: 0.0081\nEpoch 16/50\n11/11 [==============================] - 0s 13ms/step - loss: 5.4148e-04 - val_loss: 0.0014\nEpoch 17/50\n11/11 [==============================] - 0s 13ms/step - loss: 1.9434e-04 - val_loss: 0.0015\nEpoch 18/50\n11/11 [==============================] - 0s 12ms/step - loss: 2.0086e-04 - val_loss: 0.0027\nEpoch 19/50\n11/11 [==============================] - 0s 13ms/step - loss: 2.2898e-04 - val_loss: 0.0047\nEpoch 20/50\n11/11 [==============================] - 0s 20ms/step - loss: 3.6830e-04 - val_loss: 0.0015\nEpoch 21/50\n11/11 [==============================] - 0s 13ms/step - loss: 2.1767e-04 - val_loss: 0.0013\nEpoch 22/50\n11/11 [==============================] - 0s 12ms/step - loss: 1.5870e-04 - val_loss: 0.0014\nEpoch 23/50\n11/11 [==============================] - 0s 12ms/step - loss: 1.4760e-04 - val_loss: 0.0016\nEpoch 24/50\n11/11 [==============================] - 0s 12ms/step - loss: 1.5468e-04 - val_loss: 0.0073\nEpoch 25/50\n11/11 [==============================] - 0s 12ms/step - loss: 5.8333e-04 - val_loss: 0.0073\nEpoch 26/50\n11/11 [==============================] - 0s 12ms/step - loss: 3.0529e-04 - val_loss: 0.0041\nEpoch 27/50\n11/11 [==============================] - 0s 13ms/step - loss: 3.8449e-04 - val_loss: 0.0018\nEpoch 28/50\n11/11 [==============================] - 0s 13ms/step - loss: 2.2575e-04 - val_loss: 0.0016\nEpoch 29/50\n11/11 [==============================] - 0s 12ms/step - loss: 1.5802e-04 - val_loss: 0.0012\nEpoch 30/50\n11/11 [==============================] - 0s 12ms/step - loss: 1.1718e-04 - val_loss: 0.0011\nEpoch 31/50\n11/11 [==============================] - 0s 13ms/step - loss: 1.2446e-04 - val_loss: 9.5531e-04\nEpoch 32/50\n11/11 [==============================] - 0s 12ms/step - loss: 1.0792e-04 - val_loss: 9.1374e-04\nEpoch 33/50\n11/11 [==============================] - 0s 13ms/step - loss: 1.1080e-04 - val_loss: 0.0012\nEpoch 34/50\n11/11 [==============================] - 0s 13ms/step - loss: 1.1769e-04 - val_loss: 7.9812e-04\nEpoch 35/50\n11/11 [==============================] - 0s 13ms/step - loss: 8.8748e-05 - val_loss: 6.6667e-04\nEpoch 36/50\n11/11 [==============================] - 0s 12ms/step - loss: 1.0898e-04 - val_loss: 7.7581e-04\nEpoch 37/50\n11/11 [==============================] - 0s 12ms/step - loss: 8.3250e-05 - val_loss: 0.0014\nEpoch 38/50\n11/11 [==============================] - 0s 12ms/step - loss: 1.5730e-04 - val_loss: 0.0015\nEpoch 39/50\n11/11 [==============================] - 0s 13ms/step - loss: 1.6502e-04 - val_loss: 8.1844e-04\nEpoch 40/50\n11/11 [==============================] - 0s 13ms/step - loss: 1.0669e-04 - val_loss: 8.3199e-04\nEpoch 41/50\n11/11 [==============================] - 0s 13ms/step - loss: 1.1308e-04 - val_loss: 0.0012\nEpoch 42/50\n11/11 [==============================] - 0s 12ms/step - loss: 9.3731e-05 - val_loss: 0.0024\nEpoch 43/50\n11/11 [==============================] - 0s 12ms/step - loss: 1.5355e-04 - val_loss: 0.0012\nEpoch 44/50\n11/11 [==============================] - 0s 13ms/step - loss: 1.1790e-04 - val_loss: 0.0011\nEpoch 45/50\n11/11 [==============================] - 0s 12ms/step - loss: 9.7926e-05 - val_loss: 8.3819e-04\nEpoch 46/50\n11/11 [==============================] - 0s 12ms/step - loss: 9.0219e-05 - val_loss: 0.0018\nEpoch 47/50\n11/11 [==============================] - 0s 12ms/step - loss: 1.7545e-04 - val_loss: 0.0079\nEpoch 48/50\n11/11 [==============================] - 0s 12ms/step - loss: 1.7695e-04 - val_loss: 0.0018\nEpoch 49/50\n11/11 [==============================] - 0s 12ms/step - loss: 1.4218e-04 - val_loss: 0.0028\nEpoch 50/50\n11/11 [==============================] - 0s 12ms/step - loss: 2.8367e-04 - val_loss: 0.0022\n\n\nThe code displays a visual representation of the model’s training and validation loss over the epochs using a line chart. The x-axis represents the number of epochs, while the y-axis indicates the mean squared error, allowing users to observe how the model’s performance evolves over time.\n\nplt.figure(figsize=(12, 6))\nplt.plot(history.history['loss'], label='Training Loss')\nplt.plot(history.history['val_loss'], label='Validation Loss')\nplt.title('Training and Validation Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Mean Squared Error')\nplt.legend()\nplt.show()\n\n\n\n\n\ntest_predictions = model.predict(X_test)\ntest_predictions = scaler.inverse_transform(test_predictions)\ntrue_values = scaler.inverse_transform(y_test)\n\nmse = mean_squared_error(true_values, test_predictions)\nprint(f\"Test MSE: {mse}\")\n\n15/15 [==============================] - 1s 3ms/step\nTest MSE: 462.6375895467179\n\n\n\nplt.figure(figsize=(12, 6))\nplt.plot(true_values, label='True Values')\nplt.plot(test_predictions, label='Predictions', alpha=0.6)\nplt.title('Test Set Predictions vs. True Values')\nplt.legend()\nplt.show()\n\n\n\n\n\n!pip install keras-tuner\n\nRequirement already satisfied: keras-tuner in /usr/local/lib/python3.10/dist-packages (1.4.4)\nRequirement already satisfied: keras-core in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (0.1.7)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (23.2)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (2.31.0)\nRequirement already satisfied: kt-legacy in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (1.0.5)\nRequirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras-core-&gt;keras-tuner) (1.4.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras-core-&gt;keras-tuner) (1.23.5)\nRequirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras-core-&gt;keras-tuner) (13.6.0)\nRequirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras-core-&gt;keras-tuner) (0.0.7)\nRequirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras-core-&gt;keras-tuner) (3.9.0)\nRequirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from keras-core-&gt;keras-tuner) (0.1.8)\nRequirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;keras-tuner) (3.3.0)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;keras-tuner) (3.4)\nRequirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;keras-tuner) (2.0.6)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;keras-tuner) (2023.7.22)\nRequirement already satisfied: markdown-it-py&gt;=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich-&gt;keras-core-&gt;keras-tuner) (3.0.0)\nRequirement already satisfied: pygments&lt;3.0.0,&gt;=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich-&gt;keras-core-&gt;keras-tuner) (2.16.1)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py&gt;=2.2.0-&gt;rich-&gt;keras-core-&gt;keras-tuner) (0.1.2)\n\n\nThe code defines a function to construct a neural network model using varying hyperparameters, aiming to optimize its architecture. Subsequently, the RandomSearch method from Keras Tuner is employed to explore 200 different model configurations, assessing their performance to determine the best hyperparameters that minimize the validation loss.\n\nfrom tensorflow.keras.layers import Input, RepeatVector, LSTM, Dense\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nimport tensorflow as tf\nimport kerastuner as kt\n\ndef build_model(hp):\n    # Input\n    input_layer = Input(shape=(look_back, n_features))\n\n    # Encoder\n    encoder_output = InformerEncoder(d_model=hp.Int('d_model', min_value=32, max_value=512, step=16),\n                                     num_heads=hp.Int('num_heads', 2, 8, step=2),\n                                     conv_filters=hp.Int('conv_filters', min_value=16, max_value=256, step=16))(input_layer)\n\n    # Decoder\n    repeated_output = RepeatVector(4)(encoder_output)  # Repeating encoder's output\n    decoder_lstm = LSTM(312, return_sequences=True)(repeated_output)\n    decoder_output = Dense(4)(decoder_lstm[:, -1, :])  # Use the last sequence output to predict the next value\n\n    # Model\n    model = Model(inputs=input_layer, outputs=decoder_output)\n\n    # Compile the model with the specified learning rate\n    optimizer = Adam(learning_rate=hp.Choice('learning_rate', [1e-3, 1e-2, 1e-1]))\n    model.compile(optimizer=optimizer, loss='mean_squared_error')\n\n    return model\n\n# Define the tuner\ntuner = kt.RandomSearch(\n    build_model,\n    objective='val_loss',\n    max_trials=30,\n    executions_per_trial=5,\n    directory='hyperparam_search',\n    project_name='informer_model'\n)\n\nThe code sets up two training callbacks: one for early stopping if validation loss doesn’t improve after 10 epochs, and another to save the model weights at their best performance. With these callbacks, the tuner conducts a search over the hyperparameter space using the training data, and evaluates model configurations over 100 epochs, saving the most optimal weights and potentially halting early if improvements stagnate.\n\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n\nearly_stopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1, restore_best_weights=True)\nmodel_checkpoint = ModelCheckpoint(filepath='trial_best.h5', monitor='val_loss', verbose=1, save_best_only=True)\n\ntuner.search(X_train, y_train,\n             epochs=100,\n             validation_split=0.2,\n             callbacks=[early_stopping, model_checkpoint])\n\nTrial 30 Complete [00h 00m 01s]\n\nBest val_loss So Far: 0.0009468139498494566\nTotal elapsed time: 00h 30m 43s\n\n\n\n# Get the best hyperparameters\nbest_hp = tuner.get_best_hyperparameters()[0]\n\n# Retrieve the best model\nbest_model = tuner.get_best_models()[0]\nbest_model.summary()\n\nModel: \"model\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_1 (InputLayer)        [(None, 10, 4)]           0         \n                                                                 \n informer_encoder (Informer  (None, 4)                 108480    \n Encoder)                                                        \n                                                                 \n repeat_vector (RepeatVecto  (None, 4, 4)              0         \n r)                                                              \n                                                                 \n lstm (LSTM)                 (None, 4, 312)            395616    \n                                                                 \n tf.__operators__.getitem (  (None, 312)               0         \n SlicingOpLambda)                                                \n                                                                 \n dense_6 (Dense)             (None, 4)                 1252      \n                                                                 \n=================================================================\nTotal params: 505348 (1.93 MB)\nTrainable params: 505348 (1.93 MB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n\n\n\ntest_loss = best_model.evaluate(X_test, y_test)\nprint(f\"Test MSE: {test_loss}\")\n\n15/15 [==============================] - 0s 3ms/step - loss: 0.0086\nTest MSE: 0.008609036915004253\n\n\n\nplt.figure(figsize=(20, 12))\nfor i in range(true_values.shape[1]):\n    plt.subplot(2, 2, i+1)\n    plt.plot(true_values[:, i], label='True Values', color='blue')\n    plt.plot(test_predictions[:, i], label='Predictions', color='red', linestyle='--')\n    plt.title(f\"Feature {i+1}\")\n    plt.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nfrom sklearn.metrics import mean_absolute_error\nmae = mean_absolute_error(true_values, test_predictions)\nrmse = np.sqrt(test_loss)  # since loss is MSE\nprint(f\"MAE: {mae}, RMSE: {rmse}\")\n\nMAE: 19.377517553476185, RMSE: 0.09278489594219662\n\n\n\nprint(f\"Best d_model: {best_hp.get('d_model')}\")\nprint(f\"Best num_heads: {best_hp.get('num_heads')}\")\nprint(f\"Best conv_filters: {best_hp.get('conv_filters')}\")\nprint(f\"Best learning_rate: {best_hp.get('learning_rate')}\")\n\nBest d_model: 64\nBest num_heads: 2\nBest conv_filters: 256\nBest learning_rate: 0.001\n\n\n\n!pip install shap\nimport shap\n\n\n# The reference can be a dataset or just random data\nbackground = X_train[np.random.choice(X_train.shape[0], 300, replace=False)]  # Taking a random sample of the training data as background\nexplainer = shap.GradientExplainer(best_model, background)\n\n\nshap_values = explainer.shap_values(X_test[:300])  # Computing for a subset for performance reasons\n\n\nfor timestep in range(10):\n    print(f\"Summary plot for timestep {timestep + 1}\")\n    shap.summary_plot(shap_values[0][:, timestep, :], X_test[:300, timestep, :])\n\nSummary plot for timestep 1\nSummary plot for timestep 2\nSummary plot for timestep 3\nSummary plot for timestep 4\nSummary plot for timestep 5\nSummary plot for timestep 6\nSummary plot for timestep 7\nSummary plot for timestep 8\nSummary plot for timestep 9\nSummary plot for timestep 10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Shapley values\nSHAP values are indicating by how much the presence of a particular feature influenced the model’s prediction, compared to if that feature was absent. The color represents the actual value of the feature itself. In the context of the present work, Shapley values are employed as a method to enhance the interpretability of the model. Shapley values provide insights into the contribution of each feature to the model’s predictions. Specifically, they quantify how each feature influences the prediction by evaluating its impact when combined with all other features. By calculating Shapley values, we gain a clear understanding of the relative importance of each feature in multivariate time series prediction.\nOverall, Shapley values enhance model interpretability by offering a systematic and quantitative way to dissect complex models and understand their decision-making processes. They enable us to identify which features are the key drivers of predictions, helping us make more informed decisions and potentially improving model performance. This interpretability is crucial in various applications, from finance to healthcare, where understanding the factors influencing predictions is paramount for trust and decision-making.\n\n!jupyter nbconvert Multivariate_multistep_Informer.ipynb --to markdown --NbConvertApp.output_files_dir=.\n\n[NbConvertApp] Converting notebook Multivariate_multistep_Informer.ipynb to markdown\n[NbConvertApp] Support files will be in ./\n[NbConvertApp] Writing 31740 bytes to Multivariate_multistep_Informer.md\n\n\n\n!cat covid_analysis.md | tee -a index.md\n\ncat: covid_analysis.md: No such file or directory\n\n\n\n!cat Multivariate_multistep_Informer.md | tee -a index.md\n\n```python\n!pip install yfinance\n```\n\n    Requirement already satisfied: yfinance in /home/deepak/anaconda3/lib/python3.7/site-packages (0.1.81)\n    Requirement already satisfied: pandas&gt;=0.24.0 in /home/deepak/anaconda3/lib/python3.7/site-packages (from yfinance) (1.3.5)\n    Requirement already satisfied: numpy&gt;=1.15 in /home/deepak/anaconda3/lib/python3.7/site-packages (from yfinance) (1.21.5)\n    Requirement already satisfied: requests&gt;=2.26 in /home/deepak/anaconda3/lib/python3.7/site-packages (from yfinance) (2.31.0)\n    Requirement already satisfied: multitasking&gt;=0.0.7 in /home/deepak/anaconda3/lib/python3.7/site-packages (from yfinance) (0.0.11)\n    Requirement already satisfied: lxml&gt;=4.5.1 in /home/deepak/anaconda3/lib/python3.7/site-packages (from yfinance) (4.9.1)\n    Requirement already satisfied: appdirs&gt;=1.4.4 in /home/deepak/anaconda3/lib/python3.7/site-packages (from yfinance) (1.4.4)\n    Requirement already satisfied: python-dateutil&gt;=2.7.3 in /home/deepak/anaconda3/lib/python3.7/site-packages (from pandas&gt;=0.24.0-&gt;yfinance) (2.8.2)\n    Requirement already satisfied: pytz&gt;=2017.3 in /home/deepak/anaconda3/lib/python3.7/site-packages (from pandas&gt;=0.24.0-&gt;yfinance) (2023.3)\n    Requirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /home/deepak/anaconda3/lib/python3.7/site-packages (from requests&gt;=2.26-&gt;yfinance) (2.1.1)\n    Requirement already satisfied: idna&lt;4,&gt;=2.5 in /home/deepak/anaconda3/lib/python3.7/site-packages (from requests&gt;=2.26-&gt;yfinance) (3.4)\n    Requirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /home/deepak/anaconda3/lib/python3.7/site-packages (from requests&gt;=2.26-&gt;yfinance) (2.0.4)\n    Requirement already satisfied: certifi&gt;=2017.4.17 in /home/deepak/anaconda3/lib/python3.7/site-packages (from requests&gt;=2.26-&gt;yfinance) (2023.7.22)\n    Requirement already satisfied: six&gt;=1.5 in /home/deepak/anaconda3/lib/python3.7/site-packages (from python-dateutil&gt;=2.7.3-&gt;pandas&gt;=0.24.0-&gt;yfinance) (1.16.0)\n\n\n\n```python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\nimport tensorflow as tf\nimport yfinance as yf\n```\n\n\n```python\n# Download historical data as dataframe\ndata = yf.download(\"AAPL\", start=\"2018-01-01\", end=\"2023-09-01\")\n\n```\n\n    [*********************100%***********************]  1 of 1 completed\n    \n    1 Failed download:\n    - AAPL: No data found for this date range, symbol may be delisted\n\n\n# Informer\n\nThe Informer model variant is designed for multivariate prediction. Let's consider 'Open', 'High', 'Low', and 'Close' prices for simplicity. The provided code is designed to fetch and preprocess historical stock prices for Apple Inc. for the purpose of multivariate time series forecasting using an LSTM model. Initially, the code downloads Apple's stock data, specifically capturing four significant features: Open, High, Low, and Close prices. To make the data suitable for deep learning models, it is normalized to fit within a range of 0 to 1. The sequential data is then transformed into a format suitable for supervised learning, where the data from the past `look_back` days is used to predict the next day's features. Finally, the data is partitioned into training (67%) and test sets, ensuring separate datasets for model training and evaluation.\n\n\n\n```python\n# Using multiple columns for multivariate prediction\ndf = data[[\"Open\", \"High\", \"Low\", \"Close\"]]\n\n# Normalize the data\nscaler = MinMaxScaler(feature_range=(0, 1))\ndf_scaled = scaler.fit_transform(df)\n\n# Prepare data for LSTM\nlook_back = 10  # Number of previous time steps to use as input variables\nn_features = df.shape[1]  # number of features\n\n# Convert to supervised learning problem\nX, y = [], []\nfor i in range(len(df_scaled) - look_back):\n    X.append(df_scaled[i:i+look_back, :])\n    y.append(df_scaled[i + look_back, :])\nX, y = np.array(X), np.array(y)\n\n# Reshape input to be [samples, time steps, features]\nX = np.reshape(X, (X.shape[0], X.shape[1], n_features))\n\n# Train-test split\ntrain_size = int(len(X) * 0.67)\ntest_size = len(X) - train_size\nX_train, X_test = X[0:train_size], X[train_size:]\ny_train, y_test = y[0:train_size], y[train_size:]\n\nprint(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n```\n\n\n    ---------------------------------------------------------------------------\n\n    NameError                                 Traceback (most recent call last)\n\n    /tmp/ipykernel_584257/995404858.py in &lt;module&gt;\n          1 # Using multiple columns for multivariate prediction\n    ----&gt; 2 df = data[[\"Open\", \"High\", \"Low\", \"Close\"]]\n          3 \n          4 # Normalize the data\n          5 scaler = MinMaxScaler(feature_range=(0, 1))\n\n\n    NameError: name 'data' is not defined\n\n\n# ProbSparse Self-Attention:\n\nSelf-attention involves computing a weighted sum of all values in the sequence, based on the dot product between the query and key. In ProbSparse, we don't compute this for all query-key pairs, but rather select dominant ones, thus making the computation more efficient. The given code defines a custom Keras layer, `ProbSparseSelfAttention`, which implements the multi-head self-attention mechanism, a critical component of Transformer models. This layer initializes three dense networks for the Query, Key, and Value matrices, and splits the input data into multiple heads to enable parallel processing. During the forward pass (`call` method), the Query, Key, and Value matrices are calculated, scaled, and then used to compute attention scores. These scores indicate the importance of each element in the sequence when predicting another element. The output is a weighted sum of the input values, which is then passed through another dense layer to produce the final result.\n\n\n\n```python\nclass ProbSparseSelfAttention(tf.keras.layers.Layer):\n    def __init__(self, d_model, num_heads, **kwargs):\n        super(ProbSparseSelfAttention, self).__init__(**kwargs)\n        self.num_heads = num_heads\n        self.d_model = d_model\n\n        # Assert that d_model is divisible by num_heads\n        assert self.d_model % self.num_heads == 0, f\"d_model ({d_model}) must be divisible by num_heads ({num_heads})\"\n\n        self.depth = d_model // self.num_heads\n\n        # Defining the dense layers for Query, Key and Value\n        self.wq = tf.keras.layers.Dense(d_model)\n        self.wk = tf.keras.layers.Dense(d_model)\n        self.wv = tf.keras.layers.Dense(d_model)\n\n        self.dense = tf.keras.layers.Dense(d_model)\n\n    def split_heads(self, x, batch_size):\n        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n        return tf.transpose(x, perm=[0, 2, 1, 3])\n\n    def call(self, v, k, q):\n        batch_size = tf.shape(q)[0]\n\n        q = self.wq(q)\n        k = self.wk(k)\n        v = self.wv(v)\n\n        q = self.split_heads(q, batch_size)\n        k = self.split_heads(k, batch_size)\n        v = self.split_heads(v, batch_size)\n\n        # Fixing matrix multiplication\n        matmul_qk = tf.matmul(q, k, transpose_b=True)\n\n        d_k = tf.cast(self.depth, tf.float32)\n        scaled_attention_logits = matmul_qk / tf.math.sqrt(d_k)\n\n        attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n        output = tf.matmul(attention_weights, v)\n\n        output = tf.transpose(output, perm=[0, 2, 1, 3])\n        concat_attention = tf.reshape(output, (batch_size, -1, self.d_model))\n\n        return self.dense(concat_attention)\n\n```\n\n# Informer Encoder:\n\nThe `InformerEncoder` is a custom Keras layer designed to process sequential data using a combination of attention and convolutional mechanisms. Within the encoder, the input data undergoes multi-head self-attention, utilizing the `ProbSparseSelfAttention` mechanism, to capture relationships in the data regardless of their distance. Post attention, the data is transformed and normalized, then further processed using two 1D convolutional layers, emphasizing local features in the data. After another normalization step, the processed data is pooled to a lower dimensionality, ensuring the model captures global context, and then passed through a dense layer to produce the final output.\n\n\n\n```python\nclass InformerEncoder(tf.keras.layers.Layer):\n    def __init__(self, d_model, num_heads, conv_filters, **kwargs):\n        super(InformerEncoder, self).__init__(**kwargs)\n        self.d_model = d_model\n        self.num_heads = num_heads\n\n        # Assert that d_model is divisible by num_heads\n        assert self.d_model % self.num_heads == 0, f\"d_model ({d_model}) must be divisible by num_heads ({num_heads})\"\n\n        self.self_attention = ProbSparseSelfAttention(d_model=d_model, num_heads=num_heads)\n\n        self.norm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n\n        # This dense layer will transform the input 'x' to have the dimensionality 'd_model'\n        self.dense_transform = tf.keras.layers.Dense(d_model)\n\n        self.conv1 = tf.keras.layers.Conv1D(conv_filters, 3, padding='same')\n        self.conv2 = tf.keras.layers.Conv1D(d_model, 3, padding='same')\n        self.norm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n\n        self.global_avg_pooling = tf.keras.layers.GlobalAveragePooling1D()\n        self.dense = tf.keras.layers.Dense(d_model)\n\n    def call(self, x):\n        attn_output = self.self_attention(x, x, x)\n\n        # Transform 'x' to have the desired dimensionality\n        x_transformed = self.dense_transform(x)\n        attn_output = self.norm1(attn_output + x_transformed)\n\n        conv_output = self.conv1(attn_output)\n        conv_output = tf.nn.relu(conv_output)\n        conv_output = self.conv2(conv_output)\n\n        encoded_output = self.norm2(conv_output + attn_output)\n\n        pooled_output = self.global_avg_pooling(encoded_output)\n        return self.dense(pooled_output)[:, -4:]\n\n\n\n```\n\n\n```python\ninput_layer = tf.keras.layers.Input(shape=(look_back, n_features))\n\n# Encoder\nencoder_output = InformerEncoder(d_model=360, num_heads=8, conv_filters=64)(input_layer)\n\n# Decoder (with attention)\ndecoder_lstm = tf.keras.layers\n```\n\nThe `InformerModel` function is designed to create a deep learning architecture tailored for sequential data prediction. It takes an input sequence and processes it using the `InformerEncoder`, a custom encoder layer, which captures both local and global patterns in the data. Following the encoding step, a decoder structure unravels the encoded data by first repeating the encoder's output, then passing it through an LSTM layer to retain sequential dependencies, and finally making predictions using a dense layer. The resulting architecture is then compiled with the Adam optimizer and Mean Squared Error loss, ready for training on time series data.\n\n\n\n```python\nfrom tensorflow.keras.layers import RepeatVector\nfrom tensorflow.keras.layers import Input\nfrom tensorflow.keras.layers import LSTM\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\n\n\ndef InformerModel(input_shape, d_model=64, num_heads=2, conv_filters=256, learning_rate= 1e-3):\n    # Input\n    input_layer = Input(shape=input_shape)\n\n    # Encoder\n    encoder_output = InformerEncoder(d_model=d_model, num_heads=num_heads, conv_filters=conv_filters)(input_layer)\n\n    # Decoder\n    repeated_output = RepeatVector(4)(encoder_output)  # Repeating encoder's output\n    decoder_lstm = LSTM(312, return_sequences=True)(repeated_output)\n    decoder_output = Dense(4)(decoder_lstm[:, -1, :])  # Use the last sequence output to predict the next value\n\n    # Model\n    model = Model(inputs=input_layer, outputs=decoder_output)\n    # Compile the model with the specified learning rate\n    optimizer = Adam(learning_rate=learning_rate)\n    model.compile(optimizer=optimizer, loss='mean_squared_error')\n\n\n\n    return model\n\nmodel = InformerModel(input_shape=(look_back, n_features))\nmodel.compile(optimizer='adam', loss='mean_squared_error')\nmodel.summary()\n\n\n\n```\n\n    Model: \"model_10\"\n    _________________________________________________________________\n     Layer (type)                Output Shape              Param #   \n    =================================================================\n     input_12 (InputLayer)       [(None, 10, 4)]           0         \n                                                                     \n     informer_encoder_11 (Infor  (None, 4)                 108480    \n     merEncoder)                                                     \n                                                                     \n     repeat_vector_10 (RepeatVe  (None, 4, 4)              0         \n     ctor)                                                           \n                                                                     \n     lstm_10 (LSTM)              (None, 4, 312)            395616    \n                                                                     \n     tf.__operators__.getitem_1  (None, 312)               0         \n     0 (SlicingOpLambda)                                             \n                                                                     \n     dense_82 (Dense)            (None, 4)                 1252      \n                                                                     \n    =================================================================\n    Total params: 505348 (1.93 MB)\n    Trainable params: 505348 (1.93 MB)\n    Non-trainable params: 0 (0.00 Byte)\n    _________________________________________________________________\n\n\nThe `model.fit` method trains the neural network using the provided training data (`X_train` and `y_train`) for a total of 50 epochs with mini-batches of 32 samples. During training, 20% of the training data is set aside for validation to monitor and prevent overfitting.\n\n\n\n```python\nhistory = model.fit(\n    X_train, y_train,\n    epochs=50,\n    batch_size=64,\n    validation_split=0.3,\n    shuffle=True\n)\n\n```\n\n    Epoch 1/50\n    11/11 [==============================] - 7s 85ms/step - loss: 0.0148 - val_loss: 0.0845\n    Epoch 2/50\n    11/11 [==============================] - 0s 13ms/step - loss: 0.0050 - val_loss: 0.0753\n    Epoch 3/50\n    11/11 [==============================] - 0s 13ms/step - loss: 0.0020 - val_loss: 0.0373\n    Epoch 4/50\n    11/11 [==============================] - 0s 13ms/step - loss: 8.5089e-04 - val_loss: 0.0167\n    Epoch 5/50\n    11/11 [==============================] - 0s 13ms/step - loss: 3.6341e-04 - val_loss: 0.0052\n    Epoch 6/50\n    11/11 [==============================] - 0s 13ms/step - loss: 3.2985e-04 - val_loss: 0.0072\n    Epoch 7/50\n    11/11 [==============================] - 0s 13ms/step - loss: 3.6485e-04 - val_loss: 0.0032\n    Epoch 8/50\n    11/11 [==============================] - 0s 14ms/step - loss: 2.5527e-04 - val_loss: 0.0014\n    Epoch 9/50\n    11/11 [==============================] - 0s 19ms/step - loss: 2.0932e-04 - val_loss: 0.0027\n    Epoch 10/50\n    11/11 [==============================] - 0s 15ms/step - loss: 2.2858e-04 - val_loss: 0.0012\n    Epoch 11/50\n    11/11 [==============================] - 0s 12ms/step - loss: 2.3448e-04 - val_loss: 0.0026\n    Epoch 12/50\n    11/11 [==============================] - 0s 12ms/step - loss: 2.2099e-04 - val_loss: 0.0015\n    Epoch 13/50\n    11/11 [==============================] - 0s 13ms/step - loss: 1.9906e-04 - val_loss: 0.0023\n    Epoch 14/50\n    11/11 [==============================] - 0s 18ms/step - loss: 1.9555e-04 - val_loss: 0.0010\n    Epoch 15/50\n    11/11 [==============================] - 0s 16ms/step - loss: 2.4684e-04 - val_loss: 0.0081\n    Epoch 16/50\n    11/11 [==============================] - 0s 13ms/step - loss: 5.4148e-04 - val_loss: 0.0014\n    Epoch 17/50\n    11/11 [==============================] - 0s 13ms/step - loss: 1.9434e-04 - val_loss: 0.0015\n    Epoch 18/50\n    11/11 [==============================] - 0s 12ms/step - loss: 2.0086e-04 - val_loss: 0.0027\n    Epoch 19/50\n    11/11 [==============================] - 0s 13ms/step - loss: 2.2898e-04 - val_loss: 0.0047\n    Epoch 20/50\n    11/11 [==============================] - 0s 20ms/step - loss: 3.6830e-04 - val_loss: 0.0015\n    Epoch 21/50\n    11/11 [==============================] - 0s 13ms/step - loss: 2.1767e-04 - val_loss: 0.0013\n    Epoch 22/50\n    11/11 [==============================] - 0s 12ms/step - loss: 1.5870e-04 - val_loss: 0.0014\n    Epoch 23/50\n    11/11 [==============================] - 0s 12ms/step - loss: 1.4760e-04 - val_loss: 0.0016\n    Epoch 24/50\n    11/11 [==============================] - 0s 12ms/step - loss: 1.5468e-04 - val_loss: 0.0073\n    Epoch 25/50\n    11/11 [==============================] - 0s 12ms/step - loss: 5.8333e-04 - val_loss: 0.0073\n    Epoch 26/50\n    11/11 [==============================] - 0s 12ms/step - loss: 3.0529e-04 - val_loss: 0.0041\n    Epoch 27/50\n    11/11 [==============================] - 0s 13ms/step - loss: 3.8449e-04 - val_loss: 0.0018\n    Epoch 28/50\n    11/11 [==============================] - 0s 13ms/step - loss: 2.2575e-04 - val_loss: 0.0016\n    Epoch 29/50\n    11/11 [==============================] - 0s 12ms/step - loss: 1.5802e-04 - val_loss: 0.0012\n    Epoch 30/50\n    11/11 [==============================] - 0s 12ms/step - loss: 1.1718e-04 - val_loss: 0.0011\n    Epoch 31/50\n    11/11 [==============================] - 0s 13ms/step - loss: 1.2446e-04 - val_loss: 9.5531e-04\n    Epoch 32/50\n    11/11 [==============================] - 0s 12ms/step - loss: 1.0792e-04 - val_loss: 9.1374e-04\n    Epoch 33/50\n    11/11 [==============================] - 0s 13ms/step - loss: 1.1080e-04 - val_loss: 0.0012\n    Epoch 34/50\n    11/11 [==============================] - 0s 13ms/step - loss: 1.1769e-04 - val_loss: 7.9812e-04\n    Epoch 35/50\n    11/11 [==============================] - 0s 13ms/step - loss: 8.8748e-05 - val_loss: 6.6667e-04\n    Epoch 36/50\n    11/11 [==============================] - 0s 12ms/step - loss: 1.0898e-04 - val_loss: 7.7581e-04\n    Epoch 37/50\n    11/11 [==============================] - 0s 12ms/step - loss: 8.3250e-05 - val_loss: 0.0014\n    Epoch 38/50\n    11/11 [==============================] - 0s 12ms/step - loss: 1.5730e-04 - val_loss: 0.0015\n    Epoch 39/50\n    11/11 [==============================] - 0s 13ms/step - loss: 1.6502e-04 - val_loss: 8.1844e-04\n    Epoch 40/50\n    11/11 [==============================] - 0s 13ms/step - loss: 1.0669e-04 - val_loss: 8.3199e-04\n    Epoch 41/50\n    11/11 [==============================] - 0s 13ms/step - loss: 1.1308e-04 - val_loss: 0.0012\n    Epoch 42/50\n    11/11 [==============================] - 0s 12ms/step - loss: 9.3731e-05 - val_loss: 0.0024\n    Epoch 43/50\n    11/11 [==============================] - 0s 12ms/step - loss: 1.5355e-04 - val_loss: 0.0012\n    Epoch 44/50\n    11/11 [==============================] - 0s 13ms/step - loss: 1.1790e-04 - val_loss: 0.0011\n    Epoch 45/50\n    11/11 [==============================] - 0s 12ms/step - loss: 9.7926e-05 - val_loss: 8.3819e-04\n    Epoch 46/50\n    11/11 [==============================] - 0s 12ms/step - loss: 9.0219e-05 - val_loss: 0.0018\n    Epoch 47/50\n    11/11 [==============================] - 0s 12ms/step - loss: 1.7545e-04 - val_loss: 0.0079\n    Epoch 48/50\n    11/11 [==============================] - 0s 12ms/step - loss: 1.7695e-04 - val_loss: 0.0018\n    Epoch 49/50\n    11/11 [==============================] - 0s 12ms/step - loss: 1.4218e-04 - val_loss: 0.0028\n    Epoch 50/50\n    11/11 [==============================] - 0s 12ms/step - loss: 2.8367e-04 - val_loss: 0.0022\n\n\nThe code displays a visual representation of the model's training and validation loss over the epochs using a line chart. The x-axis represents the number of epochs, while the y-axis indicates the mean squared error, allowing users to observe how the model's performance evolves over time.\n\n\n\n```python\nplt.figure(figsize=(12, 6))\nplt.plot(history.history['loss'], label='Training Loss')\nplt.plot(history.history['val_loss'], label='Validation Loss')\nplt.title('Training and Validation Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Mean Squared Error')\nplt.legend()\nplt.show()\n\n```\n\n\n    \n![png](./Multivariate_multistep_Informer_15_0.png)\n    \n\n\n\n```python\ntest_predictions = model.predict(X_test)\ntest_predictions = scaler.inverse_transform(test_predictions)\ntrue_values = scaler.inverse_transform(y_test)\n\nmse = mean_squared_error(true_values, test_predictions)\nprint(f\"Test MSE: {mse}\")\n\n```\n\n    15/15 [==============================] - 1s 3ms/step\n    Test MSE: 462.6375895467179\n\n\n\n```python\nplt.figure(figsize=(12, 6))\nplt.plot(true_values, label='True Values')\nplt.plot(test_predictions, label='Predictions', alpha=0.6)\nplt.title('Test Set Predictions vs. True Values')\nplt.legend()\nplt.show()\n\n```\n\n\n    \n![png](./Multivariate_multistep_Informer_17_0.png)\n    \n\n\n\n```python\n!pip install keras-tuner\n\n```\n\n    Requirement already satisfied: keras-tuner in /usr/local/lib/python3.10/dist-packages (1.4.4)\n    Requirement already satisfied: keras-core in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (0.1.7)\n    Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (23.2)\n    Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (2.31.0)\n    Requirement already satisfied: kt-legacy in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (1.0.5)\n    Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras-core-&gt;keras-tuner) (1.4.0)\n    Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras-core-&gt;keras-tuner) (1.23.5)\n    Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras-core-&gt;keras-tuner) (13.6.0)\n    Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras-core-&gt;keras-tuner) (0.0.7)\n    Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras-core-&gt;keras-tuner) (3.9.0)\n    Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from keras-core-&gt;keras-tuner) (0.1.8)\n    Requirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;keras-tuner) (3.3.0)\n    Requirement already satisfied: idna&lt;4,&gt;=2.5 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;keras-tuner) (3.4)\n    Requirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;keras-tuner) (2.0.6)\n    Requirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;keras-tuner) (2023.7.22)\n    Requirement already satisfied: markdown-it-py&gt;=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich-&gt;keras-core-&gt;keras-tuner) (3.0.0)\n    Requirement already satisfied: pygments&lt;3.0.0,&gt;=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich-&gt;keras-core-&gt;keras-tuner) (2.16.1)\n    Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py&gt;=2.2.0-&gt;rich-&gt;keras-core-&gt;keras-tuner) (0.1.2)\n\n\nThe code defines a function to construct a neural network model using varying hyperparameters, aiming to optimize its architecture. Subsequently, the RandomSearch method from Keras Tuner is employed to explore 200 different model configurations, assessing their performance to determine the best hyperparameters that minimize the validation loss.\n\n\n\n```python\nfrom tensorflow.keras.layers import Input, RepeatVector, LSTM, Dense\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nimport tensorflow as tf\nimport kerastuner as kt\n\ndef build_model(hp):\n    # Input\n    input_layer = Input(shape=(look_back, n_features))\n\n    # Encoder\n    encoder_output = InformerEncoder(d_model=hp.Int('d_model', min_value=32, max_value=512, step=16),\n                                     num_heads=hp.Int('num_heads', 2, 8, step=2),\n                                     conv_filters=hp.Int('conv_filters', min_value=16, max_value=256, step=16))(input_layer)\n\n    # Decoder\n    repeated_output = RepeatVector(4)(encoder_output)  # Repeating encoder's output\n    decoder_lstm = LSTM(312, return_sequences=True)(repeated_output)\n    decoder_output = Dense(4)(decoder_lstm[:, -1, :])  # Use the last sequence output to predict the next value\n\n    # Model\n    model = Model(inputs=input_layer, outputs=decoder_output)\n\n    # Compile the model with the specified learning rate\n    optimizer = Adam(learning_rate=hp.Choice('learning_rate', [1e-3, 1e-2, 1e-1]))\n    model.compile(optimizer=optimizer, loss='mean_squared_error')\n\n    return model\n\n# Define the tuner\ntuner = kt.RandomSearch(\n    build_model,\n    objective='val_loss',\n    max_trials=30,\n    executions_per_trial=5,\n    directory='hyperparam_search',\n    project_name='informer_model'\n)\n```\n\nThe code sets up two training callbacks: one for early stopping if validation loss doesn't improve after 10 epochs, and another to save the model weights at their best performance. With these callbacks, the tuner conducts a search over the hyperparameter space using the training data, and evaluates model configurations over 100 epochs, saving the most optimal weights and potentially halting early if improvements stagnate.\n\n\n\n```python\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n\nearly_stopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1, restore_best_weights=True)\nmodel_checkpoint = ModelCheckpoint(filepath='trial_best.h5', monitor='val_loss', verbose=1, save_best_only=True)\n\ntuner.search(X_train, y_train,\n             epochs=100,\n             validation_split=0.2,\n             callbacks=[early_stopping, model_checkpoint])\n\n```\n\n    Trial 30 Complete [00h 00m 01s]\n    \n    Best val_loss So Far: 0.0009468139498494566\n    Total elapsed time: 00h 30m 43s\n\n\n\n```python\n# Get the best hyperparameters\nbest_hp = tuner.get_best_hyperparameters()[0]\n\n# Retrieve the best model\nbest_model = tuner.get_best_models()[0]\nbest_model.summary()\n\n```\n\n    Model: \"model\"\n    _________________________________________________________________\n     Layer (type)                Output Shape              Param #   \n    =================================================================\n     input_1 (InputLayer)        [(None, 10, 4)]           0         \n                                                                     \n     informer_encoder (Informer  (None, 4)                 108480    \n     Encoder)                                                        \n                                                                     \n     repeat_vector (RepeatVecto  (None, 4, 4)              0         \n     r)                                                              \n                                                                     \n     lstm (LSTM)                 (None, 4, 312)            395616    \n                                                                     \n     tf.__operators__.getitem (  (None, 312)               0         \n     SlicingOpLambda)                                                \n                                                                     \n     dense_6 (Dense)             (None, 4)                 1252      \n                                                                     \n    =================================================================\n    Total params: 505348 (1.93 MB)\n    Trainable params: 505348 (1.93 MB)\n    Non-trainable params: 0 (0.00 Byte)\n    _________________________________________________________________\n\n\n\n```python\ntest_loss = best_model.evaluate(X_test, y_test)\nprint(f\"Test MSE: {test_loss}\")\n\n```\n\n    15/15 [==============================] - 0s 3ms/step - loss: 0.0086\n    Test MSE: 0.008609036915004253\n\n\n\n```python\nplt.figure(figsize=(20, 12))\nfor i in range(true_values.shape[1]):\n    plt.subplot(2, 2, i+1)\n    plt.plot(true_values[:, i], label='True Values', color='blue')\n    plt.plot(test_predictions[:, i], label='Predictions', color='red', linestyle='--')\n    plt.title(f\"Feature {i+1}\")\n    plt.legend()\nplt.tight_layout()\nplt.show()\n\n```\n\n\n    \n![png](./Multivariate_multistep_Informer_25_0.png)\n    \n\n\n\n```python\nfrom sklearn.metrics import mean_absolute_error\nmae = mean_absolute_error(true_values, test_predictions)\nrmse = np.sqrt(test_loss)  # since loss is MSE\nprint(f\"MAE: {mae}, RMSE: {rmse}\")\n\n```\n\n    MAE: 19.377517553476185, RMSE: 0.09278489594219662\n\n\n\n```python\nprint(f\"Best d_model: {best_hp.get('d_model')}\")\nprint(f\"Best num_heads: {best_hp.get('num_heads')}\")\nprint(f\"Best conv_filters: {best_hp.get('conv_filters')}\")\nprint(f\"Best learning_rate: {best_hp.get('learning_rate')}\")\n\n```\n\n    Best d_model: 64\n    Best num_heads: 2\n    Best conv_filters: 256\n    Best learning_rate: 0.001\n\n\n\n```python\n!pip install shap\nimport shap\n\n```\n\n\n```python\n# The reference can be a dataset or just random data\nbackground = X_train[np.random.choice(X_train.shape[0], 300, replace=False)]  # Taking a random sample of the training data as background\nexplainer = shap.GradientExplainer(best_model, background)\n\n```\n\n\n```python\nshap_values = explainer.shap_values(X_test[:300])  # Computing for a subset for performance reasons\n\n```\n\n\n```python\nfor timestep in range(10):\n    print(f\"Summary plot for timestep {timestep + 1}\")\n    shap.summary_plot(shap_values[0][:, timestep, :], X_test[:300, timestep, :])\n\n```\n\n    Summary plot for timestep 1\n\n\n\n    \n![png](./Multivariate_multistep_Informer_31_1.png)\n    \n\n\n    Summary plot for timestep 2\n\n\n\n    \n![png](./Multivariate_multistep_Informer_31_3.png)\n    \n\n\n    Summary plot for timestep 3\n\n\n\n    \n![png](./Multivariate_multistep_Informer_31_5.png)\n    \n\n\n    Summary plot for timestep 4\n\n\n\n    \n![png](./Multivariate_multistep_Informer_31_7.png)\n    \n\n\n    Summary plot for timestep 5\n\n\n\n    \n![png](./Multivariate_multistep_Informer_31_9.png)\n    \n\n\n    Summary plot for timestep 6\n\n\n\n    \n![png](./Multivariate_multistep_Informer_31_11.png)\n    \n\n\n    Summary plot for timestep 7\n\n\n\n    \n![png](./Multivariate_multistep_Informer_31_13.png)\n    \n\n\n    Summary plot for timestep 8\n\n\n\n    \n![png](./Multivariate_multistep_Informer_31_15.png)\n    \n\n\n    Summary plot for timestep 9\n\n\n\n    \n![png](./Multivariate_multistep_Informer_31_17.png)\n    \n\n\n    Summary plot for timestep 10\n\n\n\n    \n![png](./Multivariate_multistep_Informer_31_19.png)\n    \n\n\n # Shapley values\n\n SHAP values are indicating by how much the presence of a particular feature influenced the model's prediction, compared to if that feature was absent. The color represents the actual value of the feature itself. In the context of the present work, Shapley values are employed as a method to enhance the interpretability of the model. Shapley values provide insights into the contribution of each feature to the model's predictions. Specifically, they quantify how each feature influences the prediction by evaluating its impact when combined with all other features. By calculating Shapley values, we gain a clear understanding of the relative importance of each feature in multivariate time series prediction.\n\nOverall, Shapley values enhance model interpretability by offering a systematic and quantitative way to dissect complex models and understand their decision-making processes. They enable us to identify which features are the key drivers of predictions, helping us make more informed decisions and potentially improving model performance. This interpretability is crucial in various applications, from finance to healthcare, where understanding the factors influencing predictions is paramount for trust and decision-making.\n\n\n```python\n\n```\n\n\n\n!rm Multivariate_multistep_Informer.md"
  },
  {
    "objectID": "Research Projects/Informer Model/index.html",
    "href": "Research Projects/Informer Model/index.html",
    "title": "Informer Model",
    "section": "",
    "text": "The Informer model variant is designed for multivariate prediction. Let’s consider ‘Open’, ‘High’, ‘Low’, and ‘Close’ prices for simplicity. The provided code is designed to fetch and preprocess historical stock prices for Apple Inc. for the purpose of multivariate time series forecasting using an LSTM model. Initially, the code downloads Apple’s stock data, specifically capturing four significant features: Open, High, Low, and Close prices. To make the data suitable for deep learning models, it is normalized to fit within a range of 0 to 1. The sequential data is then transformed into a format suitable for supervised learning, where the data from the past look_back days is used to predict the next day’s features. Finally, the data is partitioned into training (67%) and test sets, ensuring separate datasets for model training and evaluation.\n!pip install yfinance\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\nimport tensorflow as tf\nimport yfinance as yf\n# Download historical data as dataframe\ndata = yf.download(\"AAPL\", start=\"2018-01-01\", end=\"2023-09-01\")\n# Using multiple columns for multivariate prediction\ndf = data[[\"Open\", \"High\", \"Low\", \"Close\"]]\n\n# Normalize the data\nscaler = MinMaxScaler(feature_range=(0, 1))\ndf_scaled = scaler.fit_transform(df)\n\n# Prepare data for LSTM\nlook_back = 10  # Number of previous time steps to use as input variables\nn_features = df.shape[1]  # number of features\n\n# Convert to supervised learning problem\nX, y = [], []\nfor i in range(len(df_scaled) - look_back):\n    X.append(df_scaled[i:i+look_back, :])\n    y.append(df_scaled[i + look_back, :])\nX, y = np.array(X), np.array(y)\n\n# Reshape input to be [samples, time steps, features]\nX = np.reshape(X, (X.shape[0], X.shape[1], n_features))\n\n# Train-test split\ntrain_size = int(len(X) * 0.67)\ntest_size = len(X) - train_size\nX_train, X_test = X[0:train_size], X[train_size:]\ny_train, y_test = y[0:train_size], y[train_size:]\n\nprint(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n\nProbSparse Self-Attention:\nSelf-attention involves computing a weighted sum of all values in the sequence, based on the dot product between the query and key. In ProbSparse, we don’t compute this for all query-key pairs, but rather select dominant ones, thus making the computation more efficient. The given code defines a custom Keras layer, ProbSparseSelfAttention, which implements the multi-head self-attention mechanism, a critical component of Transformer models. This layer initializes three dense networks for the Query, Key, and Value matrices, and splits the input data into multiple heads to enable parallel processing. During the forward pass (call method), the Query, Key, and Value matrices are calculated, scaled, and then used to compute attention scores. These scores indicate the importance of each element in the sequence when predicting another element. The output is a weighted sum of the input values, which is then passed through another dense layer to produce the final result.\nclass ProbSparseSelfAttention(tf.keras.layers.Layer):\n    def __init__(self, d_model, num_heads, **kwargs):\n        super(ProbSparseSelfAttention, self).__init__(**kwargs)\n        self.num_heads = num_heads\n        self.d_model = d_model\n\n        # Assert that d_model is divisible by num_heads\n        assert self.d_model % self.num_heads == 0, f\"d_model ({d_model}) must be divisible by num_heads ({num_heads})\"\n\n        self.depth = d_model // self.num_heads\n\n        # Defining the dense layers for Query, Key and Value\n        self.wq = tf.keras.layers.Dense(d_model)\n        self.wk = tf.keras.layers.Dense(d_model)\n        self.wv = tf.keras.layers.Dense(d_model)\n\n        self.dense = tf.keras.layers.Dense(d_model)\n\n    def split_heads(self, x, batch_size):\n        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n        return tf.transpose(x, perm=[0, 2, 1, 3])\n\n    def call(self, v, k, q):\n        batch_size = tf.shape(q)[0]\n\n        q = self.wq(q)\n        k = self.wk(k)\n        v = self.wv(v)\n\n        q = self.split_heads(q, batch_size)\n        k = self.split_heads(k, batch_size)\n        v = self.split_heads(v, batch_size)\n\n        # Fixing matrix multiplication\n        matmul_qk = tf.matmul(q, k, transpose_b=True)\n\n        d_k = tf.cast(self.depth, tf.float32)\n        scaled_attention_logits = matmul_qk / tf.math.sqrt(d_k)\n\n        attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n        output = tf.matmul(attention_weights, v)\n\n        output = tf.transpose(output, perm=[0, 2, 1, 3])\n        concat_attention = tf.reshape(output, (batch_size, -1, self.d_model))\n\n        return self.dense(concat_attention)\n\n\nInformer Encoder:\nThe InformerEncoder is a custom Keras layer designed to process sequential data using a combination of attention and convolutional mechanisms. Within the encoder, the input data undergoes multi-head self-attention, utilizing the ProbSparseSelfAttention mechanism, to capture relationships in the data regardless of their distance. Post attention, the data is transformed and normalized, then further processed using two 1D convolutional layers, emphasizing local features in the data. After another normalization step, the processed data is pooled to a lower dimensionality, ensuring the model captures global context, and then passed through a dense layer to produce the final output.\nclass InformerEncoder(tf.keras.layers.Layer):\n    def __init__(self, d_model, num_heads, conv_filters, **kwargs):\n        super(InformerEncoder, self).__init__(**kwargs)\n        self.d_model = d_model\n        self.num_heads = num_heads\n\n        # Assert that d_model is divisible by num_heads\n        assert self.d_model % self.num_heads == 0, f\"d_model ({d_model}) must be divisible by num_heads ({num_heads})\"\n\n        self.self_attention = ProbSparseSelfAttention(d_model=d_model, num_heads=num_heads)\n\n        self.norm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n\n        # This dense layer will transform the input 'x' to have the dimensionality 'd_model'\n        self.dense_transform = tf.keras.layers.Dense(d_model)\n\n        self.conv1 = tf.keras.layers.Conv1D(conv_filters, 3, padding='same')\n        self.conv2 = tf.keras.layers.Conv1D(d_model, 3, padding='same')\n        self.norm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n\n        self.global_avg_pooling = tf.keras.layers.GlobalAveragePooling1D()\n        self.dense = tf.keras.layers.Dense(d_model)\n\n    def call(self, x):\n        attn_output = self.self_attention(x, x, x)\n\n        # Transform 'x' to have the desired dimensionality\n        x_transformed = self.dense_transform(x)\n        attn_output = self.norm1(attn_output + x_transformed)\n\n        conv_output = self.conv1(attn_output)\n        conv_output = tf.nn.relu(conv_output)\n        conv_output = self.conv2(conv_output)\n\n        encoded_output = self.norm2(conv_output + attn_output)\n\n        pooled_output = self.global_avg_pooling(encoded_output)\n        return self.dense(pooled_output)[:, -4:]\ninput_layer = tf.keras.layers.Input(shape=(look_back, n_features))\n\n# Encoder\nencoder_output = InformerEncoder(d_model=360, num_heads=8, conv_filters=64)(input_layer)\n\n# Decoder (with attention)\ndecoder_lstm = tf.keras.layers\nThe InformerModel function is designed to create a deep learning architecture tailored for sequential data prediction. It takes an input sequence and processes it using the InformerEncoder, a custom encoder layer, which captures both local and global patterns in the data. Following the encoding step, a decoder structure unravels the encoded data by first repeating the encoder’s output, then passing it through an LSTM layer to retain sequential dependencies, and finally making predictions using a dense layer. The resulting architecture is then compiled with the Adam optimizer and Mean Squared Error loss, ready for training on time series data.\nfrom tensorflow.keras.layers import RepeatVector\nfrom tensorflow.keras.layers import Input\nfrom tensorflow.keras.layers import LSTM\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\n\n\ndef InformerModel(input_shape, d_model=64, num_heads=2, conv_filters=256, learning_rate= 1e-3):\n    # Input\n    input_layer = Input(shape=input_shape)\n\n    # Encoder\n    encoder_output = InformerEncoder(d_model=d_model, num_heads=num_heads, conv_filters=conv_filters)(input_layer)\n\n    # Decoder\n    repeated_output = RepeatVector(4)(encoder_output)  # Repeating encoder's output\n    decoder_lstm = LSTM(312, return_sequences=True)(repeated_output)\n    decoder_output = Dense(4)(decoder_lstm[:, -1, :])  # Use the last sequence output to predict the next value\n\n    # Model\n    model = Model(inputs=input_layer, outputs=decoder_output)\n    # Compile the model with the specified learning rate\n    optimizer = Adam(learning_rate=learning_rate)\n    model.compile(optimizer=optimizer, loss='mean_squared_error')\n\n\n\n    return model\n\nmodel = InformerModel(input_shape=(look_back, n_features))\nmodel.compile(optimizer='adam', loss='mean_squared_error')\nmodel.summary()\n\n\nModel: \"model_10\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_12 (InputLayer)       [(None, 10, 4)]           0         \n                                                                 \n informer_encoder_11 (Infor  (None, 4)                 108480    \n merEncoder)                                                     \n                                                                 \n repeat_vector_10 (RepeatVe  (None, 4, 4)              0         \n ctor)                                                           \n                                                                 \n lstm_10 (LSTM)              (None, 4, 312)            395616    \n                                                                 \n tf.__operators__.getitem_1  (None, 312)               0         \n 0 (SlicingOpLambda)                                             \n                                                                 \n dense_82 (Dense)            (None, 4)                 1252      \n                                                                 \n=================================================================\nTotal params: 505348 (1.93 MB)\nTrainable params: 505348 (1.93 MB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\nThe model.fit method trains the neural network using the provided training data (X_train and y_train) for a total of 50 epochs with mini-batches of 32 samples. During training, 20% of the training data is set aside for validation to monitor and prevent overfitting.\nhistory = model.fit(\n    X_train, y_train,\n    epochs=50,\n    batch_size=64,\n    validation_split=0.3,\n    shuffle=True\n)\nThe code displays a visual representation of the model’s training and validation loss over the epochs using a line chart. The x-axis represents the number of epochs, while the y-axis indicates the mean squared error, allowing users to observe how the model’s performance evolves over time.\nplt.figure(figsize=(12, 6))\nplt.plot(history.history['loss'], label='Training Loss')\nplt.plot(history.history['val_loss'], label='Validation Loss')\nplt.title('Training and Validation Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Mean Squared Error')\nplt.legend()\nplt.show()\n\n\n\npng\n\n\ntest_predictions = model.predict(X_test)\ntest_predictions = scaler.inverse_transform(test_predictions)\ntrue_values = scaler.inverse_transform(y_test)\n\nmse = mean_squared_error(true_values, test_predictions)\nprint(f\"Test MSE: {mse}\")\n15/15 [==============================] - 1s 3ms/step\nTest MSE: 462.6375895467179\nplt.figure(figsize=(12, 6))\nplt.plot(true_values, label='True Values')\nplt.plot(test_predictions, label='Predictions', alpha=0.6)\nplt.title('Test Set Predictions vs. True Values')\nplt.legend()\nplt.show()\n\n\n\npng\n\n\n!pip install keras-tuner\nThe code defines a function to construct a neural network model using varying hyperparameters, aiming to optimize its architecture. Subsequently, the RandomSearch method from Keras Tuner is employed to explore 200 different model configurations, assessing their performance to determine the best hyperparameters that minimize the validation loss.\nfrom tensorflow.keras.layers import Input, RepeatVector, LSTM, Dense\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nimport tensorflow as tf\nimport kerastuner as kt\n\ndef build_model(hp):\n    # Input\n    input_layer = Input(shape=(look_back, n_features))\n\n    # Encoder\n    encoder_output = InformerEncoder(d_model=hp.Int('d_model', min_value=32, max_value=512, step=16),\n                                     num_heads=hp.Int('num_heads', 2, 8, step=2),\n                                     conv_filters=hp.Int('conv_filters', min_value=16, max_value=256, step=16))(input_layer)\n\n    # Decoder\n    repeated_output = RepeatVector(4)(encoder_output)  # Repeating encoder's output\n    decoder_lstm = LSTM(312, return_sequences=True)(repeated_output)\n    decoder_output = Dense(4)(decoder_lstm[:, -1, :])  # Use the last sequence output to predict the next value\n\n    # Model\n    model = Model(inputs=input_layer, outputs=decoder_output)\n\n    # Compile the model with the specified learning rate\n    optimizer = Adam(learning_rate=hp.Choice('learning_rate', [1e-3, 1e-2, 1e-1]))\n    model.compile(optimizer=optimizer, loss='mean_squared_error')\n\n    return model\n\n# Define the tuner\ntuner = kt.RandomSearch(\n    build_model,\n    objective='val_loss',\n    max_trials=30,\n    executions_per_trial=5,\n    directory='hyperparam_search',\n    project_name='informer_model'\n)\nThe code sets up two training callbacks: one for early stopping if validation loss doesn’t improve after 10 epochs, and another to save the model weights at their best performance. With these callbacks, the tuner conducts a search over the hyperparameter space using the training data, and evaluates model configurations over 100 epochs, saving the most optimal weights and potentially halting early if improvements stagnate.\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n\nearly_stopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1, restore_best_weights=True)\nmodel_checkpoint = ModelCheckpoint(filepath='trial_best.h5', monitor='val_loss', verbose=1, save_best_only=True)\n\ntuner.search(X_train, y_train,\n             epochs=100,\n             validation_split=0.2,\n             callbacks=[early_stopping, model_checkpoint])\nTrial 30 Complete [00h 00m 01s]\n\nBest val_loss So Far: 0.0009468139498494566\nTotal elapsed time: 00h 30m 43s\n# Get the best hyperparameters\nbest_hp = tuner.get_best_hyperparameters()[0]\n\n# Retrieve the best model\nbest_model = tuner.get_best_models()[0]\nbest_model.summary()\nModel: \"model\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_1 (InputLayer)        [(None, 10, 4)]           0         \n                                                                 \n informer_encoder (Informer  (None, 4)                 108480    \n Encoder)                                                        \n                                                                 \n repeat_vector (RepeatVecto  (None, 4, 4)              0         \n r)                                                              \n                                                                 \n lstm (LSTM)                 (None, 4, 312)            395616    \n                                                                 \n tf.__operators__.getitem (  (None, 312)               0         \n SlicingOpLambda)                                                \n                                                                 \n dense_6 (Dense)             (None, 4)                 1252      \n                                                                 \n=================================================================\nTotal params: 505348 (1.93 MB)\nTrainable params: 505348 (1.93 MB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\ntest_loss = best_model.evaluate(X_test, y_test)\nprint(f\"Test MSE: {test_loss}\")\n15/15 [==============================] - 0s 3ms/step - loss: 0.0086\nTest MSE: 0.008609036915004253\nplt.figure(figsize=(20, 12))\nfor i in range(true_values.shape[1]):\n    plt.subplot(2, 2, i+1)\n    plt.plot(true_values[:, i], label='True Values', color='blue')\n    plt.plot(test_predictions[:, i], label='Predictions', color='red', linestyle='--')\n    plt.title(f\"Feature {i+1}\")\n    plt.legend()\nplt.tight_layout()\nplt.show()\n\n\n\npng\n\n\nfrom sklearn.metrics import mean_absolute_error\nmae = mean_absolute_error(true_values, test_predictions)\nrmse = np.sqrt(test_loss)  # since loss is MSE\nprint(f\"MAE: {mae}, RMSE: {rmse}\")\nMAE: 19.377517553476185, RMSE: 0.09278489594219662\nprint(f\"Best d_model: {best_hp.get('d_model')}\")\nprint(f\"Best num_heads: {best_hp.get('num_heads')}\")\nprint(f\"Best conv_filters: {best_hp.get('conv_filters')}\")\nprint(f\"Best learning_rate: {best_hp.get('learning_rate')}\")\nBest d_model: 64\nBest num_heads: 2\nBest conv_filters: 256\nBest learning_rate: 0.001\n!pip install shap\nimport shap\n# The reference can be a dataset or just random data\nbackground = X_train[np.random.choice(X_train.shape[0], 300, replace=False)]  # Taking a random sample of the training data as background\nexplainer = shap.GradientExplainer(best_model, background)\nshap_values = explainer.shap_values(X_test[:300])  # Computing for a subset for performance reasons\nfor timestep in range(10):\n    print(f\"Summary plot for timestep {timestep + 1}\")\n    shap.summary_plot(shap_values[0][:, timestep, :], X_test[:300, timestep, :])\nSummary plot for timestep 1\n\n\n\npng\n\n\nSummary plot for timestep 2\n\n\n\npng\n\n\nSummary plot for timestep 3\n\n\n\npng\n\n\nSummary plot for timestep 4\n\n\n\npng\n\n\nSummary plot for timestep 5\n\n\n\npng\n\n\nSummary plot for timestep 6\n\n\n\npng\n\n\nSummary plot for timestep 7\n\n\n\npng\n\n\nSummary plot for timestep 8\n\n\n\npng\n\n\nSummary plot for timestep 9\n\n\n\npng\n\n\nSummary plot for timestep 10\n\n\n\npng\n\n\n\n\nShapley values\nSHAP values are indicating by how much the presence of a particular feature influenced the model’s prediction, compared to if that feature was absent. The color represents the actual value of the feature itself. In the context of the present work, Shapley values are employed as a method to enhance the interpretability of the model. Shapley values provide insights into the contribution of each feature to the model’s predictions. Specifically, they quantify how each feature influences the prediction by evaluating its impact when combined with all other features. By calculating Shapley values, we gain a clear understanding of the relative importance of each feature in multivariate time series prediction.\nOverall, Shapley values enhance model interpretability by offering a systematic and quantitative way to dissect complex models and understand their decision-making processes. They enable us to identify which features are the key drivers of predictions, helping us make more informed decisions and potentially improving model performance. This interpretability is crucial in various applications, from finance to healthcare, where understanding the factors influencing predictions is paramount for trust and decision-making."
  },
  {
    "objectID": "Research Projects/Exoplanet Modeling/research_statement.html",
    "href": "Research Projects/Exoplanet Modeling/research_statement.html",
    "title": "Exoplanet Feature Distributions",
    "section": "",
    "text": "# Import required libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nThe figure below showcases a spectrum integral to exoplanet detection, charting the transit depth against the wavelength in microns. Every observation point in the spectrum carries an inherent uncertainty, denoted by the vertical error bars. To decode and potentially minimize this uncertainty, it’s pivotal to fathom how features like planet radius, planet temperature, and the logarithmic concentrations of H₂O, CO₂, CO, CH₄, and NH₃ influence the transit depth. Leveraging interpretative tools like SHAP can provide insights into how these exoplanetary features impact the observed transit depth, refining our understanding and accuracy in exoplanet spectral analysis.\nfrom IPython.display import Image\nImage(filename='my_spectrum.png')\n# Load the CSV file into a Pandas DataFrame\ndf = pd.read_csv(\"small_astro.csv\")\ndf_backup = df.copy()\ndf_backup.head(10)\n\n\n\n\n\n\n\n\nUnnamed: 0\nplanet_radius\nplanet_temp\nlog_h2o\nlog_co2\nlog_co\nlog_ch4\nlog_nh3\nx1\nx2\n...\nx43\nx44\nx45\nx46\nx47\nx48\nx49\nx50\nx51\nx52\n\n\n\n\n0\n1\n0.559620\n863.394770\n-8.865868\n-6.700707\n-5.557561\n-8.957615\n-3.097540\n0.003836\n0.003834\n...\n0.003938\n0.003941\n0.003903\n0.003931\n0.003983\n0.004019\n0.004046\n0.004072\n0.004054\n0.004056\n\n\n1\n2\n1.118308\n1201.700465\n-4.510258\n-8.228966\n-3.565427\n-7.807424\n-3.633658\n0.015389\n0.015148\n...\n0.015450\n0.015447\n0.015461\n0.015765\n0.016099\n0.016376\n0.016549\n0.016838\n0.016781\n0.016894\n\n\n2\n3\n0.400881\n1556.096477\n-7.225472\n-6.931472\n-3.081975\n-8.567854\n-5.378472\n0.002089\n0.002073\n...\n0.001989\n0.002168\n0.002176\n0.002123\n0.002079\n0.002081\n0.002106\n0.002167\n0.002149\n0.002185\n\n\n3\n4\n0.345974\n1268.624884\n-7.461157\n-5.853334\n-3.044711\n-5.149378\n-3.815568\n0.002523\n0.002392\n...\n0.002745\n0.003947\n0.004296\n0.003528\n0.003352\n0.003629\n0.003929\n0.004363\n0.004216\n0.004442\n\n\n4\n5\n0.733184\n1707.323564\n-4.140844\n-7.460278\n-3.181793\n-5.996593\n-4.535345\n0.002957\n0.002924\n...\n0.003402\n0.003575\n0.003667\n0.003740\n0.003823\n0.003904\n0.003897\n0.004004\n0.004111\n0.004121\n\n\n5\n6\n0.161165\n620.185809\n-4.875000\n-5.074766\n-3.861240\n-5.388011\n-8.390503\n0.000444\n0.000442\n...\n0.000432\n0.000486\n0.000473\n0.000462\n0.000447\n0.000455\n0.000455\n0.000457\n0.000463\n0.000474\n\n\n6\n7\n0.194312\n900.597575\n-8.299899\n-6.850709\n-4.314491\n-3.712038\n-3.951455\n0.001794\n0.001721\n...\n0.001048\n0.001052\n0.000948\n0.000976\n0.001122\n0.001274\n0.001395\n0.001522\n0.001456\n0.001823\n\n\n7\n8\n1.132685\n1176.443900\n-6.765865\n-7.398548\n-3.378307\n-3.763737\n-5.881384\n0.012950\n0.012946\n...\n0.014019\n0.013871\n0.013810\n0.013902\n0.014024\n0.014150\n0.014298\n0.014392\n0.014401\n0.015042\n\n\n8\n9\n0.158621\n1189.209841\n-8.376041\n-6.321977\n-3.243900\n-8.711851\n-3.449195\n0.000444\n0.000445\n...\n0.000562\n0.000595\n0.000571\n0.000590\n0.000628\n0.000663\n0.000692\n0.000734\n0.000718\n0.000736\n\n\n9\n10\n0.660642\n528.023669\n-3.804286\n-8.919378\n-4.686964\n-8.150277\n-3.068319\n0.008997\n0.009035\n...\n0.009435\n0.009375\n0.009315\n0.009357\n0.009563\n0.009739\n0.009821\n0.009890\n0.009819\n0.009734\n\n\n\n\n10 rows × 60 columns\n# Columns of interest for planetary and chemical properties and transit depth\nfeature_columns = df.columns[1:8]\ntransit_depth_column = 'x1' # pick the first wavelength\n\n# Calculate mean and variance for features and transit depth\nfeature_mean = df[feature_columns].mean()\nfeature_variance = df[feature_columns].var()\ntransit_depth_mean = df[transit_depth_column].mean()\ntransit_depth_variance = df[transit_depth_column].var()\n\n# Visualize the distributions\nfig, axes = plt.subplots(1, 8, figsize=(18, 4))\nfor i, col in enumerate(feature_columns):\n    sns.histplot(df[col], bins=20, kde=True, ax=axes[i])\n    axes[i].set_title(f'{col}\\nMean: {feature_mean[col]:.2f}\\nVariance: {feature_variance[col]:.2f}')\n\n# Add visualization for transit depth\nsns.histplot(df[transit_depth_column], bins=20, kde=True, ax=axes[-1])\naxes[-1].set_title(f'{transit_depth_column}\\nMean: {transit_depth_mean:.2f}\\nVariance: {transit_depth_variance:.2f}')\n\nplt.tight_layout()\nplt.show()\n\nfeature_mean, feature_variance, transit_depth_mean, transit_depth_variance\n\n\n\n\n(planet_radius       0.703714\n planet_temp      1073.229674\n log_h2o            -5.934889\n log_co2            -6.873009\n log_co             -4.497141\n log_ch4            -5.799850\n log_nh3            -6.051791\n dtype: float64,\n planet_radius         0.198990\n planet_temp      154495.743225\n log_h2o               3.130868\n log_co2               1.649658\n log_co                0.738255\n log_ch4               3.208283\n log_nh3               3.050545\n dtype: float64,\n 0.009442470522591322,\n 0.00016172106267489707)\nThe provided visualizations and statistics shed light on the distribution of various exoplanetary features and the observed transit depth at the x1 wavelength.\nThese distributions and their accompanying statistics offer invaluable insights into the data’s nature and its inherent variability, essential for accurate spectral analysis and interpretation.\n# Import required libraries for modeling and SHAP values\nimport xgboost as xgb\nimport shap\n\n# Prepare the feature matrix (X) and the target vector (y)\nX = df.iloc[:, 1:8]\ny = df['x1']\n\n# Train an XGBoost model\nmodel = xgb.XGBRegressor(objective ='reg:squarederror')\nmodel.fit(X, y)\n\n# Initialize the SHAP explainer\nexplainer = shap.Explainer(model)\n\n# Calculate SHAP values\nshap_values = explainer(X)\n\n# Summary plot for SHAP values\nshap.summary_plot(shap_values, X, title='Shapley Feature Importance')\n\nUsing `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\nSHAP (SHapley Additive exPlanations) values stem from cooperative game theory and provide a way to interpret machine learning model predictions. Each feature in a model is analogous to a player in a game, and the contribution of each feature to a prediction is like the payout a player receives in the game. In SHAP, we calculate the value of each feature by considering all possible combinations (coalitions) of features, assessing the change in prediction with and without that feature. The resulting value, the Shapley value, represents the average contribution of a feature to all possible predictions.\nThe summary plot visualizes these values. Each dot represents a SHAP value for a specific instance of a feature; positive values (right of the centerline) indicate that a feature increases the model’s output, while negative values (left of the centerline) suggest a decrease. The color depicts the actual value of the feature for the given instance, enabling a comprehensive view of feature influence across the dataset.\n# Import the Random Forest Regressor and visualization libraries\nfrom sklearn.ensemble import RandomForestRegressor\n\n# Train a Random Forest model\nrf_model = RandomForestRegressor(n_estimators=100, random_state=42)\nrf_model.fit(X, y)\n\n# Extract feature importances\nfeature_importances = rf_model.feature_importances_\n\n# Create a DataFrame for visualization\nimportance_df = pd.DataFrame({\n    'Feature': feature_columns,\n    'Importance': feature_importances\n}).sort_values(by='Importance', ascending=True)\n\n# Create a horizontal bar chart for feature importances\nplt.figure(figsize=(10, 6))\nplt.barh(importance_df['Feature'], importance_df['Importance'], color='dodgerblue')\nplt.xlabel('Importance')\nplt.ylabel('')\nplt.title('Random Forest Feature Importance')\nplt.xticks(fontsize=13)  # Increase the font size of the x-axis tick labels\nplt.yticks(fontsize=14)  # Increase the font size of the x-axis tick labels\n# Save the figure before showing it\nplt.savefig('random_forest_importance_plot.png', bbox_inches='tight', dpi=300)  # 'bbox_inches' ensures the entire plot is saved\n\nplt.show()\n\n\n\nfeature_importances\n\n\n\n\narray([0.71850575, 0.10367982, 0.01655567, 0.07232808, 0.05603188,\n       0.0146824 , 0.0182164 ])"
  },
  {
    "objectID": "Research Projects/Exoplanet Modeling/research_statement.html#random-forest-feature-importance-analysis",
    "href": "Research Projects/Exoplanet Modeling/research_statement.html#random-forest-feature-importance-analysis",
    "title": "Exoplanet Feature Distributions",
    "section": "Random Forest Feature Importance Analysis",
    "text": "Random Forest Feature Importance Analysis\nTo gain insights into which exoplanetary features most influence the observed transit depth, a Random Forest Regressor was utilized. Here’s an outline of the procedure:\n\nModel Initialization: A Random Forest Regressor model was instantiated with 100 trees and a fixed random seed of 42 for reproducibility.\nModel Training: The model was trained on the feature set X and target variable y.\nFeature Importance Extraction: After training, the importance of each feature was extracted using the feature_importances_ attribute of the trained model.\nData Preparation for Visualization: A DataFrame was created to house each feature alongside its respective importance. The features were then sorted in ascending order of importance for better visualization.\nVisualization: A horizontal bar chart was plotted to showcase the importance of each feature. The chart offers a clear visual comparison, with the y-axis representing the features and the x-axis indicating their importance. Special attention was paid to font size adjustments for better readability. Furthermore, before displaying the chart, it was saved as a PNG image with high resolution.\n\nThe resulting visualization, titled ‘Random Forest Feature Importance’, provides a clear understanding of the relative significance of each feature in predicting the transit depth, as discerned by the Random Forest model.\n\nfrom IPython.display import Image\nImage(filename='PME.png')"
  },
  {
    "objectID": "Research Projects/Exoplanet Modeling/research_statement.html#pme-feature-importance-analysis",
    "href": "Research Projects/Exoplanet Modeling/research_statement.html#pme-feature-importance-analysis",
    "title": "Exoplanet Feature Distributions",
    "section": "PME Feature Importance Analysis",
    "text": "PME Feature Importance Analysis\nUsing a modeling technique, the impact of different exoplanetary features on the observed transit depth was assessed, and their importance was visualized in the attached figure titled ‘PME Feature Importance’. Here’s a breakdown of the visual representation:\n\nMost Influential Feature: The planet_radius stands out as the most influential feature with the highest PME (Predictive Modeling Estimate) value. This suggests that the radius of the planet plays a pivotal role in determining the observed transit depth.\nOther Features: Logarithmic concentrations of gases, such as log_co2, log_co, log_nh3, log_h2o, and log_ch4, also exhibit varying degrees of importance. Among these, log_co2 and log_co are the more significant contributors compared to others.\nLeast Influential Feature: The planet_temp, representing the temperature of the planet, has the least importance in this analysis, suggesting its minimal role in influencing the transit depth, at least according to the PME metric.\nVisual Clarity: The horizontal bar chart offers a lucid comparison of feature importances. Each bar’s length represents the PME value of a feature, providing a direct visual cue to its significance.\nInterpretation: This visualization aids in discerning which exoplanetary characteristics are most relevant when predicting the transit depth using the given model. It can guide future analyses by highlighting key features to focus on or, conversely, those that might be less consequential.\n\nBy examining the ‘PME Feature Importance’ chart, one gains a deeper understanding of the relative significance of each feature in predicting the transit depth within this specific modeling context."
  },
  {
    "objectID": "Research Projects/Exoplanet Modeling/research_statement.html#uncertainty-quantification-and-feature-reduction-in-pme-feature-importance-analysis",
    "href": "Research Projects/Exoplanet Modeling/research_statement.html#uncertainty-quantification-and-feature-reduction-in-pme-feature-importance-analysis",
    "title": "Exoplanet Feature Distributions",
    "section": "Uncertainty Quantification and Feature Reduction in PME Feature Importance Analysis",
    "text": "Uncertainty Quantification and Feature Reduction in PME Feature Importance Analysis\nWhen delving deep into the realms of uncertainty quantification and feature reduction in predictive modeling, it’s crucial to evaluate feature importance metrics critically. The provided PME (Predictive Modeling Estimate) Feature Importance Analysis offers a valuable lens for this task. Below is a nuanced exploration of its significance in the described contexts:\n\nUncertainty Quantification:\n\nOrigin of Uncertainty:\n\nIn exoplanetary spectral analysis, uncertainty can arise from observational noise, instrumental errors, or intrinsic variability of the observed phenomena. This uncertainty often manifests in the form of error bars in spectra, like the ones shown in transit depth against wavelengths.\n\nFeature Impact on Uncertainty:\n\nThe degree of influence a feature has on the predicted outcome can be a proxy for how that feature might contribute to the overall predictive uncertainty. If a feature like planet_radius has a high PME value, it might be a critical determinant of transit depth. Any uncertainty in measuring or estimating the planet_radius could propagate and significantly affect the prediction’s reliability.\n\nPME as a Measure of Stochastic Uncertainty:\n\nThe PME values themselves might be obtained by analyzing a model’s sensitivity to perturbations in input features. A high PME value indicates that slight changes in the feature can lead to notable changes in the output, thereby implying a greater inherent stochastic uncertainty tied to that feature.\n\n\n\n\nFeature Reduction:\n\nIdentifying Critical Features:\n\nWhen dealing with a multitude of features, not all may be equally relevant. The PME analysis provides a hierarchy of feature importance. In this case, while planet_radius emerges as crucial, planet_temp appears less consequential. This differentiation is fundamental for feature reduction, guiding us on which features to prioritize in modeling.\n\nReducing Dimensionality & Complexity:\n\nIn data-driven modeling, especially with limited data points, overfitting is a genuine concern. By understanding which features significantly influence the predictions (like planet_radius or log_co2), one can potentially reduce the model’s complexity and the risk of overfitting by focusing only on these paramount features.\n\nInforming Experimental Design:\n\nIf further observational or experimental data is required, knowing feature importances can guide where resources are channeled. For instance, more precise measurements might be sought for features with high PME values, as their accurate estimation is vital for reliable predictions.\n\nTrade-off with Predictive Performance:\n\nIt’s essential to understand that while feature reduction can simplify models and make them more interpretable, there’s always a trade-off with predictive performance. Removing features based on their PME values should be done judiciously, ensuring that the model’s predictive capability isn’t unduly compromised.\n\n\nIn summary, the ‘PME Feature Importance’ chart isn’t merely a representation of feature significance but serves as a cornerstone for rigorous analytical decisions in uncertainty quantification and feature reduction. Analyzing such importance metrics within the broader context of the problem at hand ensures that models are both robust and interpretable, catering effectively to the dual objectives of predictive accuracy and analytical clarity.\n\n\nWeighted Principal Component Analysis (PCA) on Exoplanet Data\n\nConceptual Overview:\nPCA is a method used to emphasize variation and capture strong patterns in a dataset. The “weighted PCA” approach fine-tunes this by considering the importance of different features, effectively giving more attention to features deemed vital.\n\n\nDetailed Breakdown:\n\nStandardization:\n\nBefore applying PCA, the dataset is standardized to give each feature a mean of 0 and a variance of 1. This is essential because PCA is influenced by the scale of the data.\n\nWeighted Features:\n\nFeatures are weighted according to their importance, as identified by the Random Forest model. Taking the square root of the weights ensures proper scaling during matrix multiplication in PCA.\n\nPCA Application:\n\nPCA projects the data into a new space defined by its principal components. The first two components often hold most of the dataset’s variance, making them crucial for visualization.\n\nVisualization:\n\nThe scatter plot visualizes the data in the space of the first two principal components. The color indicates Transit Depth (x1), shedding light on how this parameter varies across the main patterns in the data.\n\nExplained Variance:\n\nThis provides an understanding of how much original variance the first two components capture. A high percentage indicates that the PCA representation retains much of the data’s original structure.\n\n\n\n\nSignificance:\n\nData Compression:\n\nPCA offers a simplified yet rich representation of the data, which can be invaluable for visualization and pattern recognition.\n\nFeature Emphasis:\n\nUsing feature importances ensures the PCA representation highlights the most critical patterns related to influential features.\n\nFramework for Further Exploration:\n\nObserving patterns or groupings in the PCA plot can guide subsequent investigations, pinpointing areas of interest or potential clusters.\n\nEfficient Data Overview:\n\nThe visualization provides a comprehensive but digestible overview of the data, suitable for a wide range of audiences.\n\n\nIn essence, weighted PCA melds the dimensionality reduction capabilities of PCA with the interpretative power of feature importances, offering a profound view into the dataset’s intricate structures and relationships.\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nimport numpy as np\n\n# Standardize the feature matrix\nX_scaled = StandardScaler().fit_transform(X)\n\n# Multiply each feature by its square root of importance weight for weighted PCA\n# The square root is used because each feature contributes to both rows and columns in the dot product calculation\nX_weighted = X_scaled * np.sqrt(feature_importances)\n\n# Perform PCA on the weighted data\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X_weighted)\n\n# Plotting the first two principal components\nplt.figure(figsize=(10, 7))\nplt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis', edgecolor='k', s=40)\nplt.colorbar().set_label('Transit Depth (x1)', rotation=270, labelpad=15)\nplt.xlabel('Weighted Principal Component 1')\nplt.ylabel('Weighted Principal Component 2')\nplt.title('Weighted PCA: First Two Principal Components')\nplt.show()\n\n# Variance explained by the first two principal components\nvariance_explained = pca.explained_variance_ratio_\nvariance_explained\n\n\n\n\narray([0.71985328, 0.10370239])\n\n\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\n# Your data initialization (this part is assumed, as you haven't provided it in the original code)\n# X, y, feature_importances = ...\n\n# Standardize the feature matrix\nX_scaled = StandardScaler().fit_transform(X)\n\n# Multiply each feature by its square root of importance weight for weighted PCA\nX_weighted = X_scaled * np.sqrt(feature_importances)\n\n# Perform PCA on the weighted data with three components\npca = PCA(n_components=3)\nX_pca = pca.fit_transform(X_weighted)\n\n# Plotting the first three principal components\nfig = plt.figure(figsize=(10, 7))\nax = fig.add_subplot(111, projection='3d')\nscatter = ax.scatter(X_pca[:, 0], X_pca[:, 1], X_pca[:, 2], c=y, cmap='viridis', edgecolor='k', s=40)\nplt.colorbar(scatter, ax=ax, pad=0.2).set_label('Transit Depth (x1)', rotation=270, labelpad=15)\nax.set_xlabel('Weighted Principal Component 1')\nax.set_ylabel('Weighted Principal Component 2')\nax.set_zlabel('Weighted Principal Component 3')\nplt.title('Weighted PCA: First Three Principal Components')\nplt.show()\n\n# Variance explained by the first three principal components\nvariance_explained = pca.explained_variance_ratio_\nvariance_explained\n\n\n\n\narray([0.71985328, 0.10370239, 0.07160805])\n\n\n\n# side by side\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\n# Your data initialization (this part is assumed, as you haven't provided it in the original code)\n# X, y, feature_importances = ...\n\n# Standardize the feature matrix\nX_scaled = StandardScaler().fit_transform(X)\n\n# Multiply each feature by its square root of importance weight for weighted PCA\nX_weighted = X_scaled * np.sqrt(feature_importances)\n\n# Create a combined figure with subplots\nfig, axes = plt.subplots(1, 2, figsize=(20, 7))\n\n# Perform PCA on the weighted data with two components and plot\npca2 = PCA(n_components=2)\nX_pca2 = pca2.fit_transform(X_weighted)\nscatter_2d = axes[0].scatter(X_pca2[:, 0], X_pca2[:, 1], c=y, cmap='viridis', edgecolor='k', s=40)\nfig.colorbar(scatter_2d, ax=axes[0], orientation='vertical').set_label('Transit Depth (x1)', rotation=270, labelpad=15)\naxes[0].set_xlabel('Weighted Principal Component 1')\naxes[0].set_ylabel('Weighted Principal Component 2')\naxes[0].set_title('Weighted PCA: First Two Principal Components')\n\n# Perform PCA on the weighted data with three components and plot\npca3 = PCA(n_components=3)\nX_pca3 = pca3.fit_transform(X_weighted)\nax3d = fig.add_subplot(1, 2, 2, projection='3d')\nscatter_3d = ax3d.scatter(X_pca3[:, 0], X_pca3[:, 1], X_pca3[:, 2], c=y, cmap='viridis', edgecolor='k', s=40)\nfig.colorbar(scatter_3d, ax=ax3d, pad=0.2).set_label('Transit Depth (x1)', rotation=270, labelpad=15)\nax3d.set_xlabel('Weighted Principal Component 1')\nax3d.set_ylabel('Weighted Principal Component 2')\nax3d.set_zlabel('Weighted Principal Component 3')\nax3d.set_title('Weighted PCA: First Three Principal Components')\n\n# Display the combined plot\nplt.tight_layout()\nplt.savefig('wPCA_2D_3D.png', bbox_inches='tight', dpi=300)  # 'bbox_inches' ensures the entire plot is saved\n\nplt.show()\n\n\n\n\n\nimport pandas as pd\nimport seaborn as sns\n\n# Your data initialization (this part is assumed, as you haven't provided it in the original code)\n# df, feature_columns, transit_depth_column = ...\n\n# Reconstruct the approximate original data using the first three principal components\nreconstructed_data = np.dot(X_pca, pca.components_[:3, :]) + np.mean(X_scaled, axis=0)\n\n# Create a DataFrame for the reconstructed data\nreconstructed_df = pd.DataFrame(reconstructed_data, columns=feature_columns)\n\n# Visualize the approximate original histograms\nfig, axes = plt.subplots(1, len(feature_columns) + 1, figsize=(18, 4))  # Adjusted for the number of features\nfor i, col in enumerate(feature_columns):\n    sns.histplot(reconstructed_df[col], bins=20, kde=True, ax=axes[i], color='orange')\n    axes[i].set_title(f'Approx of {col}')\n\n# Add visualization for actual transit depth (since it was not part of the PCA)\nsns.histplot(df[transit_depth_column], bins=20, kde=True, ax=axes[-1], color='green')\naxes[-1].set_title(f'Actual {transit_depth_column}')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\nfig, axes = plt.subplots(2, 8, figsize=(18, 8))\n\n# Original Data\nfor i, col in enumerate(feature_columns):\n    sns.histplot(df[col], bins=20, kde=True, ax=axes[0, i])\n    axes[0, i].set_title(f'{col}\\nMean: {feature_mean[col]:.2f}\\nVariance: {feature_variance[col]:.2f}')\n\nsns.histplot(df[transit_depth_column], bins=20, kde=True, ax=axes[0, -1])\naxes[0, -1].set_title(f'{transit_depth_column}\\nMean: {transit_depth_mean:.2f}\\nVariance: {transit_depth_variance:.2f}')\n\n# Reconstructed Data\nfor i, col in enumerate(feature_columns):\n    sns.histplot(reconstructed_df[col], bins=20, kde=True, ax=axes[1, i], color='orange')\n    axes[1, i].set_title(f'Surrogate of {col}')\n\nsns.histplot(df[transit_depth_column], bins=20, kde=True, ax=axes[1, -1], color='green')\naxes[1, -1].set_title(f'Actual {transit_depth_column}')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nimport numpy as np\n\n# Assuming data initialization and feature_importances, feature_columns, and transit_depth_column definitions exist\n\n# Standardize the feature matrix\nX_scaled = StandardScaler().fit_transform(X)\n\n# Multiply each feature by its square root of importance weight for weighted PCA\nX_weighted = X_scaled * np.sqrt(feature_importances)\n\n# Initialize the figure and axes\nfig, axes = plt.subplots(3, len(feature_columns) + 1, figsize=(18, 12))\n\n# Original Data histograms\nfor i, col in enumerate(feature_columns):\n    sns.histplot(df[col], bins=20, kde=True, ax=axes[0, i])\n    axes[0, i].set_title(f'Original {col}')\n\nsns.histplot(df[transit_depth_column], bins=20, kde=True, ax=axes[0, -1])\naxes[0, -1].set_title(f'Original {transit_depth_column}')\n\n# Reconstruction using 2 PCs\npca2 = PCA(n_components=2)\nX_pca2 = pca2.fit_transform(X_weighted)\nreconstructed_data_2PCs = np.dot(X_pca2, pca2.components_[:2, :]) + np.mean(X_scaled, axis=0)\nreconstructed_df_2PCs = pd.DataFrame(reconstructed_data_2PCs, columns=feature_columns)\n\nfor i, col in enumerate(feature_columns):\n    sns.histplot(reconstructed_df_2PCs[col], bins=20, kde=True, ax=axes[1, i], color='orange')\n    axes[1, i].set_title(f'2 PCs of {col}')\n\nsns.histplot(df[transit_depth_column], bins=20, kde=True, ax=axes[1, -1], color='green')\naxes[1, -1].set_title(f'Original {transit_depth_column}')\n\n# Reconstruction using 3 PCs\npca3 = PCA(n_components=3)\nX_pca3 = pca3.fit_transform(X_weighted)\nreconstructed_data_3PCs = np.dot(X_pca3, pca3.components_[:3, :]) + np.mean(X_scaled, axis=0)\nreconstructed_df_3PCs = pd.DataFrame(reconstructed_data_3PCs, columns=feature_columns)\n\nfor i, col in enumerate(feature_columns):\n    sns.histplot(reconstructed_df_3PCs[col], bins=20, kde=True, ax=axes[2, i], color='purple')\n    axes[2, i].set_title(f'3 PCs of {col}')\n\nsns.histplot(df[transit_depth_column], bins=20, kde=True, ax=axes[2, -1], color='green')\naxes[2, -1].set_title(f'Original {transit_depth_column}')\n\nplt.tight_layout()\nplt.savefig('wPCA_combined_plot.png', bbox_inches='tight', dpi=500)  # 'bbox_inches' ensures the entire plot is saved\n\nplt.show()\n\n\n\n\n\n\n\nReconstruction of Exoplanet Data Using Principal Component Analysis (PCA)\nThe code segment aims to reconstruct exoplanet data using different numbers of principal components (PCs) from PCA, offering insights into how much information retention occurs as we vary the number of PCs.\n\nData Standardization:\n\nThe features undergo standardization, ensuring each feature has a mean of 0 and variance of 1. This normalization is crucial for PCA, as the algorithm is sensitive to varying scales across features.\n\n\n\nWeighted Features:\n\nFeatures are adjusted based on their significance as ascertained by a prior model, specifically the Random Forest. Weighting the features adjusts the emphasis the PCA places on each feature during the dimensionality reduction process.\n\n\n\nData Reconstruction:\n\nAfter performing PCA, the algorithm seeks to transform the reduced data back to the original high-dimensional space. This “reconstructed” data is an approximation of the original but is built using fewer dimensions.\n\n\n\nVisualization of Reconstructions:\n\nOriginal Data Histograms:\n\nThe initial histograms show the distributions of the original features and the transit depth.\n\nReconstruction Using 2 Principal Components:\n\nUsing only the first two PCs, the data is reconstructed and its histograms visualized. This illustrates the patterns and distributions captured when only the first two components are considered.\n\nReconstruction Using 3 Principal Components:\n\nAn analogous reconstruction is done with three PCs. The addition of a third dimension might capture more nuanced variations in the data.\n\n\n\n\nSignificance:\n\nData Compression vs. Retention:\n\nThe visualizations enable us to compare the reconstructions against the original data. We discern how much information is retained and what is lost as we reduce dimensions.\n\nGuide for Dimensionality Decision:\n\nBy juxtaposing the original with the reconstructions, we gain insights into the optimal number of PCs to use for specific tasks, striking a balance between compression and information retention.\n\nEmpirical Understanding:\n\nThese histograms and visual representations offer a tangible way to grasp the abstract notion of dimensionality reduction. They elucidate how PCA captures the essence of the data while diminishing dimensions.\n\n\nIn conclusion, this analysis, coupled with the visualizations, equips us with a robust understanding of how PCA reconstructs data using varying numbers of components. It underscores the trade-offs involved and the implications of choosing specific dimensionality levels in data-driven tasks.\n\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport matplotlib.gridspec as gridspec\n\n# Load images\nimg_shap = mpimg.imread('shapley.png')\nimg_rf = mpimg.imread('random_forest_importance_plot.png')\nimg_pme = mpimg.imread('PME.png')\n\n# Create a grid for the subplots\nfig_combined = plt.figure(figsize=(20, 12))\ngs = gridspec.GridSpec(2, 2, width_ratios=[1, 1])\n\n# Display SHAP plot\nax0 = plt.subplot(gs[0])\nax0.imshow(img_shap)\nax0.axis('off')\n\n# Display RF Importance plot\nax1 = plt.subplot(gs[1])\nax1.imshow(img_rf)\nax1.axis('off')\n\n# Display PME image in the middle of the 2nd row\nax2 = plt.subplot(gs[2:4])  # This makes the PME plot span both columns on the second row\nax2.imshow(img_pme)\nax2.axis('off')\n\nplt.tight_layout()\nplt.savefig('sensitivity_combined_plot.png', bbox_inches='tight', dpi=300)  # 'bbox_inches' ensures the entire plot is saved\n\nplt.show()\n\n\n\n\nThis code snippet consolidates and displays three distinct plots—SHAP values, Random Forest Feature Importance, and PME Feature Importance—into a single visualization. The images are loaded and arranged in a 2x2 grid, with the SHAP and Random Forest plots on the top row, and the PME plot spanning both columns on the bottom row. After layout adjustments, the combined visualization is saved as a high-resolution PNG image titled ‘sensitivity_combined_plot.png’ and then displayed.\n\n\n\nReference\nChangeat, Q., & Yip, K. H. (2023). ESA-Ariel Data Challenge NeurIPS 2022: Introduction to exo-atmospheric studies and presentation of the Atmospheric Big Challenge (ABC) Database. arXiv preprint arXiv:2206.14633.\nHerin, M., Il Idrissi, M., Chabridon, V., & Iooss, B. (2022). Proportional marginal effects for global sensitivity analysis. arXiv preprint arXiv:2210.13065.\n\n!jupyter nbconvert research_statement.ipynb --to markdown --NbConvertApp.output_files_dir=.\n\n[NbConvertApp] Converting notebook research_statement.ipynb to markdown\n[NbConvertApp] Support files will be in ./\n[NbConvertApp] Writing 36217 bytes to research_statement.md\n\n\n\n!cat research_statement.md | tee -a index.qmd\n\n```python\n# Import required libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n```\n\nThe figure below showcases a spectrum integral to exoplanet detection, charting the transit depth against the wavelength in microns. Every observation point in the spectrum carries an inherent uncertainty, denoted by the vertical error bars. To decode and potentially minimize this uncertainty, it's pivotal to fathom how features like `planet radius`, `planet temperature`, and the logarithmic concentrations of `H₂O`, `CO₂`, `CO`, `CH₄`, and `NH₃` influence the transit depth. Leveraging interpretative tools like SHAP can provide insights into how these exoplanetary features impact the observed transit depth, refining our understanding and accuracy in exoplanet spectral analysis.\n\n\n```python\nfrom IPython.display import Image\nImage(filename='my_spectrum.png')\n```\n\n\n\n\n    \n![png](./research_statement_2_0.png)\n    \n\n\n\n\n```python\n# Load the CSV file into a Pandas DataFrame\ndf = pd.read_csv(\"small_astro.csv\")\n```\n\n\n```python\ndf_backup = df.copy()\ndf_backup.head(10)\n```\n\n\n\n\n&lt;div&gt;\n&lt;style scoped&gt;\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n&lt;/style&gt;\n&lt;table border=\"1\" class=\"dataframe\"&gt;\n  &lt;thead&gt;\n    &lt;tr style=\"text-align: right;\"&gt;\n      &lt;th&gt;&lt;/th&gt;\n      &lt;th&gt;Unnamed: 0&lt;/th&gt;\n      &lt;th&gt;planet_radius&lt;/th&gt;\n      &lt;th&gt;planet_temp&lt;/th&gt;\n      &lt;th&gt;log_h2o&lt;/th&gt;\n      &lt;th&gt;log_co2&lt;/th&gt;\n      &lt;th&gt;log_co&lt;/th&gt;\n      &lt;th&gt;log_ch4&lt;/th&gt;\n      &lt;th&gt;log_nh3&lt;/th&gt;\n      &lt;th&gt;x1&lt;/th&gt;\n      &lt;th&gt;x2&lt;/th&gt;\n      &lt;th&gt;...&lt;/th&gt;\n      &lt;th&gt;x43&lt;/th&gt;\n      &lt;th&gt;x44&lt;/th&gt;\n      &lt;th&gt;x45&lt;/th&gt;\n      &lt;th&gt;x46&lt;/th&gt;\n      &lt;th&gt;x47&lt;/th&gt;\n      &lt;th&gt;x48&lt;/th&gt;\n      &lt;th&gt;x49&lt;/th&gt;\n      &lt;th&gt;x50&lt;/th&gt;\n      &lt;th&gt;x51&lt;/th&gt;\n      &lt;th&gt;x52&lt;/th&gt;\n    &lt;/tr&gt;\n  &lt;/thead&gt;\n  &lt;tbody&gt;\n    &lt;tr&gt;\n      &lt;th&gt;0&lt;/th&gt;\n      &lt;td&gt;1&lt;/td&gt;\n      &lt;td&gt;0.559620&lt;/td&gt;\n      &lt;td&gt;863.394770&lt;/td&gt;\n      &lt;td&gt;-8.865868&lt;/td&gt;\n      &lt;td&gt;-6.700707&lt;/td&gt;\n      &lt;td&gt;-5.557561&lt;/td&gt;\n      &lt;td&gt;-8.957615&lt;/td&gt;\n      &lt;td&gt;-3.097540&lt;/td&gt;\n      &lt;td&gt;0.003836&lt;/td&gt;\n      &lt;td&gt;0.003834&lt;/td&gt;\n      &lt;td&gt;...&lt;/td&gt;\n      &lt;td&gt;0.003938&lt;/td&gt;\n      &lt;td&gt;0.003941&lt;/td&gt;\n      &lt;td&gt;0.003903&lt;/td&gt;\n      &lt;td&gt;0.003931&lt;/td&gt;\n      &lt;td&gt;0.003983&lt;/td&gt;\n      &lt;td&gt;0.004019&lt;/td&gt;\n      &lt;td&gt;0.004046&lt;/td&gt;\n      &lt;td&gt;0.004072&lt;/td&gt;\n      &lt;td&gt;0.004054&lt;/td&gt;\n      &lt;td&gt;0.004056&lt;/td&gt;\n    &lt;/tr&gt;\n    &lt;tr&gt;\n      &lt;th&gt;1&lt;/th&gt;\n      &lt;td&gt;2&lt;/td&gt;\n      &lt;td&gt;1.118308&lt;/td&gt;\n      &lt;td&gt;1201.700465&lt;/td&gt;\n      &lt;td&gt;-4.510258&lt;/td&gt;\n      &lt;td&gt;-8.228966&lt;/td&gt;\n      &lt;td&gt;-3.565427&lt;/td&gt;\n      &lt;td&gt;-7.807424&lt;/td&gt;\n      &lt;td&gt;-3.633658&lt;/td&gt;\n      &lt;td&gt;0.015389&lt;/td&gt;\n      &lt;td&gt;0.015148&lt;/td&gt;\n      &lt;td&gt;...&lt;/td&gt;\n      &lt;td&gt;0.015450&lt;/td&gt;\n      &lt;td&gt;0.015447&lt;/td&gt;\n      &lt;td&gt;0.015461&lt;/td&gt;\n      &lt;td&gt;0.015765&lt;/td&gt;\n      &lt;td&gt;0.016099&lt;/td&gt;\n      &lt;td&gt;0.016376&lt;/td&gt;\n      &lt;td&gt;0.016549&lt;/td&gt;\n      &lt;td&gt;0.016838&lt;/td&gt;\n      &lt;td&gt;0.016781&lt;/td&gt;\n      &lt;td&gt;0.016894&lt;/td&gt;\n    &lt;/tr&gt;\n    &lt;tr&gt;\n      &lt;th&gt;2&lt;/th&gt;\n      &lt;td&gt;3&lt;/td&gt;\n      &lt;td&gt;0.400881&lt;/td&gt;\n      &lt;td&gt;1556.096477&lt;/td&gt;\n      &lt;td&gt;-7.225472&lt;/td&gt;\n      &lt;td&gt;-6.931472&lt;/td&gt;\n      &lt;td&gt;-3.081975&lt;/td&gt;\n      &lt;td&gt;-8.567854&lt;/td&gt;\n      &lt;td&gt;-5.378472&lt;/td&gt;\n      &lt;td&gt;0.002089&lt;/td&gt;\n      &lt;td&gt;0.002073&lt;/td&gt;\n      &lt;td&gt;...&lt;/td&gt;\n      &lt;td&gt;0.001989&lt;/td&gt;\n      &lt;td&gt;0.002168&lt;/td&gt;\n      &lt;td&gt;0.002176&lt;/td&gt;\n      &lt;td&gt;0.002123&lt;/td&gt;\n      &lt;td&gt;0.002079&lt;/td&gt;\n      &lt;td&gt;0.002081&lt;/td&gt;\n      &lt;td&gt;0.002106&lt;/td&gt;\n      &lt;td&gt;0.002167&lt;/td&gt;\n      &lt;td&gt;0.002149&lt;/td&gt;\n      &lt;td&gt;0.002185&lt;/td&gt;\n    &lt;/tr&gt;\n    &lt;tr&gt;\n      &lt;th&gt;3&lt;/th&gt;\n      &lt;td&gt;4&lt;/td&gt;\n      &lt;td&gt;0.345974&lt;/td&gt;\n      &lt;td&gt;1268.624884&lt;/td&gt;\n      &lt;td&gt;-7.461157&lt;/td&gt;\n      &lt;td&gt;-5.853334&lt;/td&gt;\n      &lt;td&gt;-3.044711&lt;/td&gt;\n      &lt;td&gt;-5.149378&lt;/td&gt;\n      &lt;td&gt;-3.815568&lt;/td&gt;\n      &lt;td&gt;0.002523&lt;/td&gt;\n      &lt;td&gt;0.002392&lt;/td&gt;\n      &lt;td&gt;...&lt;/td&gt;\n      &lt;td&gt;0.002745&lt;/td&gt;\n      &lt;td&gt;0.003947&lt;/td&gt;\n      &lt;td&gt;0.004296&lt;/td&gt;\n      &lt;td&gt;0.003528&lt;/td&gt;\n      &lt;td&gt;0.003352&lt;/td&gt;\n      &lt;td&gt;0.003629&lt;/td&gt;\n      &lt;td&gt;0.003929&lt;/td&gt;\n      &lt;td&gt;0.004363&lt;/td&gt;\n      &lt;td&gt;0.004216&lt;/td&gt;\n      &lt;td&gt;0.004442&lt;/td&gt;\n    &lt;/tr&gt;\n    &lt;tr&gt;\n      &lt;th&gt;4&lt;/th&gt;\n      &lt;td&gt;5&lt;/td&gt;\n      &lt;td&gt;0.733184&lt;/td&gt;\n      &lt;td&gt;1707.323564&lt;/td&gt;\n      &lt;td&gt;-4.140844&lt;/td&gt;\n      &lt;td&gt;-7.460278&lt;/td&gt;\n      &lt;td&gt;-3.181793&lt;/td&gt;\n      &lt;td&gt;-5.996593&lt;/td&gt;\n      &lt;td&gt;-4.535345&lt;/td&gt;\n      &lt;td&gt;0.002957&lt;/td&gt;\n      &lt;td&gt;0.002924&lt;/td&gt;\n      &lt;td&gt;...&lt;/td&gt;\n      &lt;td&gt;0.003402&lt;/td&gt;\n      &lt;td&gt;0.003575&lt;/td&gt;\n      &lt;td&gt;0.003667&lt;/td&gt;\n      &lt;td&gt;0.003740&lt;/td&gt;\n      &lt;td&gt;0.003823&lt;/td&gt;\n      &lt;td&gt;0.003904&lt;/td&gt;\n      &lt;td&gt;0.003897&lt;/td&gt;\n      &lt;td&gt;0.004004&lt;/td&gt;\n      &lt;td&gt;0.004111&lt;/td&gt;\n      &lt;td&gt;0.004121&lt;/td&gt;\n    &lt;/tr&gt;\n    &lt;tr&gt;\n      &lt;th&gt;5&lt;/th&gt;\n      &lt;td&gt;6&lt;/td&gt;\n      &lt;td&gt;0.161165&lt;/td&gt;\n      &lt;td&gt;620.185809&lt;/td&gt;\n      &lt;td&gt;-4.875000&lt;/td&gt;\n      &lt;td&gt;-5.074766&lt;/td&gt;\n      &lt;td&gt;-3.861240&lt;/td&gt;\n      &lt;td&gt;-5.388011&lt;/td&gt;\n      &lt;td&gt;-8.390503&lt;/td&gt;\n      &lt;td&gt;0.000444&lt;/td&gt;\n      &lt;td&gt;0.000442&lt;/td&gt;\n      &lt;td&gt;...&lt;/td&gt;\n      &lt;td&gt;0.000432&lt;/td&gt;\n      &lt;td&gt;0.000486&lt;/td&gt;\n      &lt;td&gt;0.000473&lt;/td&gt;\n      &lt;td&gt;0.000462&lt;/td&gt;\n      &lt;td&gt;0.000447&lt;/td&gt;\n      &lt;td&gt;0.000455&lt;/td&gt;\n      &lt;td&gt;0.000455&lt;/td&gt;\n      &lt;td&gt;0.000457&lt;/td&gt;\n      &lt;td&gt;0.000463&lt;/td&gt;\n      &lt;td&gt;0.000474&lt;/td&gt;\n    &lt;/tr&gt;\n    &lt;tr&gt;\n      &lt;th&gt;6&lt;/th&gt;\n      &lt;td&gt;7&lt;/td&gt;\n      &lt;td&gt;0.194312&lt;/td&gt;\n      &lt;td&gt;900.597575&lt;/td&gt;\n      &lt;td&gt;-8.299899&lt;/td&gt;\n      &lt;td&gt;-6.850709&lt;/td&gt;\n      &lt;td&gt;-4.314491&lt;/td&gt;\n      &lt;td&gt;-3.712038&lt;/td&gt;\n      &lt;td&gt;-3.951455&lt;/td&gt;\n      &lt;td&gt;0.001794&lt;/td&gt;\n      &lt;td&gt;0.001721&lt;/td&gt;\n      &lt;td&gt;...&lt;/td&gt;\n      &lt;td&gt;0.001048&lt;/td&gt;\n      &lt;td&gt;0.001052&lt;/td&gt;\n      &lt;td&gt;0.000948&lt;/td&gt;\n      &lt;td&gt;0.000976&lt;/td&gt;\n      &lt;td&gt;0.001122&lt;/td&gt;\n      &lt;td&gt;0.001274&lt;/td&gt;\n      &lt;td&gt;0.001395&lt;/td&gt;\n      &lt;td&gt;0.001522&lt;/td&gt;\n      &lt;td&gt;0.001456&lt;/td&gt;\n      &lt;td&gt;0.001823&lt;/td&gt;\n    &lt;/tr&gt;\n    &lt;tr&gt;\n      &lt;th&gt;7&lt;/th&gt;\n      &lt;td&gt;8&lt;/td&gt;\n      &lt;td&gt;1.132685&lt;/td&gt;\n      &lt;td&gt;1176.443900&lt;/td&gt;\n      &lt;td&gt;-6.765865&lt;/td&gt;\n      &lt;td&gt;-7.398548&lt;/td&gt;\n      &lt;td&gt;-3.378307&lt;/td&gt;\n      &lt;td&gt;-3.763737&lt;/td&gt;\n      &lt;td&gt;-5.881384&lt;/td&gt;\n      &lt;td&gt;0.012950&lt;/td&gt;\n      &lt;td&gt;0.012946&lt;/td&gt;\n      &lt;td&gt;...&lt;/td&gt;\n      &lt;td&gt;0.014019&lt;/td&gt;\n      &lt;td&gt;0.013871&lt;/td&gt;\n      &lt;td&gt;0.013810&lt;/td&gt;\n      &lt;td&gt;0.013902&lt;/td&gt;\n      &lt;td&gt;0.014024&lt;/td&gt;\n      &lt;td&gt;0.014150&lt;/td&gt;\n      &lt;td&gt;0.014298&lt;/td&gt;\n      &lt;td&gt;0.014392&lt;/td&gt;\n      &lt;td&gt;0.014401&lt;/td&gt;\n      &lt;td&gt;0.015042&lt;/td&gt;\n    &lt;/tr&gt;\n    &lt;tr&gt;\n      &lt;th&gt;8&lt;/th&gt;\n      &lt;td&gt;9&lt;/td&gt;\n      &lt;td&gt;0.158621&lt;/td&gt;\n      &lt;td&gt;1189.209841&lt;/td&gt;\n      &lt;td&gt;-8.376041&lt;/td&gt;\n      &lt;td&gt;-6.321977&lt;/td&gt;\n      &lt;td&gt;-3.243900&lt;/td&gt;\n      &lt;td&gt;-8.711851&lt;/td&gt;\n      &lt;td&gt;-3.449195&lt;/td&gt;\n      &lt;td&gt;0.000444&lt;/td&gt;\n      &lt;td&gt;0.000445&lt;/td&gt;\n      &lt;td&gt;...&lt;/td&gt;\n      &lt;td&gt;0.000562&lt;/td&gt;\n      &lt;td&gt;0.000595&lt;/td&gt;\n      &lt;td&gt;0.000571&lt;/td&gt;\n      &lt;td&gt;0.000590&lt;/td&gt;\n      &lt;td&gt;0.000628&lt;/td&gt;\n      &lt;td&gt;0.000663&lt;/td&gt;\n      &lt;td&gt;0.000692&lt;/td&gt;\n      &lt;td&gt;0.000734&lt;/td&gt;\n      &lt;td&gt;0.000718&lt;/td&gt;\n      &lt;td&gt;0.000736&lt;/td&gt;\n    &lt;/tr&gt;\n    &lt;tr&gt;\n      &lt;th&gt;9&lt;/th&gt;\n      &lt;td&gt;10&lt;/td&gt;\n      &lt;td&gt;0.660642&lt;/td&gt;\n      &lt;td&gt;528.023669&lt;/td&gt;\n      &lt;td&gt;-3.804286&lt;/td&gt;\n      &lt;td&gt;-8.919378&lt;/td&gt;\n      &lt;td&gt;-4.686964&lt;/td&gt;\n      &lt;td&gt;-8.150277&lt;/td&gt;\n      &lt;td&gt;-3.068319&lt;/td&gt;\n      &lt;td&gt;0.008997&lt;/td&gt;\n      &lt;td&gt;0.009035&lt;/td&gt;\n      &lt;td&gt;...&lt;/td&gt;\n      &lt;td&gt;0.009435&lt;/td&gt;\n      &lt;td&gt;0.009375&lt;/td&gt;\n      &lt;td&gt;0.009315&lt;/td&gt;\n      &lt;td&gt;0.009357&lt;/td&gt;\n      &lt;td&gt;0.009563&lt;/td&gt;\n      &lt;td&gt;0.009739&lt;/td&gt;\n      &lt;td&gt;0.009821&lt;/td&gt;\n      &lt;td&gt;0.009890&lt;/td&gt;\n      &lt;td&gt;0.009819&lt;/td&gt;\n      &lt;td&gt;0.009734&lt;/td&gt;\n    &lt;/tr&gt;\n  &lt;/tbody&gt;\n&lt;/table&gt;\n&lt;p&gt;10 rows × 60 columns&lt;/p&gt;\n&lt;/div&gt;\n\n\n\n\n```python\n# Columns of interest for planetary and chemical properties and transit depth\nfeature_columns = df.columns[1:8]\ntransit_depth_column = 'x1' # pick the first wavelength\n\n# Calculate mean and variance for features and transit depth\nfeature_mean = df[feature_columns].mean()\nfeature_variance = df[feature_columns].var()\ntransit_depth_mean = df[transit_depth_column].mean()\ntransit_depth_variance = df[transit_depth_column].var()\n\n# Visualize the distributions\nfig, axes = plt.subplots(1, 8, figsize=(18, 4))\nfor i, col in enumerate(feature_columns):\n    sns.histplot(df[col], bins=20, kde=True, ax=axes[i])\n    axes[i].set_title(f'{col}\\nMean: {feature_mean[col]:.2f}\\nVariance: {feature_variance[col]:.2f}')\n\n# Add visualization for transit depth\nsns.histplot(df[transit_depth_column], bins=20, kde=True, ax=axes[-1])\naxes[-1].set_title(f'{transit_depth_column}\\nMean: {transit_depth_mean:.2f}\\nVariance: {transit_depth_variance:.2f}')\n\nplt.tight_layout()\nplt.show()\n\nfeature_mean, feature_variance, transit_depth_mean, transit_depth_variance\n```\n\n\n    \n![png](./research_statement_5_0.png)\n    \n\n\n\n\n\n    (planet_radius       0.703714\n     planet_temp      1073.229674\n     log_h2o            -5.934889\n     log_co2            -6.873009\n     log_co             -4.497141\n     log_ch4            -5.799850\n     log_nh3            -6.051791\n     dtype: float64,\n     planet_radius         0.198990\n     planet_temp      154495.743225\n     log_h2o               3.130868\n     log_co2               1.649658\n     log_co                0.738255\n     log_ch4               3.208283\n     log_nh3               3.050545\n     dtype: float64,\n     0.009442470522591322,\n     0.00016172106267489707)\n\n\n\n## Exoplanet Feature Distributions\n\nThe provided visualizations and statistics shed light on the distribution of various exoplanetary features and the observed transit depth at the `x1` wavelength.\n\n- **Planet Radius**: This feature, with a mean value of approximately 0.7037 and variance of 0.1989, mostly lies between 0.5 and 1.5 as depicted in its histogram.\n  \n- **Planet Temperature**: Exhibiting a wider spread, the temperature has a mean of approximately 1073.29 K and variance of 154495.74 K².\n\n- **Logarithmic Concentrations**:\n  - **H₂O**: Mean concentration of -5.93 with a variance of 3.13.\n  - **CO₂**: Mean concentration of -6.87 with a variance of 1.65.\n  - **CO**: Mean concentration of -4.50 with a variance of 0.74.\n  - **CH₄**: Mean concentration of -5.80 with a variance of 3.21.\n  - **NH₃**: Mean concentration of -6.05 with a variance of 3.05.\n\n- **Transit Depth at x1 Wavelength**: This depth, crucial for exoplanet detection, has an almost singular value near 0, with a mean of approximately 0.0094 and a negligible variance of 0.00006.\n\nThese distributions and their accompanying statistics offer invaluable insights into the data's nature and its inherent variability, essential for accurate spectral analysis and interpretation.\n\n\n\n```python\n# Import required libraries for modeling and SHAP values\nimport xgboost as xgb\nimport shap\n\n# Prepare the feature matrix (X) and the target vector (y)\nX = df.iloc[:, 1:8]\ny = df['x1']\n\n# Train an XGBoost model\nmodel = xgb.XGBRegressor(objective ='reg:squarederror')\nmodel.fit(X, y)\n\n# Initialize the SHAP explainer\nexplainer = shap.Explainer(model)\n\n# Calculate SHAP values\nshap_values = explainer(X)\n\n# Summary plot for SHAP values\nshap.summary_plot(shap_values, X, title='Shapley Feature Importance')\n\n```\n\n    Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n\n\n\n    \n![png](./research_statement_7_1.png)\n    \n\n\nSHAP (SHapley Additive exPlanations) values stem from cooperative game theory and provide a way to interpret machine learning model predictions. Each feature in a model is analogous to a player in a game, and the contribution of each feature to a prediction is like the payout a player receives in the game. In SHAP, we calculate the value of each feature by considering all possible combinations (coalitions) of features, assessing the change in prediction with and without that feature. The resulting value, the Shapley value, represents the average contribution of a feature to all possible predictions.\n\nThe summary plot visualizes these values. Each dot represents a SHAP value for a specific instance of a feature; positive values (right of the centerline) indicate that a feature increases the model's output, while negative values (left of the centerline) suggest a decrease. The color depicts the actual value of the feature for the given instance, enabling a comprehensive view of feature influence across the dataset.\n\n\n\n```python\n# Import the Random Forest Regressor and visualization libraries\nfrom sklearn.ensemble import RandomForestRegressor\n\n# Train a Random Forest model\nrf_model = RandomForestRegressor(n_estimators=100, random_state=42)\nrf_model.fit(X, y)\n\n# Extract feature importances\nfeature_importances = rf_model.feature_importances_\n\n# Create a DataFrame for visualization\nimportance_df = pd.DataFrame({\n    'Feature': feature_columns,\n    'Importance': feature_importances\n}).sort_values(by='Importance', ascending=True)\n\n# Create a horizontal bar chart for feature importances\nplt.figure(figsize=(10, 6))\nplt.barh(importance_df['Feature'], importance_df['Importance'], color='dodgerblue')\nplt.xlabel('Importance')\nplt.ylabel('')\nplt.title('Random Forest Feature Importance')\nplt.xticks(fontsize=13)  # Increase the font size of the x-axis tick labels\nplt.yticks(fontsize=14)  # Increase the font size of the x-axis tick labels\n# Save the figure before showing it\nplt.savefig('random_forest_importance_plot.png', bbox_inches='tight', dpi=300)  # 'bbox_inches' ensures the entire plot is saved\n\nplt.show()\n\n\n\nfeature_importances\n```\n\n\n    \n![png](./research_statement_9_0.png)\n    \n\n\n\n\n\n    array([0.71850575, 0.10367982, 0.01655567, 0.07232808, 0.05603188,\n           0.0146824 , 0.0182164 ])\n\n\n\n## Random Forest Feature Importance Analysis\n\nTo gain insights into which exoplanetary features most influence the observed transit depth, a Random Forest Regressor was utilized. Here's an outline of the procedure:\n\n1. **Model Initialization**: A Random Forest Regressor model was instantiated with 100 trees and a fixed random seed of 42 for reproducibility.\n\n2. **Model Training**: The model was trained on the feature set `X` and target variable `y`.\n\n3. **Feature Importance Extraction**: After training, the importance of each feature was extracted using the `feature_importances_` attribute of the trained model.\n\n4. **Data Preparation for Visualization**: A DataFrame was created to house each feature alongside its respective importance. The features were then sorted in ascending order of importance for better visualization.\n\n5. **Visualization**: A horizontal bar chart was plotted to showcase the importance of each feature. The chart offers a clear visual comparison, with the y-axis representing the features and the x-axis indicating their importance. Special attention was paid to font size adjustments for better readability. Furthermore, before displaying the chart, it was saved as a PNG image with high resolution.\n\nThe resulting visualization, titled 'Random Forest Feature Importance', provides a clear understanding of the relative significance of each feature in predicting the transit depth, as discerned by the Random Forest model.\n\n\n\n```python\nfrom IPython.display import Image\nImage(filename='PME.png')\n\n```\n\n\n\n\n    \n![png](./research_statement_11_0.png)\n    \n\n\n\n## PME Feature Importance Analysis\n\nUsing a modeling technique, the impact of different exoplanetary features on the observed transit depth was assessed, and their importance was visualized in the attached figure titled 'PME Feature Importance'. Here's a breakdown of the visual representation:\n\n1. **Most Influential Feature**: The `planet_radius` stands out as the most influential feature with the highest PME (Predictive Modeling Estimate) value. This suggests that the radius of the planet plays a pivotal role in determining the observed transit depth.\n\n2. **Other Features**: Logarithmic concentrations of gases, such as `log_co2`, `log_co`, `log_nh3`, `log_h2o`, and `log_ch4`, also exhibit varying degrees of importance. Among these, `log_co2` and `log_co` are the more significant contributors compared to others.\n\n3. **Least Influential Feature**: The `planet_temp`, representing the temperature of the planet, has the least importance in this analysis, suggesting its minimal role in influencing the transit depth, at least according to the PME metric.\n\n4. **Visual Clarity**: The horizontal bar chart offers a lucid comparison of feature importances. Each bar's length represents the PME value of a feature, providing a direct visual cue to its significance.\n\n5. **Interpretation**: This visualization aids in discerning which exoplanetary characteristics are most relevant when predicting the transit depth using the given model. It can guide future analyses by highlighting key features to focus on or, conversely, those that might be less consequential.\n\nBy examining the 'PME Feature Importance' chart, one gains a deeper understanding of the relative significance of each feature in predicting the transit depth within this specific modeling context.\n\n\n## Uncertainty Quantification and Feature Reduction in PME Feature Importance Analysis\n\nWhen delving deep into the realms of uncertainty quantification and feature reduction in predictive modeling, it's crucial to evaluate feature importance metrics critically. The provided PME (Predictive Modeling Estimate) Feature Importance Analysis offers a valuable lens for this task. Below is a nuanced exploration of its significance in the described contexts:\n\n### **Uncertainty Quantification**:\n\n1. **Origin of Uncertainty**: \n    - In exoplanetary spectral analysis, uncertainty can arise from observational noise, instrumental errors, or intrinsic variability of the observed phenomena. This uncertainty often manifests in the form of error bars in spectra, like the ones shown in transit depth against wavelengths.\n\n2. **Feature Impact on Uncertainty**: \n    - The degree of influence a feature has on the predicted outcome can be a proxy for how that feature might contribute to the overall predictive uncertainty. If a feature like `planet_radius` has a high PME value, it might be a critical determinant of transit depth. Any uncertainty in measuring or estimating the `planet_radius` could propagate and significantly affect the prediction's reliability.\n\n3. **PME as a Measure of Stochastic Uncertainty**: \n    - The PME values themselves might be obtained by analyzing a model's sensitivity to perturbations in input features. A high PME value indicates that slight changes in the feature can lead to notable changes in the output, thereby implying a greater inherent stochastic uncertainty tied to that feature.\n\n### **Feature Reduction**:\n\n1. **Identifying Critical Features**: \n    - When dealing with a multitude of features, not all may be equally relevant. The PME analysis provides a hierarchy of feature importance. In this case, while `planet_radius` emerges as crucial, `planet_temp` appears less consequential. This differentiation is fundamental for feature reduction, guiding us on which features to prioritize in modeling.\n\n2. **Reducing Dimensionality & Complexity**: \n    - In data-driven modeling, especially with limited data points, overfitting is a genuine concern. By understanding which features significantly influence the predictions (like `planet_radius` or `log_co2`), one can potentially reduce the model's complexity and the risk of overfitting by focusing only on these paramount features.\n\n3. **Informing Experimental Design**: \n    - If further observational or experimental data is required, knowing feature importances can guide where resources are channeled. For instance, more precise measurements might be sought for features with high PME values, as their accurate estimation is vital for reliable predictions.\n\n4. **Trade-off with Predictive Performance**: \n    - It's essential to understand that while feature reduction can simplify models and make them more interpretable, there's always a trade-off with predictive performance. Removing features based on their PME values should be done judiciously, ensuring that the model's predictive capability isn't unduly compromised.\n\nIn summary, the 'PME Feature Importance' chart isn't merely a representation of feature significance but serves as a cornerstone for rigorous analytical decisions in uncertainty quantification and feature reduction. Analyzing such importance metrics within the broader context of the problem at hand ensures that models are both robust and interpretable, catering effectively to the dual objectives of predictive accuracy and analytical clarity.\n\n\n### Weighted Principal Component Analysis (PCA) on Exoplanet Data\n\n#### **Conceptual Overview**:\n\nPCA is a method used to emphasize variation and capture strong patterns in a dataset. The \"weighted PCA\" approach fine-tunes this by considering the importance of different features, effectively giving more attention to features deemed vital.\n\n#### **Detailed Breakdown**:\n\n1. **Standardization**:\n    - Before applying PCA, the dataset is standardized to give each feature a mean of 0 and a variance of 1. This is essential because PCA is influenced by the scale of the data.\n\n2. **Weighted Features**:\n    - Features are weighted according to their importance, as identified by the Random Forest model. Taking the square root of the weights ensures proper scaling during matrix multiplication in PCA.\n\n3. **PCA Application**:\n    - PCA projects the data into a new space defined by its principal components. The first two components often hold most of the dataset's variance, making them crucial for visualization.\n\n4. **Visualization**:\n    - The scatter plot visualizes the data in the space of the first two principal components. The color indicates `Transit Depth (x1)`, shedding light on how this parameter varies across the main patterns in the data.\n\n5. **Explained Variance**:\n    - This provides an understanding of how much original variance the first two components capture. A high percentage indicates that the PCA representation retains much of the data's original structure.\n\n#### **Significance**:\n\n1. **Data Compression**:\n    - PCA offers a simplified yet rich representation of the data, which can be invaluable for visualization and pattern recognition.\n\n2. **Feature Emphasis**:\n    - Using feature importances ensures the PCA representation highlights the most critical patterns related to influential features.\n\n3. **Framework for Further Exploration**:\n    - Observing patterns or groupings in the PCA plot can guide subsequent investigations, pinpointing areas of interest or potential clusters.\n\n4. **Efficient Data Overview**:\n    - The visualization provides a comprehensive but digestible overview of the data, suitable for a wide range of audiences.\n\nIn essence, weighted PCA melds the dimensionality reduction capabilities of PCA with the interpretative power of feature importances, offering a profound view into the dataset's intricate structures and relationships.\n\n\n\n\n```python\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nimport numpy as np\n\n# Standardize the feature matrix\nX_scaled = StandardScaler().fit_transform(X)\n\n# Multiply each feature by its square root of importance weight for weighted PCA\n# The square root is used because each feature contributes to both rows and columns in the dot product calculation\nX_weighted = X_scaled * np.sqrt(feature_importances)\n\n# Perform PCA on the weighted data\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X_weighted)\n\n# Plotting the first two principal components\nplt.figure(figsize=(10, 7))\nplt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis', edgecolor='k', s=40)\nplt.colorbar().set_label('Transit Depth (x1)', rotation=270, labelpad=15)\nplt.xlabel('Weighted Principal Component 1')\nplt.ylabel('Weighted Principal Component 2')\nplt.title('Weighted PCA: First Two Principal Components')\nplt.show()\n\n# Variance explained by the first two principal components\nvariance_explained = pca.explained_variance_ratio_\nvariance_explained\n\n```\n\n\n    \n![png](./research_statement_15_0.png)\n    \n\n\n\n\n\n    array([0.71985328, 0.10370239])\n\n\n\n\n```python\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\n# Your data initialization (this part is assumed, as you haven't provided it in the original code)\n# X, y, feature_importances = ...\n\n# Standardize the feature matrix\nX_scaled = StandardScaler().fit_transform(X)\n\n# Multiply each feature by its square root of importance weight for weighted PCA\nX_weighted = X_scaled * np.sqrt(feature_importances)\n\n# Perform PCA on the weighted data with three components\npca = PCA(n_components=3)\nX_pca = pca.fit_transform(X_weighted)\n\n# Plotting the first three principal components\nfig = plt.figure(figsize=(10, 7))\nax = fig.add_subplot(111, projection='3d')\nscatter = ax.scatter(X_pca[:, 0], X_pca[:, 1], X_pca[:, 2], c=y, cmap='viridis', edgecolor='k', s=40)\nplt.colorbar(scatter, ax=ax, pad=0.2).set_label('Transit Depth (x1)', rotation=270, labelpad=15)\nax.set_xlabel('Weighted Principal Component 1')\nax.set_ylabel('Weighted Principal Component 2')\nax.set_zlabel('Weighted Principal Component 3')\nplt.title('Weighted PCA: First Three Principal Components')\nplt.show()\n\n# Variance explained by the first three principal components\nvariance_explained = pca.explained_variance_ratio_\nvariance_explained\n\n```\n\n\n    \n![png](./research_statement_16_0.png)\n    \n\n\n\n\n\n    array([0.71985328, 0.10370239, 0.07160805])\n\n\n\n\n```python\n# side by side\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\n# Your data initialization (this part is assumed, as you haven't provided it in the original code)\n# X, y, feature_importances = ...\n\n# Standardize the feature matrix\nX_scaled = StandardScaler().fit_transform(X)\n\n# Multiply each feature by its square root of importance weight for weighted PCA\nX_weighted = X_scaled * np.sqrt(feature_importances)\n\n# Create a combined figure with subplots\nfig, axes = plt.subplots(1, 2, figsize=(20, 7))\n\n# Perform PCA on the weighted data with two components and plot\npca2 = PCA(n_components=2)\nX_pca2 = pca2.fit_transform(X_weighted)\nscatter_2d = axes[0].scatter(X_pca2[:, 0], X_pca2[:, 1], c=y, cmap='viridis', edgecolor='k', s=40)\nfig.colorbar(scatter_2d, ax=axes[0], orientation='vertical').set_label('Transit Depth (x1)', rotation=270, labelpad=15)\naxes[0].set_xlabel('Weighted Principal Component 1')\naxes[0].set_ylabel('Weighted Principal Component 2')\naxes[0].set_title('Weighted PCA: First Two Principal Components')\n\n# Perform PCA on the weighted data with three components and plot\npca3 = PCA(n_components=3)\nX_pca3 = pca3.fit_transform(X_weighted)\nax3d = fig.add_subplot(1, 2, 2, projection='3d')\nscatter_3d = ax3d.scatter(X_pca3[:, 0], X_pca3[:, 1], X_pca3[:, 2], c=y, cmap='viridis', edgecolor='k', s=40)\nfig.colorbar(scatter_3d, ax=ax3d, pad=0.2).set_label('Transit Depth (x1)', rotation=270, labelpad=15)\nax3d.set_xlabel('Weighted Principal Component 1')\nax3d.set_ylabel('Weighted Principal Component 2')\nax3d.set_zlabel('Weighted Principal Component 3')\nax3d.set_title('Weighted PCA: First Three Principal Components')\n\n# Display the combined plot\nplt.tight_layout()\nplt.savefig('wPCA_2D_3D.png', bbox_inches='tight', dpi=300)  # 'bbox_inches' ensures the entire plot is saved\n\nplt.show()\n\n```\n\n\n    \n![png](./research_statement_17_0.png)\n    \n\n\n\n```python\nimport pandas as pd\nimport seaborn as sns\n\n# Your data initialization (this part is assumed, as you haven't provided it in the original code)\n# df, feature_columns, transit_depth_column = ...\n\n# Reconstruct the approximate original data using the first three principal components\nreconstructed_data = np.dot(X_pca, pca.components_[:3, :]) + np.mean(X_scaled, axis=0)\n\n# Create a DataFrame for the reconstructed data\nreconstructed_df = pd.DataFrame(reconstructed_data, columns=feature_columns)\n\n# Visualize the approximate original histograms\nfig, axes = plt.subplots(1, len(feature_columns) + 1, figsize=(18, 4))  # Adjusted for the number of features\nfor i, col in enumerate(feature_columns):\n    sns.histplot(reconstructed_df[col], bins=20, kde=True, ax=axes[i], color='orange')\n    axes[i].set_title(f'Approx of {col}')\n\n# Add visualization for actual transit depth (since it was not part of the PCA)\nsns.histplot(df[transit_depth_column], bins=20, kde=True, ax=axes[-1], color='green')\naxes[-1].set_title(f'Actual {transit_depth_column}')\n\nplt.tight_layout()\nplt.show()\n\n```\n\n\n    \n![png](./research_statement_18_0.png)\n    \n\n\n\n```python\n\nfig, axes = plt.subplots(2, 8, figsize=(18, 8))\n\n# Original Data\nfor i, col in enumerate(feature_columns):\n    sns.histplot(df[col], bins=20, kde=True, ax=axes[0, i])\n    axes[0, i].set_title(f'{col}\\nMean: {feature_mean[col]:.2f}\\nVariance: {feature_variance[col]:.2f}')\n\nsns.histplot(df[transit_depth_column], bins=20, kde=True, ax=axes[0, -1])\naxes[0, -1].set_title(f'{transit_depth_column}\\nMean: {transit_depth_mean:.2f}\\nVariance: {transit_depth_variance:.2f}')\n\n# Reconstructed Data\nfor i, col in enumerate(feature_columns):\n    sns.histplot(reconstructed_df[col], bins=20, kde=True, ax=axes[1, i], color='orange')\n    axes[1, i].set_title(f'Surrogate of {col}')\n\nsns.histplot(df[transit_depth_column], bins=20, kde=True, ax=axes[1, -1], color='green')\naxes[1, -1].set_title(f'Actual {transit_depth_column}')\n\nplt.tight_layout()\nplt.show()\n\n```\n\n\n    \n![png](./research_statement_19_0.png)\n    \n\n\n\n```python\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nimport numpy as np\n\n# Assuming data initialization and feature_importances, feature_columns, and transit_depth_column definitions exist\n\n# Standardize the feature matrix\nX_scaled = StandardScaler().fit_transform(X)\n\n# Multiply each feature by its square root of importance weight for weighted PCA\nX_weighted = X_scaled * np.sqrt(feature_importances)\n\n# Initialize the figure and axes\nfig, axes = plt.subplots(3, len(feature_columns) + 1, figsize=(18, 12))\n\n# Original Data histograms\nfor i, col in enumerate(feature_columns):\n    sns.histplot(df[col], bins=20, kde=True, ax=axes[0, i])\n    axes[0, i].set_title(f'Original {col}')\n\nsns.histplot(df[transit_depth_column], bins=20, kde=True, ax=axes[0, -1])\naxes[0, -1].set_title(f'Original {transit_depth_column}')\n\n# Reconstruction using 2 PCs\npca2 = PCA(n_components=2)\nX_pca2 = pca2.fit_transform(X_weighted)\nreconstructed_data_2PCs = np.dot(X_pca2, pca2.components_[:2, :]) + np.mean(X_scaled, axis=0)\nreconstructed_df_2PCs = pd.DataFrame(reconstructed_data_2PCs, columns=feature_columns)\n\nfor i, col in enumerate(feature_columns):\n    sns.histplot(reconstructed_df_2PCs[col], bins=20, kde=True, ax=axes[1, i], color='orange')\n    axes[1, i].set_title(f'2 PCs of {col}')\n\nsns.histplot(df[transit_depth_column], bins=20, kde=True, ax=axes[1, -1], color='green')\naxes[1, -1].set_title(f'Original {transit_depth_column}')\n\n# Reconstruction using 3 PCs\npca3 = PCA(n_components=3)\nX_pca3 = pca3.fit_transform(X_weighted)\nreconstructed_data_3PCs = np.dot(X_pca3, pca3.components_[:3, :]) + np.mean(X_scaled, axis=0)\nreconstructed_df_3PCs = pd.DataFrame(reconstructed_data_3PCs, columns=feature_columns)\n\nfor i, col in enumerate(feature_columns):\n    sns.histplot(reconstructed_df_3PCs[col], bins=20, kde=True, ax=axes[2, i], color='purple')\n    axes[2, i].set_title(f'3 PCs of {col}')\n\nsns.histplot(df[transit_depth_column], bins=20, kde=True, ax=axes[2, -1], color='green')\naxes[2, -1].set_title(f'Original {transit_depth_column}')\n\nplt.tight_layout()\nplt.savefig('wPCA_combined_plot.png', bbox_inches='tight', dpi=500)  # 'bbox_inches' ensures the entire plot is saved\n\nplt.show()\n\n```\n\n\n    \n![png](./research_statement_20_0.png)\n    \n\n\n### Reconstruction of Exoplanet Data Using Principal Component Analysis (PCA)\n\nThe code segment aims to reconstruct exoplanet data using different numbers of principal components (PCs) from PCA, offering insights into how much information retention occurs as we vary the number of PCs.\n\n#### **Data Standardization**:\n- The features undergo standardization, ensuring each feature has a mean of 0 and variance of 1. This normalization is crucial for PCA, as the algorithm is sensitive to varying scales across features.\n\n#### **Weighted Features**:\n- Features are adjusted based on their significance as ascertained by a prior model, specifically the Random Forest. Weighting the features adjusts the emphasis the PCA places on each feature during the dimensionality reduction process.\n\n#### **Data Reconstruction**:\n- After performing PCA, the algorithm seeks to transform the reduced data back to the original high-dimensional space. This \"reconstructed\" data is an approximation of the original but is built using fewer dimensions.\n\n#### **Visualization of Reconstructions**:\n1. **Original Data Histograms**:\n    - The initial histograms show the distributions of the original features and the transit depth.\n  \n2. **Reconstruction Using 2 Principal Components**:\n    - Using only the first two PCs, the data is reconstructed and its histograms visualized. This illustrates the patterns and distributions captured when only the first two components are considered.\n  \n3. **Reconstruction Using 3 Principal Components**:\n    - An analogous reconstruction is done with three PCs. The addition of a third dimension might capture more nuanced variations in the data.\n\n#### **Significance**:\n1. **Data Compression vs. Retention**:\n    - The visualizations enable us to compare the reconstructions against the original data. We discern how much information is retained and what is lost as we reduce dimensions.\n  \n2. **Guide for Dimensionality Decision**:\n    - By juxtaposing the original with the reconstructions, we gain insights into the optimal number of PCs to use for specific tasks, striking a balance between compression and information retention.\n  \n3. **Empirical Understanding**:\n    - These histograms and visual representations offer a tangible way to grasp the abstract notion of dimensionality reduction. They elucidate how PCA captures the essence of the data while diminishing dimensions.\n\nIn conclusion, this analysis, coupled with the visualizations, equips us with a robust understanding of how PCA reconstructs data using varying numbers of components. It underscores the trade-offs involved and the implications of choosing specific dimensionality levels in data-driven tasks.\n\n\n\n\n```python\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport matplotlib.gridspec as gridspec\n\n# Load images\nimg_shap = mpimg.imread('shapley.png')\nimg_rf = mpimg.imread('random_forest_importance_plot.png')\nimg_pme = mpimg.imread('PME.png')\n\n# Create a grid for the subplots\nfig_combined = plt.figure(figsize=(20, 12))\ngs = gridspec.GridSpec(2, 2, width_ratios=[1, 1])\n\n# Display SHAP plot\nax0 = plt.subplot(gs[0])\nax0.imshow(img_shap)\nax0.axis('off')\n\n# Display RF Importance plot\nax1 = plt.subplot(gs[1])\nax1.imshow(img_rf)\nax1.axis('off')\n\n# Display PME image in the middle of the 2nd row\nax2 = plt.subplot(gs[2:4])  # This makes the PME plot span both columns on the second row\nax2.imshow(img_pme)\nax2.axis('off')\n\nplt.tight_layout()\nplt.savefig('sensitivity_combined_plot.png', bbox_inches='tight', dpi=300)  # 'bbox_inches' ensures the entire plot is saved\n\nplt.show()\n\n```\n\n\n    \n![png](./research_statement_22_0.png)\n    \n\n\n\nThis code snippet consolidates and displays three distinct plots—SHAP values, Random Forest Feature Importance, and PME Feature Importance—into a single visualization. The images are loaded and arranged in a 2x2 grid, with the SHAP and Random Forest plots on the top row, and the PME plot spanning both columns on the bottom row. After layout adjustments, the combined visualization is saved as a high-resolution PNG image titled 'sensitivity_combined_plot.png' and then displayed.\n\n### Reference\n\nChangeat, Q., & Yip, K. H. (2023). ESA-Ariel Data Challenge NeurIPS 2022: Introduction to exo-atmospheric studies and presentation of the Atmospheric Big Challenge (ABC) Database. *arXiv preprint arXiv:2206.14633*.\n\nHerin, M., Il Idrissi, M., Chabridon, V., & Iooss, B. (2022). Proportional marginal effects for global sensitivity analysis. arXiv preprint arXiv:2210.13065.\n\n\n\n\n\n```python\n\n```\n\n\n\n!rm research_statement.md"
  },
  {
    "objectID": "Research Projects/Exoplanet Modeling/index.html",
    "href": "Research Projects/Exoplanet Modeling/index.html",
    "title": "Exoplanet Modeling",
    "section": "",
    "text": "The figure below showcases a spectrum integral to exoplanet detection, charting the transit depth against the wavelength in microns. Every observation point in the spectrum carries an inherent uncertainty, denoted by the vertical error bars. To decode and potentially minimize this uncertainty, it’s pivotal to fathom how features like planet radius, planet temperature, and the logarithmic concentrations of H₂O, CO₂, CO, CH₄, and NH₃ influence the transit depth. Leveraging interpretative tools like SHAP can provide insights into how these exoplanetary features impact the observed transit depth, refining our understanding and accuracy in exoplanet spectral analysis."
  },
  {
    "objectID": "Research Projects/Exoplanet Modeling/index.html#exoplanet-feature-distributions",
    "href": "Research Projects/Exoplanet Modeling/index.html#exoplanet-feature-distributions",
    "title": "Exoplanet Modeling",
    "section": "Exoplanet Feature Distributions",
    "text": "Exoplanet Feature Distributions\nThe provided visualizations and statistics shed light on the distribution of various exoplanetary features and the observed transit depth at the x1 wavelength.\n\nPlanet Radius: This feature, with a mean value of approximately 0.7037 and variance of 0.1989, mostly lies between 0.5 and 1.5 as depicted in its histogram.\nPlanet Temperature: Exhibiting a wider spread, the temperature has a mean of approximately 1073.29 K and variance of 154495.74 K².\nLogarithmic Concentrations:\n\nH₂O: Mean concentration of -5.93 with a variance of 3.13.\nCO₂: Mean concentration of -6.87 with a variance of 1.65.\nCO: Mean concentration of -4.50 with a variance of 0.74.\nCH₄: Mean concentration of -5.80 with a variance of 3.21.\nNH₃: Mean concentration of -6.05 with a variance of 3.05.\n\nTransit Depth at x1 Wavelength: This depth, crucial for exoplanet detection, has an almost singular value near 0, with a mean of approximately 0.0094 and a negligible variance of 0.00006.\n\nThese distributions and their accompanying statistics offer invaluable insights into the data’s nature and its inherent variability, essential for accurate spectral analysis and interpretation.\n# Import required libraries for modeling and SHAP values\nimport xgboost as xgb\nimport shap\n\n# Prepare the feature matrix (X) and the target vector (y)\nX = df.iloc[:, 1:8]\ny = df['x1']\n\n# Train an XGBoost model\nmodel = xgb.XGBRegressor(objective ='reg:squarederror')\nmodel.fit(X, y)\n\n# Initialize the SHAP explainer\nexplainer = shap.Explainer(model)\n\n# Calculate SHAP values\nshap_values = explainer(X)\n\n# Summary plot for SHAP values\nshap.summary_plot(shap_values, X, title='Shapley Feature Importance')\nUsing `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n\n\n\npng\n\n\nSHAP (SHapley Additive exPlanations) values stem from cooperative game theory and provide a way to interpret machine learning model predictions. Each feature in a model is analogous to a player in a game, and the contribution of each feature to a prediction is like the payout a player receives in the game. In SHAP, we calculate the value of each feature by considering all possible combinations (coalitions) of features, assessing the change in prediction with and without that feature. The resulting value, the Shapley value, represents the average contribution of a feature to all possible predictions.\nThe summary plot visualizes these values. Each dot represents a SHAP value for a specific instance of a feature; positive values (right of the centerline) indicate that a feature increases the model’s output, while negative values (left of the centerline) suggest a decrease. The color depicts the actual value of the feature for the given instance, enabling a comprehensive view of feature influence across the dataset.\n# Import the Random Forest Regressor and visualization libraries\nfrom sklearn.ensemble import RandomForestRegressor\n\n# Train a Random Forest model\nrf_model = RandomForestRegressor(n_estimators=100, random_state=42)\nrf_model.fit(X, y)\n\n# Extract feature importances\nfeature_importances = rf_model.feature_importances_\n\n# Create a DataFrame for visualization\nimportance_df = pd.DataFrame({\n    'Feature': feature_columns,\n    'Importance': feature_importances\n}).sort_values(by='Importance', ascending=True)\n\n# Create a horizontal bar chart for feature importances\nplt.figure(figsize=(10, 6))\nplt.barh(importance_df['Feature'], importance_df['Importance'], color='dodgerblue')\nplt.xlabel('Importance')\nplt.ylabel('')\nplt.title('Random Forest Feature Importance')\nplt.xticks(fontsize=13)  # Increase the font size of the x-axis tick labels\nplt.yticks(fontsize=14)  # Increase the font size of the x-axis tick labels\n# Save the figure before showing it\nplt.savefig('random_forest_importance_plot.png', bbox_inches='tight', dpi=300)  # 'bbox_inches' ensures the entire plot is saved\n\nplt.show()\n\n\n\nfeature_importances\n\n\n\npng\n\n\narray([0.71850575, 0.10367982, 0.01655567, 0.07232808, 0.05603188,\n       0.0146824 , 0.0182164 ])"
  },
  {
    "objectID": "Research Projects/Exoplanet Modeling/index.html#random-forest-feature-importance-analysis",
    "href": "Research Projects/Exoplanet Modeling/index.html#random-forest-feature-importance-analysis",
    "title": "Exoplanet Modeling",
    "section": "Random Forest Feature Importance Analysis",
    "text": "Random Forest Feature Importance Analysis\nTo gain insights into which exoplanetary features most influence the observed transit depth, a Random Forest Regressor was utilized. Here’s an outline of the procedure:\n\nModel Initialization: A Random Forest Regressor model was instantiated with 100 trees and a fixed random seed of 42 for reproducibility.\nModel Training: The model was trained on the feature set X and target variable y.\nFeature Importance Extraction: After training, the importance of each feature was extracted using the feature_importances_ attribute of the trained model.\nData Preparation for Visualization: A DataFrame was created to house each feature alongside its respective importance. The features were then sorted in ascending order of importance for better visualization.\nVisualization: A horizontal bar chart was plotted to showcase the importance of each feature. The chart offers a clear visual comparison, with the y-axis representing the features and the x-axis indicating their importance. Special attention was paid to font size adjustments for better readability. Furthermore, before displaying the chart, it was saved as a PNG image with high resolution.\n\nThe resulting visualization, titled ‘Random Forest Feature Importance’, provides a clear understanding of the relative significance of each feature in predicting the transit depth, as discerned by the Random Forest model.\nfrom IPython.display import Image\nImage(filename='PME.png')\n\n\n\npng"
  },
  {
    "objectID": "Research Projects/Exoplanet Modeling/index.html#pme-feature-importance-analysis",
    "href": "Research Projects/Exoplanet Modeling/index.html#pme-feature-importance-analysis",
    "title": "Exoplanet Modeling",
    "section": "PME Feature Importance Analysis",
    "text": "PME Feature Importance Analysis\nUsing a modeling technique, the impact of different exoplanetary features on the observed transit depth was assessed, and their importance was visualized in the attached figure titled ‘PME Feature Importance’. Here’s a breakdown of the visual representation:\n\nMost Influential Feature: The planet_radius stands out as the most influential feature with the highest PME (Predictive Modeling Estimate) value. This suggests that the radius of the planet plays a pivotal role in determining the observed transit depth.\nOther Features: Logarithmic concentrations of gases, such as log_co2, log_co, log_nh3, log_h2o, and log_ch4, also exhibit varying degrees of importance. Among these, log_co2 and log_co are the more significant contributors compared to others.\nLeast Influential Feature: The planet_temp, representing the temperature of the planet, has the least importance in this analysis, suggesting its minimal role in influencing the transit depth, at least according to the PME metric.\nVisual Clarity: The horizontal bar chart offers a lucid comparison of feature importances. Each bar’s length represents the PME value of a feature, providing a direct visual cue to its significance.\nInterpretation: This visualization aids in discerning which exoplanetary characteristics are most relevant when predicting the transit depth using the given model. It can guide future analyses by highlighting key features to focus on or, conversely, those that might be less consequential.\n\nBy examining the ‘PME Feature Importance’ chart, one gains a deeper understanding of the relative significance of each feature in predicting the transit depth within this specific modeling context."
  },
  {
    "objectID": "Research Projects/Exoplanet Modeling/index.html#uncertainty-quantification-and-feature-reduction-in-pme-feature-importance-analysis",
    "href": "Research Projects/Exoplanet Modeling/index.html#uncertainty-quantification-and-feature-reduction-in-pme-feature-importance-analysis",
    "title": "Exoplanet Modeling",
    "section": "Uncertainty Quantification and Feature Reduction in PME Feature Importance Analysis",
    "text": "Uncertainty Quantification and Feature Reduction in PME Feature Importance Analysis\nWhen delving deep into the realms of uncertainty quantification and feature reduction in predictive modeling, it’s crucial to evaluate feature importance metrics critically. The provided PME (Predictive Modeling Estimate) Feature Importance Analysis offers a valuable lens for this task. Below is a nuanced exploration of its significance in the described contexts:\n\nUncertainty Quantification:\n\nOrigin of Uncertainty:\n\nIn exoplanetary spectral analysis, uncertainty can arise from observational noise, instrumental errors, or intrinsic variability of the observed phenomena. This uncertainty often manifests in the form of error bars in spectra, like the ones shown in transit depth against wavelengths.\n\nFeature Impact on Uncertainty:\n\nThe degree of influence a feature has on the predicted outcome can be a proxy for how that feature might contribute to the overall predictive uncertainty. If a feature like planet_radius has a high PME value, it might be a critical determinant of transit depth. Any uncertainty in measuring or estimating the planet_radius could propagate and significantly affect the prediction’s reliability.\n\nPME as a Measure of Stochastic Uncertainty:\n\nThe PME values themselves might be obtained by analyzing a model’s sensitivity to perturbations in input features. A high PME value indicates that slight changes in the feature can lead to notable changes in the output, thereby implying a greater inherent stochastic uncertainty tied to that feature.\n\n\n\n\nFeature Reduction:\n\nIdentifying Critical Features:\n\nWhen dealing with a multitude of features, not all may be equally relevant. The PME analysis provides a hierarchy of feature importance. In this case, while planet_radius emerges as crucial, planet_temp appears less consequential. This differentiation is fundamental for feature reduction, guiding us on which features to prioritize in modeling.\n\nReducing Dimensionality & Complexity:\n\nIn data-driven modeling, especially with limited data points, overfitting is a genuine concern. By understanding which features significantly influence the predictions (like planet_radius or log_co2), one can potentially reduce the model’s complexity and the risk of overfitting by focusing only on these paramount features.\n\nInforming Experimental Design:\n\nIf further observational or experimental data is required, knowing feature importances can guide where resources are channeled. For instance, more precise measurements might be sought for features with high PME values, as their accurate estimation is vital for reliable predictions.\n\nTrade-off with Predictive Performance:\n\nIt’s essential to understand that while feature reduction can simplify models and make them more interpretable, there’s always a trade-off with predictive performance. Removing features based on their PME values should be done judiciously, ensuring that the model’s predictive capability isn’t unduly compromised.\n\n\nIn summary, the ‘PME Feature Importance’ chart isn’t merely a representation of feature significance but serves as a cornerstone for rigorous analytical decisions in uncertainty quantification and feature reduction. Analyzing such importance metrics within the broader context of the problem at hand ensures that models are both robust and interpretable, catering effectively to the dual objectives of predictive accuracy and analytical clarity.\n\n\nWeighted Principal Component Analysis (PCA) on Exoplanet Data\n\nConceptual Overview:\nPCA is a method used to emphasize variation and capture strong patterns in a dataset. The “weighted PCA” approach fine-tunes this by considering the importance of different features, effectively giving more attention to features deemed vital.\n\n\nDetailed Breakdown:\n\nStandardization:\n\nBefore applying PCA, the dataset is standardized to give each feature a mean of 0 and a variance of 1. This is essential because PCA is influenced by the scale of the data.\n\nWeighted Features:\n\nFeatures are weighted according to their importance, as identified by the Random Forest model. Taking the square root of the weights ensures proper scaling during matrix multiplication in PCA.\n\nPCA Application:\n\nPCA projects the data into a new space defined by its principal components. The first two components often hold most of the dataset’s variance, making them crucial for visualization.\n\nVisualization:\n\nThe scatter plot visualizes the data in the space of the first two principal components. The color indicates Transit Depth (x1), shedding light on how this parameter varies across the main patterns in the data.\n\nExplained Variance:\n\nThis provides an understanding of how much original variance the first two components capture. A high percentage indicates that the PCA representation retains much of the data’s original structure.\n\n\n\n\nSignificance:\n\nData Compression:\n\nPCA offers a simplified yet rich representation of the data, which can be invaluable for visualization and pattern recognition.\n\nFeature Emphasis:\n\nUsing feature importances ensures the PCA representation highlights the most critical patterns related to influential features.\n\nFramework for Further Exploration:\n\nObserving patterns or groupings in the PCA plot can guide subsequent investigations, pinpointing areas of interest or potential clusters.\n\nEfficient Data Overview:\n\nThe visualization provides a comprehensive but digestible overview of the data, suitable for a wide range of audiences.\n\n\nIn essence, weighted PCA melds the dimensionality reduction capabilities of PCA with the interpretative power of feature importances, offering a profound view into the dataset’s intricate structures and relationships.\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nimport numpy as np\n\n# Standardize the feature matrix\nX_scaled = StandardScaler().fit_transform(X)\n\n# Multiply each feature by its square root of importance weight for weighted PCA\n# The square root is used because each feature contributes to both rows and columns in the dot product calculation\nX_weighted = X_scaled * np.sqrt(feature_importances)\n\n# Perform PCA on the weighted data\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X_weighted)\n\n# Plotting the first two principal components\nplt.figure(figsize=(10, 7))\nplt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis', edgecolor='k', s=40)\nplt.colorbar().set_label('Transit Depth (x1)', rotation=270, labelpad=15)\nplt.xlabel('Weighted Principal Component 1')\nplt.ylabel('Weighted Principal Component 2')\nplt.title('Weighted PCA: First Two Principal Components')\nplt.show()\n\n# Variance explained by the first two principal components\nvariance_explained = pca.explained_variance_ratio_\nvariance_explained\n\n\n\npng\n\n\narray([0.71985328, 0.10370239])\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\n# Your data initialization (this part is assumed, as you haven't provided it in the original code)\n# X, y, feature_importances = ...\n\n# Standardize the feature matrix\nX_scaled = StandardScaler().fit_transform(X)\n\n# Multiply each feature by its square root of importance weight for weighted PCA\nX_weighted = X_scaled * np.sqrt(feature_importances)\n\n# Perform PCA on the weighted data with three components\npca = PCA(n_components=3)\nX_pca = pca.fit_transform(X_weighted)\n\n# Plotting the first three principal components\nfig = plt.figure(figsize=(10, 7))\nax = fig.add_subplot(111, projection='3d')\nscatter = ax.scatter(X_pca[:, 0], X_pca[:, 1], X_pca[:, 2], c=y, cmap='viridis', edgecolor='k', s=40)\nplt.colorbar(scatter, ax=ax, pad=0.2).set_label('Transit Depth (x1)', rotation=270, labelpad=15)\nax.set_xlabel('Weighted Principal Component 1')\nax.set_ylabel('Weighted Principal Component 2')\nax.set_zlabel('Weighted Principal Component 3')\nplt.title('Weighted PCA: First Three Principal Components')\nplt.show()\n\n# Variance explained by the first three principal components\nvariance_explained = pca.explained_variance_ratio_\nvariance_explained\n\n\n\npng\n\n\narray([0.71985328, 0.10370239, 0.07160805])\n# side by side\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\n# Your data initialization (this part is assumed, as you haven't provided it in the original code)\n# X, y, feature_importances = ...\n\n# Standardize the feature matrix\nX_scaled = StandardScaler().fit_transform(X)\n\n# Multiply each feature by its square root of importance weight for weighted PCA\nX_weighted = X_scaled * np.sqrt(feature_importances)\n\n# Create a combined figure with subplots\nfig, axes = plt.subplots(1, 2, figsize=(20, 7))\n\n# Perform PCA on the weighted data with two components and plot\npca2 = PCA(n_components=2)\nX_pca2 = pca2.fit_transform(X_weighted)\nscatter_2d = axes[0].scatter(X_pca2[:, 0], X_pca2[:, 1], c=y, cmap='viridis', edgecolor='k', s=40)\nfig.colorbar(scatter_2d, ax=axes[0], orientation='vertical').set_label('Transit Depth (x1)', rotation=270, labelpad=15)\naxes[0].set_xlabel('Weighted Principal Component 1')\naxes[0].set_ylabel('Weighted Principal Component 2')\naxes[0].set_title('Weighted PCA: First Two Principal Components')\n\n# Perform PCA on the weighted data with three components and plot\npca3 = PCA(n_components=3)\nX_pca3 = pca3.fit_transform(X_weighted)\nax3d = fig.add_subplot(1, 2, 2, projection='3d')\nscatter_3d = ax3d.scatter(X_pca3[:, 0], X_pca3[:, 1], X_pca3[:, 2], c=y, cmap='viridis', edgecolor='k', s=40)\nfig.colorbar(scatter_3d, ax=ax3d, pad=0.2).set_label('Transit Depth (x1)', rotation=270, labelpad=15)\nax3d.set_xlabel('Weighted Principal Component 1')\nax3d.set_ylabel('Weighted Principal Component 2')\nax3d.set_zlabel('Weighted Principal Component 3')\nax3d.set_title('Weighted PCA: First Three Principal Components')\n\n# Display the combined plot\nplt.tight_layout()\nplt.savefig('wPCA_2D_3D.png', bbox_inches='tight', dpi=300)  # 'bbox_inches' ensures the entire plot is saved\n\nplt.show()\n\n\n\npng\n\n\nimport pandas as pd\nimport seaborn as sns\n\n# Your data initialization (this part is assumed, as you haven't provided it in the original code)\n# df, feature_columns, transit_depth_column = ...\n\n# Reconstruct the approximate original data using the first three principal components\nreconstructed_data = np.dot(X_pca, pca.components_[:3, :]) + np.mean(X_scaled, axis=0)\n\n# Create a DataFrame for the reconstructed data\nreconstructed_df = pd.DataFrame(reconstructed_data, columns=feature_columns)\n\n# Visualize the approximate original histograms\nfig, axes = plt.subplots(1, len(feature_columns) + 1, figsize=(18, 4))  # Adjusted for the number of features\nfor i, col in enumerate(feature_columns):\n    sns.histplot(reconstructed_df[col], bins=20, kde=True, ax=axes[i], color='orange')\n    axes[i].set_title(f'Approx of {col}')\n\n# Add visualization for actual transit depth (since it was not part of the PCA)\nsns.histplot(df[transit_depth_column], bins=20, kde=True, ax=axes[-1], color='green')\naxes[-1].set_title(f'Actual {transit_depth_column}')\n\nplt.tight_layout()\nplt.show()\n\n\n\npng\n\n\n\nfig, axes = plt.subplots(2, 8, figsize=(18, 8))\n\n# Original Data\nfor i, col in enumerate(feature_columns):\n    sns.histplot(df[col], bins=20, kde=True, ax=axes[0, i])\n    axes[0, i].set_title(f'{col}\\nMean: {feature_mean[col]:.2f}\\nVariance: {feature_variance[col]:.2f}')\n\nsns.histplot(df[transit_depth_column], bins=20, kde=True, ax=axes[0, -1])\naxes[0, -1].set_title(f'{transit_depth_column}\\nMean: {transit_depth_mean:.2f}\\nVariance: {transit_depth_variance:.2f}')\n\n# Reconstructed Data\nfor i, col in enumerate(feature_columns):\n    sns.histplot(reconstructed_df[col], bins=20, kde=True, ax=axes[1, i], color='orange')\n    axes[1, i].set_title(f'Surrogate of {col}')\n\nsns.histplot(df[transit_depth_column], bins=20, kde=True, ax=axes[1, -1], color='green')\naxes[1, -1].set_title(f'Actual {transit_depth_column}')\n\nplt.tight_layout()\nplt.show()\n\n\n\npng\n\n\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nimport numpy as np\n\n# Assuming data initialization and feature_importances, feature_columns, and transit_depth_column definitions exist\n\n# Standardize the feature matrix\nX_scaled = StandardScaler().fit_transform(X)\n\n# Multiply each feature by its square root of importance weight for weighted PCA\nX_weighted = X_scaled * np.sqrt(feature_importances)\n\n# Initialize the figure and axes\nfig, axes = plt.subplots(3, len(feature_columns) + 1, figsize=(18, 12))\n\n# Original Data histograms\nfor i, col in enumerate(feature_columns):\n    sns.histplot(df[col], bins=20, kde=True, ax=axes[0, i])\n    axes[0, i].set_title(f'Original {col}')\n\nsns.histplot(df[transit_depth_column], bins=20, kde=True, ax=axes[0, -1])\naxes[0, -1].set_title(f'Original {transit_depth_column}')\n\n# Reconstruction using 2 PCs\npca2 = PCA(n_components=2)\nX_pca2 = pca2.fit_transform(X_weighted)\nreconstructed_data_2PCs = np.dot(X_pca2, pca2.components_[:2, :]) + np.mean(X_scaled, axis=0)\nreconstructed_df_2PCs = pd.DataFrame(reconstructed_data_2PCs, columns=feature_columns)\n\nfor i, col in enumerate(feature_columns):\n    sns.histplot(reconstructed_df_2PCs[col], bins=20, kde=True, ax=axes[1, i], color='orange')\n    axes[1, i].set_title(f'2 PCs of {col}')\n\nsns.histplot(df[transit_depth_column], bins=20, kde=True, ax=axes[1, -1], color='green')\naxes[1, -1].set_title(f'Original {transit_depth_column}')\n\n# Reconstruction using 3 PCs\npca3 = PCA(n_components=3)\nX_pca3 = pca3.fit_transform(X_weighted)\nreconstructed_data_3PCs = np.dot(X_pca3, pca3.components_[:3, :]) + np.mean(X_scaled, axis=0)\nreconstructed_df_3PCs = pd.DataFrame(reconstructed_data_3PCs, columns=feature_columns)\n\nfor i, col in enumerate(feature_columns):\n    sns.histplot(reconstructed_df_3PCs[col], bins=20, kde=True, ax=axes[2, i], color='purple')\n    axes[2, i].set_title(f'3 PCs of {col}')\n\nsns.histplot(df[transit_depth_column], bins=20, kde=True, ax=axes[2, -1], color='green')\naxes[2, -1].set_title(f'Original {transit_depth_column}')\n\nplt.tight_layout()\nplt.savefig('wPCA_combined_plot.png', bbox_inches='tight', dpi=500)  # 'bbox_inches' ensures the entire plot is saved\n\nplt.show()\n\n\n\npng\n\n\n\n\n\nReconstruction of Exoplanet Data Using Principal Component Analysis (PCA)\nThe code segment aims to reconstruct exoplanet data using different numbers of principal components (PCs) from PCA, offering insights into how much information retention occurs as we vary the number of PCs.\n\nData Standardization:\n\nThe features undergo standardization, ensuring each feature has a mean of 0 and variance of 1. This normalization is crucial for PCA, as the algorithm is sensitive to varying scales across features.\n\n\n\nWeighted Features:\n\nFeatures are adjusted based on their significance as ascertained by a prior model, specifically the Random Forest. Weighting the features adjusts the emphasis the PCA places on each feature during the dimensionality reduction process.\n\n\n\nData Reconstruction:\n\nAfter performing PCA, the algorithm seeks to transform the reduced data back to the original high-dimensional space. This “reconstructed” data is an approximation of the original but is built using fewer dimensions.\n\n\n\nVisualization of Reconstructions:\n\nOriginal Data Histograms:\n\nThe initial histograms show the distributions of the original features and the transit depth.\n\nReconstruction Using 2 Principal Components:\n\nUsing only the first two PCs, the data is reconstructed and its histograms visualized. This illustrates the patterns and distributions captured when only the first two components are considered.\n\nReconstruction Using 3 Principal Components:\n\nAn analogous reconstruction is done with three PCs. The addition of a third dimension might capture more nuanced variations in the data.\n\n\n\n\nSignificance:\n\nData Compression vs. Retention:\n\nThe visualizations enable us to compare the reconstructions against the original data. We discern how much information is retained and what is lost as we reduce dimensions.\n\nGuide for Dimensionality Decision:\n\nBy juxtaposing the original with the reconstructions, we gain insights into the optimal number of PCs to use for specific tasks, striking a balance between compression and information retention.\n\nEmpirical Understanding:\n\nThese histograms and visual representations offer a tangible way to grasp the abstract notion of dimensionality reduction. They elucidate how PCA captures the essence of the data while diminishing dimensions.\n\n\nIn conclusion, this analysis, coupled with the visualizations, equips us with a robust understanding of how PCA reconstructs data using varying numbers of components. It underscores the trade-offs involved and the implications of choosing specific dimensionality levels in data-driven tasks.\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport matplotlib.gridspec as gridspec\n\n# Load images\nimg_shap = mpimg.imread('shapley.png')\nimg_rf = mpimg.imread('random_forest_importance_plot.png')\nimg_pme = mpimg.imread('PME.png')\n\n# Create a grid for the subplots\nfig_combined = plt.figure(figsize=(20, 12))\ngs = gridspec.GridSpec(2, 2, width_ratios=[1, 1])\n\n# Display SHAP plot\nax0 = plt.subplot(gs[0])\nax0.imshow(img_shap)\nax0.axis('off')\n\n# Display RF Importance plot\nax1 = plt.subplot(gs[1])\nax1.imshow(img_rf)\nax1.axis('off')\n\n# Display PME image in the middle of the 2nd row\nax2 = plt.subplot(gs[2:4])  # This makes the PME plot span both columns on the second row\nax2.imshow(img_pme)\nax2.axis('off')\n\nplt.tight_layout()\nplt.savefig('sensitivity_combined_plot.png', bbox_inches='tight', dpi=300)  # 'bbox_inches' ensures the entire plot is saved\n\nplt.show()\n\n\n\npng\n\n\nThis code snippet consolidates and displays three distinct plots—SHAP values, Random Forest Feature Importance, and PME Feature Importance—into a single visualization. The images are loaded and arranged in a 2x2 grid, with the SHAP and Random Forest plots on the top row, and the PME plot spanning both columns on the bottom row. After layout adjustments, the combined visualization is saved as a high-resolution PNG image titled ‘sensitivity_combined_plot.png’ and then displayed.\n\n\n\nReference\nChangeat, Q., & Yip, K. H. (2023). ESA-Ariel Data Challenge NeurIPS 2022: Introduction to exo-atmospheric studies and presentation of the Atmospheric Big Challenge (ABC) Database. arXiv preprint arXiv:2206.14633.\nHerin, M., Il Idrissi, M., Chabridon, V., & Iooss, B. (2022). Proportional marginal effects for global sensitivity analysis. arXiv preprint arXiv:2210.13065."
  },
  {
    "objectID": "Research Projects/index.html",
    "href": "Research Projects/index.html",
    "title": "Research Projects",
    "section": "",
    "text": "Here are some of the research projects I am currently involved in:\n\nExoplanet Modeling\nInformer Model\nLong-term Short-term Time-series Modeling\n\nEach project explores unique challenges and methodologies in the domain of Middle-earth scientific research."
  },
  {
    "objectID": "Fun/index.html",
    "href": "Fun/index.html",
    "title": "Photography",
    "section": "",
    "text": "As a wizard and scholar of Middle-earth, I have been studying the magic of the natural world for centuries. Through my self-portraits, I aim to capture the essence of my own being and reflect on my own journey through time. Each photograph is a reflection of my own experiences and emotions. Through my photography, I hope to offer a glimpse into my life as a scholar and adventurer, and inspire others to reflect on their own journeys through the world.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nView the tutorial for this template (+ download link)"
  },
  {
    "objectID": "CV/index.html",
    "href": "CV/index.html",
    "title": "Curriculum vitae",
    "section": "",
    "text": "Download current CV\n  \n\n\n  \n\n\nView the tutorial for this template (+ download link)"
  },
  {
    "objectID": "Courses/Stat 120 Intro to Statistics/index.html",
    "href": "Courses/Stat 120 Intro to Statistics/index.html",
    "title": "Stat 120: Intro to Statistics",
    "section": "",
    "text": "This course provides an introduction to the foundational concepts of statistics. We will explore…\n\nSyllabus\nLecture Notes\nAssignments …"
  },
  {
    "objectID": "Courses/Stat 220 Intro to Data Science/index.html",
    "href": "Courses/Stat 220 Intro to Data Science/index.html",
    "title": "Stat 120: Intro to Statistics",
    "section": "",
    "text": "This course provides an introduction to the foundational concepts of statistics. We will explore…\n\nSyllabus\nLecture Notes\nAssignments …"
  },
  {
    "objectID": "Courses/Stat 230 Applied Linear Regression/index.html",
    "href": "Courses/Stat 230 Applied Linear Regression/index.html",
    "title": "Stat 120: Intro to Statistics",
    "section": "",
    "text": "This course provides an introduction to the foundational concepts of statistics. We will explore…\n\nSyllabus\nLecture Notes\nAssignments …"
  },
  {
    "objectID": "Courses/index.html",
    "href": "Courses/index.html",
    "title": "Courses",
    "section": "",
    "text": "I have been teaching various courses related to the world of Middle-earth. Here are some of the courses I offer:\n\nStat 120: Intro to Statistics\nStat 220: Intro to Data Science\nStat 230: Applied Linear Regression"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Deepak Bastola",
    "section": "",
    "text": "Twitter\n  \n  \n    \n     Github\n  \n  \n    \n     LinkedIn\n  \n  \n    \n     Email\n  \n\n  \n  \n\n\n\n\nWelcome to my website!\nI am a visiting assistant professor of statistics at Carleton College. My research interests include Bayesian statistics, time series methodology, and statistical computation. My research interests include Bayesian statistics, time series methodology, and statistical computation."
  }
]